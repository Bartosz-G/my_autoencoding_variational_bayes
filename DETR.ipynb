{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import CocoDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backbone Init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "class ResNetBackBone(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(ResNetBackBone, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            *list(pretrained_model.children())[:-2]\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.fc = nn.Conv2d(512, 256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/bartoszgawin/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "resnet18 = ResNetBackBone(pretrained_model=torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "for param in resnet18.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet18.avgpool.requires_grad = False\n",
    "resnet18.fc.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the Transformer blocks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout = 0):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention_matrices = None\n",
    "\n",
    "        self.multi_head_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout_1 = nn.Dropout(p = dropout)\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.dropout_2 = nn.Dropout(p = dropout)\n",
    "        self.norm_2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, X, positional_encoding):\n",
    "        spatially_encoded = X + positional_encoding\n",
    "        attention_output, self.attention_matrices = self.multi_head_attention(X, spatially_encoded, spatially_encoded, average_attn_weights=True)\n",
    "\n",
    "        print(f'attention_output dim: {attention_output.shape}')\n",
    "\n",
    "        attention_output = self.norm_1(X + self.dropout_1(attention_output))\n",
    "        fc_output = self.fully_connected(attention_output.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        output = self.norm_2(attention_output + self.dropout_2(fc_output))\n",
    "        return output.permute(0, 2, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking the Code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-1.4006e-02,  8.0092e-02, -4.5088e-02,  ..., -1.3631e-02,\n           -9.7450e-02,  7.6295e-02],\n          [ 1.6658e-02,  9.6366e-02, -1.7328e-02,  ..., -2.8459e-02,\n           -3.2355e-02,  7.9819e-02],\n          [-5.5816e-02, -7.6856e-03, -1.6042e-02,  ...,  1.3713e-02,\n           -8.0267e-03,  1.0999e-01],\n          ...,\n          [-1.9755e-02, -8.9302e-02, -4.1531e-02,  ...,  6.9308e-02,\n            1.2063e-02,  5.5157e-02],\n          [ 7.1922e-03, -6.2005e-03, -1.5023e-02,  ...,  2.6038e-02,\n           -9.9263e-03,  6.6982e-02],\n          [-6.8094e-02, -2.3310e-04, -5.8371e-02,  ..., -5.6937e-02,\n            4.3119e-03,  6.6102e-02]],\n \n         [[ 2.5886e-02, -2.1180e-02, -9.1879e-04,  ...,  1.2239e-01,\n            5.7915e-02, -9.1296e-02],\n          [-3.4611e-02, -4.8245e-03,  3.0025e-02,  ...,  3.1376e-02,\n            6.1042e-02, -1.3485e-01],\n          [ 2.3242e-02,  6.3017e-03, -8.7556e-02,  ...,  6.9737e-02,\n            4.7858e-02, -4.3233e-02],\n          ...,\n          [ 1.5916e-02, -6.9298e-02, -1.4598e-02,  ...,  6.4616e-02,\n            2.9702e-02, -1.7020e-01],\n          [-1.2201e-02, -4.5098e-02, -2.0996e-02,  ...,  9.2456e-02,\n            1.9064e-02, -1.3995e-01],\n          [-4.8148e-02, -1.7945e-02, -7.8074e-02,  ...,  7.0297e-02,\n            3.3259e-02, -1.1773e-01]],\n \n         [[ 2.0267e-02,  1.1317e-01,  8.5901e-02,  ...,  3.0048e-02,\n            9.3647e-02, -1.1137e-01],\n          [ 1.4760e-04,  7.6883e-02,  5.1793e-02,  ...,  9.6612e-03,\n            1.8027e-01, -1.0729e-01],\n          [-9.2680e-03,  1.1522e-01,  2.6852e-02,  ...,  6.9739e-02,\n            1.8097e-02, -7.6364e-02],\n          ...,\n          [ 2.1676e-03,  9.8878e-02,  1.2688e-01,  ...,  6.7171e-02,\n            6.9236e-02, -6.7067e-02],\n          [-7.4011e-02,  3.4082e-02,  9.7181e-02,  ..., -4.6555e-02,\n           -4.7801e-03, -8.2707e-02],\n          [-2.8908e-02,  8.4824e-02,  1.3740e-01,  ...,  8.9214e-03,\n           -2.0603e-02, -5.2464e-02]],\n \n         ...,\n \n         [[ 5.1854e-02,  1.1942e-02,  5.6607e-02,  ..., -8.8953e-03,\n            8.5682e-02, -9.1451e-02],\n          [ 3.3428e-02, -5.0936e-02,  1.3777e-01,  ...,  6.4528e-02,\n            1.8233e-01, -1.1284e-01],\n          [ 6.2682e-02, -2.3780e-02,  9.4829e-02,  ...,  3.3016e-02,\n            1.3886e-01, -1.1696e-01],\n          ...,\n          [ 5.0613e-02, -3.3693e-02,  8.4083e-03,  ...,  5.0846e-02,\n            1.0986e-01, -8.5547e-02],\n          [ 5.3044e-02, -3.8370e-02,  5.7087e-02,  ...,  1.6048e-02,\n            6.6601e-02, -1.0122e-01],\n          [ 4.3313e-02,  4.6717e-02,  7.3708e-02,  ...,  5.1388e-02,\n            2.4736e-02, -9.1920e-02]],\n \n         [[-2.0986e-01,  4.4646e-02, -2.0918e-03,  ...,  5.3784e-02,\n            2.7901e-02, -9.1242e-02],\n          [-2.3348e-02, -1.8620e-02,  5.6420e-02,  ...,  2.5735e-02,\n            7.1501e-02, -4.0829e-02],\n          [-8.1498e-02,  1.3392e-02, -2.5165e-02,  ..., -2.2327e-02,\n            5.5369e-02,  2.5163e-02],\n          ...,\n          [-1.0192e-01, -5.6766e-03, -1.3149e-02,  ...,  1.7302e-02,\n            5.4167e-02, -3.4620e-02],\n          [-7.2365e-02, -1.0251e-02, -1.7340e-03,  ...,  3.4663e-02,\n            5.3761e-02, -2.8498e-02],\n          [-7.1977e-02,  4.2472e-03, -3.2802e-02,  ..., -4.2260e-03,\n            9.7191e-02, -2.7313e-02]],\n \n         [[ 5.9913e-02, -6.0518e-02, -5.2836e-02,  ...,  2.0597e-02,\n           -5.2820e-02,  9.9057e-02],\n          [-3.2567e-02, -2.1006e-02, -8.9985e-02,  ..., -7.3220e-03,\n            1.0353e-02,  3.1361e-02],\n          [-9.2724e-03, -1.0031e-02, -4.9690e-02,  ..., -2.5019e-03,\n           -4.3700e-02,  5.4853e-02],\n          ...,\n          [ 1.4284e-02, -9.1699e-03, -3.2974e-02,  ..., -3.3296e-02,\n           -4.4342e-02,  9.3810e-02],\n          [ 4.3403e-02, -6.0637e-03, -1.0935e-01,  ..., -5.6579e-02,\n           -3.8225e-02,  9.1917e-02],\n          [ 3.2588e-02,  3.5993e-02, -5.8735e-02,  ..., -2.1708e-02,\n           -5.9345e-02,  5.1792e-02]]], grad_fn=<TransposeBackward0>),\n tensor([[[0.0222, 0.0148, 0.0190,  ..., 0.0249, 0.0123, 0.0209],\n          [0.0191, 0.0214, 0.0187,  ..., 0.0170, 0.0244, 0.0210],\n          [0.0232, 0.0180, 0.0186,  ..., 0.0171, 0.0203, 0.0173],\n          ...,\n          [0.0198, 0.0169, 0.0166,  ..., 0.0232, 0.0161, 0.0221],\n          [0.0187, 0.0198, 0.0143,  ..., 0.0185, 0.0207, 0.0161],\n          [0.0170, 0.0167, 0.0204,  ..., 0.0220, 0.0228, 0.0218]],\n \n         [[0.0183, 0.0235, 0.0167,  ..., 0.0223, 0.0227, 0.0200],\n          [0.0198, 0.0223, 0.0152,  ..., 0.0242, 0.0182, 0.0186],\n          [0.0186, 0.0283, 0.0211,  ..., 0.0148, 0.0196, 0.0241],\n          ...,\n          [0.0179, 0.0197, 0.0233,  ..., 0.0143, 0.0181, 0.0214],\n          [0.0197, 0.0216, 0.0236,  ..., 0.0164, 0.0181, 0.0195],\n          [0.0200, 0.0162, 0.0202,  ..., 0.0186, 0.0190, 0.0167]],\n \n         [[0.0260, 0.0231, 0.0180,  ..., 0.0225, 0.0236, 0.0216],\n          [0.0202, 0.0292, 0.0126,  ..., 0.0216, 0.0249, 0.0157],\n          [0.0186, 0.0251, 0.0237,  ..., 0.0146, 0.0124, 0.0210],\n          ...,\n          [0.0178, 0.0246, 0.0180,  ..., 0.0179, 0.0167, 0.0168],\n          [0.0311, 0.0200, 0.0270,  ..., 0.0166, 0.0227, 0.0199],\n          [0.0210, 0.0166, 0.0199,  ..., 0.0180, 0.0249, 0.0191]],\n \n         ...,\n \n         [[0.0196, 0.0192, 0.0255,  ..., 0.0239, 0.0192, 0.0224],\n          [0.0210, 0.0187, 0.0203,  ..., 0.0325, 0.0206, 0.0197],\n          [0.0198, 0.0215, 0.0216,  ..., 0.0267, 0.0176, 0.0174],\n          ...,\n          [0.0172, 0.0155, 0.0248,  ..., 0.0244, 0.0179, 0.0175],\n          [0.0225, 0.0213, 0.0221,  ..., 0.0218, 0.0191, 0.0179],\n          [0.0145, 0.0238, 0.0147,  ..., 0.0116, 0.0211, 0.0362]],\n \n         [[0.0149, 0.0173, 0.0148,  ..., 0.0221, 0.0174, 0.0284],\n          [0.0196, 0.0247, 0.0175,  ..., 0.0177, 0.0235, 0.0249],\n          [0.0189, 0.0142, 0.0158,  ..., 0.0162, 0.0185, 0.0270],\n          ...,\n          [0.0163, 0.0172, 0.0221,  ..., 0.0140, 0.0199, 0.0239],\n          [0.0222, 0.0189, 0.0212,  ..., 0.0210, 0.0215, 0.0220],\n          [0.0164, 0.0217, 0.0147,  ..., 0.0185, 0.0233, 0.0217]],\n \n         [[0.0163, 0.0221, 0.0122,  ..., 0.0184, 0.0178, 0.0153],\n          [0.0262, 0.0188, 0.0188,  ..., 0.0185, 0.0203, 0.0169],\n          [0.0183, 0.0275, 0.0197,  ..., 0.0186, 0.0207, 0.0152],\n          ...,\n          [0.0175, 0.0249, 0.0210,  ..., 0.0225, 0.0193, 0.0174],\n          [0.0189, 0.0212, 0.0174,  ..., 0.0159, 0.0160, 0.0179],\n          [0.0154, 0.0197, 0.0188,  ..., 0.0177, 0.0173, 0.0160]]],\n        grad_fn=<MeanBackward1>))"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "patch_size = 4\n",
    "height_and_width = 28\n",
    "D = 64\n",
    "N = int(height_and_width**2 / patch_size**2)\n",
    "H = 8\n",
    "dimensions = int(D / H)\n",
    "\n",
    "multi_head_attention = nn.MultiheadAttention(D, H, batch_first=True)\n",
    "input = torch.randn(batch_size, N + 1, D)\n",
    "output = multi_head_attention(input, input, input, average_attn_weights=True)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 50, 64])"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.permute(0, 2, 1).permute(0, 2, 1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 64])\n",
      "attention_output dim: torch.Size([10, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "patch_size = 4\n",
    "height_and_width = 28\n",
    "D = 64\n",
    "N = int(height_and_width**2 / patch_size**2)\n",
    "H = 8\n",
    "C = 64\n",
    "dimensions = int(D / H)\n",
    "\n",
    "multi_head_attention = EncoderBlock(D, H)\n",
    "input = torch.randn(batch_size, N+1, D)\n",
    "print(input.shape)\n",
    "output = multi_head_attention(input, input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/bartoszgawin/.medmnist/dermamnist.npz\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from medmnist import DermaMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])\n",
    "\n",
    "train_download = DermaMNIST(split='train', transform=train_transformations, download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 224, 224)\n",
    "print(x.shape)\n",
    "output = resnet18(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 256, 7, 7])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Source Code of ResNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Source code of ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock, Bottleneck]],\n",
    "            layers: List[int],\n",
    "            num_classes: int = 1000,\n",
    "            zero_init_residual: bool = False,\n",
    "            groups: int = 1,\n",
    "            width_per_group: int = 64,\n",
    "            replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "            self,\n",
    "            block: Type[Union[BasicBlock, Bottleneck]],\n",
    "            planes: int,\n",
    "            blocks: int,\n",
    "            stride: int = 1,\n",
    "            dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}