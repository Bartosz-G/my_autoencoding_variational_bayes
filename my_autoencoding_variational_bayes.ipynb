{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "absolute_path = os.path.join(os.getcwd(), '/mnist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "test_dataset  = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "one_image_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # For visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def plot_image(tensor):\n",
    "\n",
    "    if tensor.shape[0] == 1:\n",
    "        tensor = tensor.squeeze(0)\n",
    "\n",
    "    image = tensor.numpy()\n",
    "\n",
    "\n",
    "    plt.imshow(image, cmap='gray' if tensor.ndim == 2 else None)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_dataset[5][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian-Gaussian Autoencoder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "class GaussianAutoEncodingVariationalBayes(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, inter_dim = 256, classification = False):\n",
    "        super(GaussianAutoEncodingVariationalBayes, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.inter_dim = inter_dim\n",
    "\n",
    "        # Appendix C.2, switch-out Z with X as described ============================\n",
    "        self.encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, inter_dim), # Here are the weights W_3 that make up phi\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.latent_u_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_4 that make up phi\n",
    "        self.latent_variance_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_5 that make up phi\n",
    "        # ===========================================================================\n",
    "\n",
    "\n",
    "        # Appendix C.2, implemented exactly in this way ---------------------------------\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_layers, inter_dim), # Here are the weights W_3 that make up theta\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        if classification:\n",
    "            self.output_u_prediction_head = nn.Sequential(\n",
    "            nn.Linear(inter_dim, input_dim), # Here are the weights W_4 that make up theta\n",
    "            nn.Sigmoid() # Page 6 last sentence explains this part\n",
    "            )\n",
    "        else:\n",
    "            nn.Linear(inter_dim,input_dim) # Here are the weights W_4 that make up theta\n",
    "\n",
    "        self.output_variance_prediction_head = nn.Linear(inter_dim, input_dim) # Here are the weights W_5 that make up theta\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "    def encoder_forward(self, X):\n",
    "        h = self.encoder_mlp(X)\n",
    "        latent_u = self.latent_u_prediction_head(h)\n",
    "        latent_variance = self.latent_variance_prediction_head(h)\n",
    "\n",
    "        latent_logvariance = torch.log(latent_variance)\n",
    "\n",
    "        return latent_u, latent_logvariance # These are the estimates of parameters for q_phi(Z|X) ~ N(latent_u, latent_variance)\n",
    "\n",
    "    def decoder_forward(self, latent_u, latent_logvariance, sampled_error_e = 0):\n",
    "        Z = latent_u + torch.sqrt(torch.exp(latent_logvariance))*sampled_error_e # Latent variables Z\n",
    "        # We sample from prior only during training, so during prediction Z = latent_u\n",
    "\n",
    "        h = self.decoder_mlp(Z)\n",
    "        output_u = self.output_u_prediction_head(h) # This is the main prediction of X_hat\n",
    "        output_variance = self.output_variance_prediction_head(h) # Not needed to reconstruct, but gives us an estimate of the variance\n",
    "\n",
    "        output_logvariance = torch.log(output_variance)\n",
    "\n",
    "        return output_u, output_logvariance # These are the estimates of parameters for p_theta(X|Z) ~ N(output_u, output_variance)\n",
    "\n",
    "    def sample_from_prior(self):\n",
    "        samples = torch.randn(self.hidden_layers) # This is a prior p(Z) ~ N(0,1)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, X):\n",
    "        latent_u, latent_logvariance = self.encoder_forward(X)\n",
    "\n",
    "        sampled_error_e = self.sample_from_prior()\n",
    "\n",
    "        X_hat, logvariance = self.decoder_forward(latent_u, latent_logvariance, sampled_error_e)\n",
    "\n",
    "        return X_hat, logvariance, latent_u, latent_logvariance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian-Bernoulli Autoencoder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class BernoulliAutoEncodingVariationalBayes(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, inter_dim = 256):\n",
    "        super(BernoulliAutoEncodingVariationalBayes, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.inter_dim = inter_dim\n",
    "\n",
    "        # Appendix C.2, switch-out Z with X as described ============================\n",
    "        self.encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, inter_dim), # Here are the weights W_3 that make up phi\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.latent_u_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_4 that make up phi\n",
    "        self.latent_variance_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_5 that make up phi\n",
    "        # ===========================================================================\n",
    "\n",
    "\n",
    "        # Appendix C.1, implemented exactly in this way ---------------------------------\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_layers, inter_dim), # Here are the weights W_1 that make up theta\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(inter_dim, input_dim), # Here are the weights W_2 that make up theta\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def encoder_forward(self, X):\n",
    "        h = self.encoder_mlp(X)\n",
    "        latent_u = self.latent_u_prediction_head(h)\n",
    "        latent_variance = self.latent_variance_prediction_head(h)\n",
    "\n",
    "        latent_logvariance = torch.log(latent_variance)\n",
    "\n",
    "        return latent_u, latent_logvariance # These are the estimates of parameters for q_phi(Z|X) ~ N(latent_u, latent_variance)\n",
    "\n",
    "    def decoder_forward(self, latent_u, latent_logvariance, sampled_error_e = 0):\n",
    "        Z = latent_u + torch.sqrt(torch.exp(latent_logvariance))*sampled_error_e # Latent variables Z\n",
    "        # We sample from prior only during training, so during prediction Z = latent_u\n",
    "\n",
    "        output_u = self.decoder_mlp(Z)\n",
    "\n",
    "        return output_u # These are the estimates of parameters for p_theta(X|Z) ~ Bernoulli(Z)\n",
    "\n",
    "    def sample_from_prior(self):\n",
    "        samples = torch.randn(self.hidden_layers) # This is a prior p(Z) ~ N(0,1)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, X):\n",
    "        latent_u, latent_logvariance = self.encoder_forward(X)\n",
    "\n",
    "        sampled_error_e = self.sample_from_prior()\n",
    "\n",
    "        X_hat = self.decoder_forward(latent_u, latent_logvariance, sampled_error_e)\n",
    "\n",
    "        return X_hat, latent_u, latent_logvariance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "# The paper mentions initializing parameters with N(0, 0.01), but I've also provided a xavier initialization\n",
    "def init_weights_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def init_weights_xavier(m): # Not recommended for Bernoulli\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing whether the VAE works:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n        grad_fn=<SigmoidBackward0>),\n tensor([[-7.9676e-05,  2.1187e-03]], grad_fn=<AddmmBackward0>),\n tensor([[    nan, -7.3586]], grad_fn=<LogBackward0>))"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GVAE = GaussianAutoEncodingVariationalBayes(784, 2, classification=True)\n",
    "BVAE = BernoulliAutoEncodingVariationalBayes(784, 2)\n",
    "GVAE.apply(init_weights_normal)\n",
    "BVAE.apply(init_weights_normal)\n",
    "\n",
    "\n",
    "single_image, _ = next(iter(one_image_loader))\n",
    "single_image = single_image.view(single_image.size(0),-1)\n",
    "\n",
    "GVAE(single_image)[0]\n",
    "BVAE(single_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian Reconstruction Loss:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(X, X_hat, logvar):\n",
    "    reconstruction_loss = torch.sum(-0.5 * (torch.log(2 * torch.pi) + logvar + (X - X_hat) ** 2 / torch.exp(logvar)))\n",
    "    return reconstruction_loss / X.size(0)  # Normalize by batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bernoulli Reconstruction Loss:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def bernoulli_log_likelihood(X, X_hat):\n",
    "    X_hat = X_hat.clamp(min=0, max=1)\n",
    "    X = X.clamp(min=0, max=1)\n",
    "    bce_loss = torch.nn.functional.binary_cross_entropy(X_hat, X, reduction='sum')\n",
    "    return bce_loss / X.size(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Kullback Leiber Divergence:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def kullback_leiber_divergence(latent_u, latent_logvar):\n",
    "    kl_div = -0.5 * torch.sum(1 + latent_logvar - latent_u.pow(2) - latent_logvar.exp())\n",
    "    return kl_div"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bernoulli Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[168], line 24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Compute losses\u001B[39;00m\n\u001B[1;32m     23\u001B[0m kl_div_loss \u001B[38;5;241m=\u001B[39m kullback_leiber_divergence(latent_u, latent_logvar)\n\u001B[0;32m---> 24\u001B[0m recon_loss \u001B[38;5;241m=\u001B[39m \u001B[43mbernoulli_log_likelihood\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_hat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m loss \u001B[38;5;241m=\u001B[39m kl_div_loss \u001B[38;5;241m+\u001B[39m recon_loss\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misnan(loss):\n",
      "Cell \u001B[0;32mIn[110], line 4\u001B[0m, in \u001B[0;36mbernoulli_log_likelihood\u001B[0;34m(X, X_hat)\u001B[0m\n\u001B[1;32m      2\u001B[0m X_hat \u001B[38;5;241m=\u001B[39m X_hat\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m bce_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bce_loss \u001B[38;5;241m/\u001B[39m X\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:3122\u001B[0m, in \u001B[0;36mbinary_cross_entropy\u001B[0;34m(input, target, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3119\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m _infer_size(target\u001B[38;5;241m.\u001B[39msize(), weight\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m   3120\u001B[0m     weight \u001B[38;5;241m=\u001B[39m weight\u001B[38;5;241m.\u001B[39mexpand(new_size)\n\u001B[0;32m-> 3122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction_enum\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "BVAE = BernoulliAutoEncodingVariationalBayes(784, 10)\n",
    "# BVAE.apply(init_weights_normal)\n",
    "num_epochs = 50\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(BVAE.parameters(), lr=0.00001)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    BVAE.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # Assuming data is your input and is already properly scaled/preprocessed\n",
    "        data = data.view(data.size(0), -1)  # Flatten the data if necessary\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        X_hat, latent_u, latent_logvar  = BVAE(data)\n",
    "\n",
    "        # Compute losses\n",
    "        kl_div_loss = kullback_leiber_divergence(latent_u, latent_logvar)\n",
    "        recon_loss = bernoulli_log_likelihood(data, X_hat)\n",
    "        loss = kl_div_loss + recon_loss\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"Stopping training due to NaN loss\")\n",
    "            break\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Compute average loss for this epoch\n",
    "    average_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([100, 784])"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1966, -0.1454, -0.1002, -0.0111,  0.1254, -0.0657, -0.1182, -0.0272,\n         -0.1300,  0.2046],\n        [-0.1671, -0.0708, -0.0048, -0.0626, -0.0260,  0.0571,  0.0940,  0.1157,\n         -0.0302, -0.0407],\n        [-0.1526, -0.1109, -0.0858,  0.0171,  0.0121, -0.0708, -0.1341,  0.0596,\n          0.0384,  0.1759],\n        [-0.1077, -0.0303,  0.0079, -0.0093,  0.0958,  0.0223,  0.0513,  0.0514,\n         -0.0148,  0.0448],\n        [-0.0976, -0.0710, -0.0360,  0.0152,  0.0762, -0.0010,  0.0437,  0.1161,\n         -0.0499,  0.1488],\n        [-0.1696, -0.2040, -0.1786, -0.0031, -0.0247,  0.0621, -0.0028,  0.1014,\n          0.0191,  0.2044],\n        [ 0.0053, -0.0219, -0.1401, -0.0481,  0.0037,  0.1253, -0.0342, -0.0835,\n         -0.0214,  0.1301],\n        [-0.1115, -0.0885, -0.0245,  0.0244, -0.1074,  0.0478,  0.0561,  0.0051,\n         -0.2270, -0.0038],\n        [-0.2733, -0.1362, -0.2323, -0.0571, -0.0676, -0.0854, -0.1058, -0.0482,\n         -0.0263,  0.3938],\n        [-0.4562, -0.2279, -0.2335,  0.0579,  0.0978, -0.0074, -0.0589,  0.0255,\n         -0.0473,  0.0871],\n        [-0.0930, -0.1714, -0.1511, -0.0136, -0.1062,  0.1259, -0.0604,  0.0844,\n         -0.0228,  0.0939],\n        [-0.1294,  0.0080,  0.0340, -0.0254, -0.0267,  0.0038,  0.1026,  0.1158,\n         -0.0814, -0.0164],\n        [-0.1764, -0.0962, -0.1484,  0.0218,  0.0721, -0.0800, -0.0826,  0.1349,\n         -0.0989,  0.1676],\n        [-0.2004, -0.2230, -0.0427,  0.0447, -0.0762, -0.0917, -0.0062,  0.1809,\n         -0.1037,  0.1928],\n        [-0.2929, -0.2534, -0.1091, -0.0329, -0.0138, -0.1635, -0.0205,  0.0156,\n         -0.1479,  0.1341],\n        [-0.1687, -0.1202, -0.2939,  0.0753,  0.0141,  0.0345, -0.0876,  0.0314,\n         -0.0192,  0.2551],\n        [-0.2351, -0.2302, -0.0855,  0.0454,  0.0581, -0.0553, -0.0693,  0.2216,\n          0.0098,  0.0503],\n        [-0.2273, -0.1814, -0.0560, -0.0105, -0.0346,  0.1029, -0.0271,  0.0731,\n         -0.0438,  0.0445],\n        [-0.4175, -0.2831, -0.2644, -0.0321,  0.0669, -0.0268, -0.0595, -0.1627,\n         -0.0832,  0.2141],\n        [-0.3753, -0.2293, -0.1282, -0.0370, -0.0717, -0.3300, -0.1396, -0.1413,\n         -0.1413,  0.2940],\n        [-0.2416, -0.0814, -0.1249, -0.0730, -0.0350,  0.0384,  0.0699,  0.0867,\n         -0.0791,  0.0284],\n        [-0.1367,  0.0303, -0.0073, -0.0257, -0.0574,  0.0032,  0.0115, -0.0029,\n         -0.0642,  0.0773],\n        [-0.1674, -0.0635,  0.0012,  0.0094,  0.0191, -0.0537,  0.0593,  0.0561,\n         -0.0532, -0.0737],\n        [-0.4075, -0.2158, -0.3107,  0.0187, -0.0460,  0.0501,  0.0543, -0.0326,\n         -0.0234,  0.2436],\n        [-0.0549,  0.0532, -0.0097,  0.0697,  0.0296,  0.0143, -0.0721, -0.0280,\n         -0.0957,  0.1309],\n        [-0.1481, -0.0162, -0.1123,  0.1368,  0.0411,  0.0836, -0.1233,  0.0655,\n          0.0305,  0.0061],\n        [-0.2722, -0.1066, -0.3077,  0.1007, -0.0993,  0.1138, -0.0799,  0.0229,\n          0.0452,  0.2639],\n        [-0.1380, -0.0433, -0.1153,  0.0702,  0.2047, -0.0181, -0.0470,  0.0655,\n         -0.0009,  0.1519],\n        [-0.1993, -0.0925, -0.1254, -0.0391, -0.0926, -0.0164,  0.0433,  0.1821,\n         -0.1258,  0.0309],\n        [-0.3266, -0.2722, -0.1308,  0.0407,  0.0467, -0.0527, -0.0342,  0.0829,\n         -0.2123,  0.1935],\n        [-0.1523, -0.1211, -0.2933,  0.0760, -0.0618,  0.0967, -0.0195, -0.0076,\n         -0.0694,  0.2697],\n        [-0.2462, -0.0888, -0.0540, -0.0439, -0.0266,  0.0305, -0.0339,  0.1628,\n         -0.0840, -0.0465],\n        [-0.2035, -0.1796, -0.0040, -0.0126,  0.0627, -0.1576,  0.0848,  0.2609,\n         -0.1641,  0.1593],\n        [-0.0937, -0.1362, -0.0818, -0.0432, -0.0330,  0.1172,  0.0582,  0.0518,\n         -0.0149, -0.0274],\n        [-0.3272, -0.2271, -0.1429,  0.0418,  0.0349,  0.0825,  0.0103, -0.1254,\n         -0.0073,  0.2539],\n        [-0.3314, -0.2321, -0.2012,  0.0306,  0.0184, -0.1104, -0.1570,  0.0862,\n         -0.1448,  0.1734],\n        [-0.2261, -0.1164,  0.0260,  0.0793,  0.0814, -0.0390, -0.0773,  0.1157,\n         -0.0146, -0.0616],\n        [-0.0713, -0.1019, -0.0201,  0.1037,  0.0759, -0.0626, -0.1449,  0.2201,\n          0.0308,  0.0377],\n        [-0.3881, -0.1310, -0.1877,  0.0617, -0.0417, -0.0212, -0.0539, -0.1862,\n         -0.1411,  0.2793],\n        [-0.1657, -0.1845, -0.2315,  0.1648,  0.0218,  0.1181, -0.1025,  0.0387,\n          0.0357,  0.1697],\n        [-0.3148, -0.1751, -0.2544,  0.0653, -0.0122, -0.1411, -0.1423,  0.0993,\n         -0.2497,  0.2622],\n        [-0.3800, -0.0467, -0.1263,  0.0335, -0.0331, -0.0958, -0.0993,  0.0345,\n         -0.1946,  0.2218],\n        [-0.0778, -0.0502, -0.0450,  0.0210,  0.0478,  0.1050, -0.0131, -0.0460,\n         -0.0058,  0.0328],\n        [-0.2140, -0.0628, -0.1158, -0.0447, -0.0225,  0.0511,  0.0317,  0.1037,\n          0.0112,  0.0568],\n        [-0.2578, -0.1465, -0.1973,  0.0409,  0.0856,  0.0472, -0.0740, -0.0749,\n         -0.1221,  0.3102],\n        [-0.2118, -0.1813, -0.0645,  0.0655, -0.0041,  0.0328, -0.0481, -0.1305,\n         -0.1133,  0.2479],\n        [-0.1066,  0.0525, -0.0888, -0.0241,  0.0105, -0.0219, -0.0829,  0.0305,\n         -0.1180,  0.1449],\n        [-0.2543, -0.2514, -0.2532,  0.0467, -0.0038, -0.0086,  0.0306,  0.1644,\n         -0.1734,  0.1386],\n        [-0.2314, -0.0391, -0.0936, -0.0752,  0.0477,  0.0586,  0.0605, -0.0014,\n         -0.0044,  0.0634],\n        [-0.4062, -0.2900, -0.2453, -0.0452,  0.0092, -0.1033, -0.0589, -0.0477,\n         -0.0663,  0.3899],\n        [-0.2703, -0.1123, -0.1392, -0.0328, -0.0294,  0.1222, -0.0534,  0.0407,\n         -0.1044,  0.2017],\n        [-0.0992, -0.0331, -0.0524,  0.1186, -0.0202, -0.0176, -0.0584,  0.0300,\n         -0.1064,  0.0951],\n        [-0.2863, -0.1302, -0.0806,  0.0378,  0.0470,  0.0039, -0.1715,  0.1006,\n         -0.1647,  0.0468],\n        [-0.3119, -0.2141, -0.1357,  0.0611, -0.0307, -0.0735, -0.0329,  0.0276,\n         -0.1055,  0.1368],\n        [-0.1923, -0.0689, -0.1625, -0.0250,  0.0006,  0.0509, -0.0576,  0.1545,\n          0.1495,  0.1014],\n        [-0.1487, -0.0528, -0.1161, -0.0203,  0.1570,  0.0028,  0.0030,  0.1258,\n         -0.0819,  0.2180],\n        [-0.0478, -0.0938, -0.0856, -0.0261,  0.0998,  0.1634, -0.1004,  0.0847,\n          0.0446,  0.1743],\n        [-0.1088, -0.0429,  0.0093, -0.0006, -0.0330,  0.0646,  0.1095,  0.1448,\n         -0.0290, -0.0461],\n        [-0.1972, -0.0453, -0.2689,  0.0814, -0.0389, -0.0301, -0.0565,  0.1889,\n         -0.0504,  0.2306],\n        [-0.3670, -0.1575, -0.2479,  0.2176,  0.0071, -0.0017, -0.0283,  0.0523,\n         -0.0438,  0.1469],\n        [-0.2080, -0.0980, -0.0133,  0.0940,  0.1111, -0.0665, -0.1157,  0.1043,\n         -0.0823,  0.0028],\n        [-0.3246, -0.1630, -0.0913, -0.0487,  0.1411, -0.0731, -0.0288,  0.2479,\n         -0.0169,  0.1803],\n        [-0.2784, -0.1479, -0.1194, -0.1150, -0.0980, -0.1094, -0.0524,  0.2117,\n         -0.1729,  0.0222],\n        [-0.1606,  0.0130, -0.2047,  0.0839, -0.0189, -0.0888, -0.1229, -0.0779,\n         -0.1916,  0.1672],\n        [-0.0852, -0.2597, -0.1289, -0.0840,  0.0542,  0.0496,  0.0126,  0.1155,\n          0.0293, -0.0086],\n        [-0.0335, -0.0586, -0.0147,  0.0011,  0.0493,  0.0786, -0.0290,  0.0990,\n         -0.0436,  0.0747],\n        [-0.3048, -0.1326, -0.2063,  0.0131,  0.0278,  0.0092, -0.0974, -0.2638,\n         -0.1088,  0.2754],\n        [-0.2105, -0.0488, -0.1323,  0.0672,  0.0669,  0.0740,  0.0063, -0.0424,\n         -0.0143,  0.1503],\n        [-0.1386, -0.0486, -0.0536,  0.0729,  0.0721,  0.1348,  0.0016, -0.0918,\n         -0.0378, -0.0666],\n        [-0.1770, -0.0843, -0.0798, -0.0782, -0.0689,  0.0623,  0.1207,  0.1884,\n         -0.0644,  0.0060],\n        [-0.1568, -0.2566, -0.0912,  0.0182, -0.1803, -0.0661, -0.0042,  0.0445,\n         -0.1774,  0.0163],\n        [-0.1615, -0.0379, -0.0281, -0.0716, -0.0120, -0.0150,  0.0466,  0.0904,\n         -0.0902, -0.0109],\n        [-0.2829, -0.1007, -0.1329,  0.1564,  0.1239, -0.0383, -0.0895,  0.0985,\n          0.1202,  0.1110],\n        [-0.3009, -0.1791, -0.0840,  0.0062, -0.0498, -0.2111,  0.0173,  0.1175,\n         -0.1581,  0.1659],\n        [-0.1378, -0.0374, -0.0957,  0.0108,  0.0133, -0.0393, -0.1052, -0.0372,\n         -0.0900,  0.1297],\n        [-0.2119, -0.0695, -0.0700,  0.0398,  0.0203, -0.1155,  0.0160,  0.0870,\n         -0.1916,  0.2010],\n        [-0.2341, -0.1151,  0.0529, -0.0779,  0.0743, -0.1100, -0.0051, -0.0116,\n         -0.1218, -0.0869],\n        [-0.1671, -0.0069, -0.0475,  0.1160,  0.0893, -0.0750, -0.1153, -0.0413,\n         -0.0233,  0.0995],\n        [-0.4005, -0.3014, -0.3150,  0.0762, -0.1859,  0.0163, -0.0183, -0.0547,\n         -0.0065,  0.3571],\n        [-0.0971, -0.1874, -0.1236,  0.0459, -0.0029, -0.0880, -0.0824,  0.1191,\n         -0.0620,  0.1473],\n        [-0.1369, -0.2105, -0.0035, -0.0042,  0.0413, -0.0541, -0.0934,  0.0643,\n          0.0076,  0.2206],\n        [-0.2261, -0.1152, -0.2485, -0.0052,  0.0067, -0.1591, -0.1108,  0.0892,\n         -0.2212,  0.2159],\n        [-0.4895, -0.2936, -0.2702,  0.1022,  0.0632, -0.1278, -0.0639,  0.0891,\n         -0.3088,  0.1737],\n        [-0.0758,  0.1216, -0.0739,  0.0874,  0.1797, -0.0711, -0.1509,  0.0890,\n          0.0148,  0.1807],\n        [-0.1696, -0.1917, -0.2285,  0.0705, -0.0514,  0.0387, -0.0616,  0.0830,\n          0.0209,  0.0334],\n        [-0.2148, -0.0450, -0.2708,  0.0846, -0.0794,  0.0684, -0.0043,  0.0282,\n          0.0356,  0.0945],\n        [-0.1951, -0.0704, -0.0994,  0.1451, -0.0060, -0.0072, -0.1404, -0.2117,\n         -0.0228,  0.1021],\n        [-0.1235,  0.0415, -0.1801,  0.0229, -0.0132,  0.0093, -0.0460, -0.0665,\n         -0.0782,  0.1589],\n        [-0.2184, -0.2536, -0.1383,  0.1106,  0.0050, -0.0560,  0.0366,  0.0930,\n         -0.1656,  0.1147],\n        [-0.3742, -0.1615, -0.1943, -0.0822,  0.0562, -0.0185, -0.0532,  0.1155,\n         -0.0937,  0.1081],\n        [-0.2234, -0.1310, -0.1479, -0.0542, -0.0296,  0.0592,  0.0180,  0.0758,\n          0.0162,  0.0704],\n        [-0.1085, -0.0839, -0.0766,  0.0861,  0.1606,  0.0050, -0.0994,  0.1530,\n         -0.0161,  0.0504],\n        [-0.4197, -0.2795, -0.3303,  0.0665,  0.0199, -0.0749, -0.0692, -0.0036,\n         -0.0969,  0.0219],\n        [-0.3423, -0.1710, -0.2477,  0.0268, -0.1093, -0.0869, -0.0553,  0.0534,\n         -0.2753,  0.0895],\n        [-0.1620, -0.0932,  0.0141, -0.0571, -0.0346,  0.0030, -0.0070,  0.1449,\n         -0.1358, -0.0893],\n        [-0.0797, -0.0284, -0.1664,  0.0495,  0.0983, -0.0145, -0.1605, -0.0787,\n          0.0145,  0.0835],\n        [-0.3090, -0.1880, -0.2501, -0.0018, -0.1439,  0.0016, -0.0632,  0.0527,\n         -0.1558,  0.2633],\n        [-0.2704, -0.2283, -0.1715, -0.0142,  0.1878, -0.0156, -0.0792,  0.1089,\n         -0.0527,  0.1563],\n        [-0.2163, -0.1926, -0.1073,  0.0113, -0.0655, -0.0829, -0.0320, -0.0344,\n         -0.1021,  0.2694],\n        [-0.2342, -0.2537, -0.1727, -0.0111,  0.1984, -0.0985, -0.0036,  0.1906,\n         -0.0770,  0.1633]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_u"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[    nan, -2.6036, -3.7195,     nan, -2.6672,     nan, -4.7744, -1.6671,\n             nan, -2.5582],\n        [    nan, -3.5053, -1.6356,     nan, -3.8181, -2.7937, -2.0908, -3.5862,\n             nan, -4.8165],\n        [    nan, -1.9883, -2.6620,     nan, -3.9308,     nan, -1.9863, -1.9647,\n             nan,     nan],\n        [    nan, -2.0430, -1.7018,     nan,     nan,     nan, -3.7781, -2.5799,\n             nan, -4.6932],\n        [    nan, -4.0852, -1.9397,     nan,     nan,     nan, -2.4834, -2.1258,\n             nan,     nan],\n        [    nan,     nan, -3.4597, -2.3626, -5.4935,     nan, -3.6100, -2.9948,\n             nan,     nan],\n        [-2.2312, -3.9892, -1.5854,     nan, -2.6320,     nan,     nan, -1.8914,\n             nan,     nan],\n        [    nan,     nan, -1.2651,     nan, -3.1936,     nan, -1.7352, -2.9865,\n             nan,     nan],\n        [-2.9415,     nan, -2.1760,     nan, -4.9662,     nan, -1.8576, -3.1228,\n             nan,     nan],\n        [    nan, -2.4364,     nan,     nan,     nan, -5.4545, -2.4845, -2.4921,\n             nan,     nan],\n        [    nan, -2.8022, -2.2883, -4.3203, -3.3811,     nan, -2.7378, -1.9590,\n             nan, -2.7340],\n        [    nan,     nan, -2.1715,     nan, -6.2479, -3.4758, -2.9173,     nan,\n             nan,     nan],\n        [    nan, -1.5420, -2.4765,     nan,     nan,     nan, -3.6410, -2.5021,\n             nan, -2.4011],\n        [-4.3884, -3.8473,     nan,     nan,     nan,     nan, -3.3501, -3.4074,\n             nan, -2.6593],\n        [-2.7440, -4.2785,     nan, -3.4641, -1.9514,     nan, -2.9024,     nan,\n             nan,     nan],\n        [    nan, -3.3076, -1.6322,     nan,     nan,     nan, -1.4333, -3.3609,\n             nan, -2.8768],\n        [-4.6317, -3.1259, -2.3105,     nan, -4.0276,     nan,     nan, -3.2786,\n             nan,     nan],\n        [    nan, -2.2268, -1.6970,     nan, -3.0257, -3.0328, -2.1139, -2.7841,\n             nan,     nan],\n        [    nan,     nan,     nan,     nan, -2.5841,     nan, -2.2710,     nan,\n             nan,     nan],\n        [-2.5938,     nan, -2.1253,     nan, -2.3774,     nan, -2.2244,     nan,\n             nan,     nan],\n        [    nan, -5.6257, -1.4983,     nan, -3.3355,     nan, -1.5362, -4.1247,\n             nan, -3.7957],\n        [    nan, -3.3243, -1.3672,     nan,     nan,     nan, -2.8239, -5.3702,\n             nan, -5.9203],\n        [    nan,     nan, -2.2028,     nan,     nan, -3.6066, -3.9527, -2.6839,\n             nan, -4.4733],\n        [    nan, -2.4887, -2.2778, -2.5846, -2.4235,     nan, -1.3508, -2.2412,\n             nan,     nan],\n        [-6.0843, -2.1821, -1.4652,     nan, -2.9150,     nan, -3.7397, -3.6756,\n             nan,     nan],\n        [    nan, -2.4362, -3.7166, -5.6191, -1.8721,     nan, -2.0434, -1.7965,\n             nan,     nan],\n        [-3.2670, -5.8129, -1.4638, -5.7854, -5.1203,     nan, -1.7036, -5.4797,\n             nan, -4.6758],\n        [    nan, -2.7416,     nan,     nan, -2.7269,     nan,     nan, -2.2971,\n             nan, -2.9943],\n        [    nan,     nan, -1.4265,     nan, -2.6579,     nan, -1.8217, -3.5564,\n             nan, -4.2793],\n        [    nan,     nan,     nan,     nan, -2.6121,     nan, -1.8185, -2.4215,\n             nan,     nan],\n        [-4.4767, -1.9640, -1.2795,     nan, -3.5378,     nan, -1.9318, -2.5858,\n             nan, -2.9373],\n        [    nan, -2.9182, -1.9719,     nan,     nan,     nan, -1.6119, -2.9798,\n             nan, -3.5002],\n        [    nan, -1.5740,     nan,     nan,     nan,     nan, -2.0114, -3.7049,\n             nan,     nan],\n        [    nan,     nan, -1.3778,     nan, -2.8321,     nan, -2.4062, -2.6632,\n             nan,     nan],\n        [-2.6525,     nan, -1.9232,     nan, -2.5652,     nan, -1.7049, -3.7324,\n             nan,     nan],\n        [    nan, -3.7575,     nan, -2.7174, -2.0728, -3.8684, -2.1777, -2.9225,\n             nan,     nan],\n        [    nan, -2.5244, -5.0556,     nan,     nan,     nan, -2.5188, -1.8837,\n             nan,     nan],\n        [    nan, -3.4193, -2.7743,     nan, -3.3230,     nan, -2.4419, -2.7862,\n             nan,     nan],\n        [-3.0104,     nan, -1.5858,     nan, -3.3935,     nan, -0.9832,     nan,\n             nan, -4.7649],\n        [    nan, -2.2112, -4.7817,     nan, -2.2116,     nan, -2.4133, -2.2419,\n             nan,     nan],\n        [    nan,     nan, -2.5189,     nan, -3.3799,     nan, -1.9079, -2.3181,\n             nan,     nan],\n        [-2.2977,     nan, -1.3284, -2.9345,     nan,     nan, -0.9274, -2.4891,\n             nan,     nan],\n        [    nan, -1.8168, -1.6478,     nan, -3.3244, -6.8650, -8.2293, -2.6384,\n             nan, -3.0997],\n        [-3.1472,     nan, -2.5031,     nan,     nan,     nan, -1.8850,     nan,\n             nan,     nan],\n        [-2.9851, -4.5767, -1.7471,     nan, -1.6535,     nan, -2.9459, -3.4685,\n             nan,     nan],\n        [    nan,     nan, -4.4623,     nan, -2.4441,     nan, -2.7430, -4.1711,\n             nan,     nan],\n        [    nan, -2.2507, -1.3257,     nan, -4.3220,     nan, -2.7069, -2.3059,\n             nan, -2.6379],\n        [    nan, -2.7442, -1.2488, -2.8462, -2.0507,     nan, -1.1469, -1.8909,\n             nan,     nan],\n        [    nan,     nan, -1.4767,     nan, -7.6447,     nan, -2.3267, -3.7137,\n             nan,     nan],\n        [    nan, -4.2799, -1.7612,     nan, -2.8845,     nan, -1.8186,     nan,\n             nan,     nan],\n        [    nan, -1.9446, -1.8412,     nan, -3.2892,     nan, -1.9631, -4.1959,\n             nan, -2.6072],\n        [    nan, -3.3677, -1.4560,     nan,     nan,     nan, -3.6348, -2.4546,\n             nan, -3.0352],\n        [-3.4000,     nan, -1.4739,     nan,     nan,     nan, -1.7816, -2.7414,\n             nan,     nan],\n        [-4.0253, -3.4836, -2.9429,     nan, -4.0715,     nan, -1.2857,     nan,\n             nan,     nan],\n        [    nan, -2.2489, -3.8749,     nan,     nan,     nan, -2.5092, -3.0738,\n             nan, -2.2578],\n        [    nan, -3.9192, -1.8135,     nan, -2.8367,     nan, -2.6312, -2.5280,\n             nan,     nan],\n        [-2.8331, -2.4242, -1.4461,     nan, -3.0614,     nan, -3.5122, -4.0120,\n             nan,     nan],\n        [    nan, -5.6200, -2.0127,     nan, -6.3969, -3.3761, -2.6451, -6.1794,\n             nan,     nan],\n        [-2.3386,     nan, -1.5384,     nan, -4.0834,     nan, -1.7995, -3.1054,\n             nan, -2.9036],\n        [    nan, -3.3674,     nan,     nan,     nan,     nan, -1.6612, -2.4554,\n             nan,     nan],\n        [    nan, -2.4753,     nan,     nan,     nan,     nan, -3.1083, -3.9188,\n             nan,     nan],\n        [    nan, -2.3615, -2.5111,     nan,     nan,     nan, -1.6914, -2.3539,\n             nan,     nan],\n        [    nan, -4.8833, -1.4964,     nan, -2.1897,     nan, -1.9963,     nan,\n             nan,     nan],\n        [    nan, -2.3692, -1.6199,     nan,     nan,     nan,     nan, -2.8351,\n             nan,     nan],\n        [    nan,     nan, -2.0789,     nan, -2.1801, -4.2976, -2.5499, -5.0031,\n             nan,     nan],\n        [-3.4808, -4.1124, -1.8411,     nan, -3.7528,     nan, -2.9725, -4.4699,\n             nan,     nan],\n        [-2.6112, -4.6482, -2.1816,     nan, -3.1205,     nan, -1.6245,     nan,\n             nan,     nan],\n        [-2.0835, -2.3878, -3.1859,     nan,     nan,     nan, -3.9560,     nan,\n             nan,     nan],\n        [-3.8889, -4.9713, -2.8355, -5.7159, -2.5616,     nan, -1.8474,     nan,\n         -3.3431,     nan],\n        [    nan, -4.4025, -1.9692,     nan, -2.4966,     nan, -2.2693, -4.3276,\n             nan, -5.2170],\n        [    nan, -3.3938, -2.8547,     nan, -2.1148,     nan, -1.7523, -2.4627,\n             nan,     nan],\n        [    nan,     nan, -1.9759,     nan, -2.6484,     nan, -2.7473,     nan,\n             nan,     nan],\n        [-5.0173, -2.1626, -3.2054,     nan, -3.6113,     nan, -1.8692,     nan,\n             nan,     nan],\n        [-2.7098,     nan,     nan,     nan, -3.2415,     nan, -2.4044, -6.6149,\n             nan,     nan],\n        [-3.6660, -2.2133, -1.8969,     nan, -2.9793,     nan, -3.9241, -4.5121,\n             nan,     nan],\n        [    nan, -1.5180, -2.3444,     nan, -3.3032,     nan, -1.9754, -4.8942,\n             nan,     nan],\n        [    nan,     nan,     nan,     nan, -5.2821, -3.7425, -2.9192, -2.4976,\n             nan,     nan],\n        [    nan, -2.2861, -3.4871,     nan, -3.4569,     nan, -5.4358, -3.1342,\n             nan,     nan],\n        [-3.0459,     nan, -1.8673, -3.3857, -1.7990,     nan, -1.3077, -4.1169,\n             nan,     nan],\n        [-3.7581,     nan, -2.9120, -2.3504, -1.7562,     nan, -3.2717, -2.8412,\n             nan,     nan],\n        [    nan, -2.9211, -4.0643,     nan, -3.6985, -4.7248,     nan, -3.0987,\n             nan,     nan],\n        [    nan, -2.1289, -1.8655,     nan, -2.7591,     nan, -2.3712, -2.5135,\n             nan,     nan],\n        [    nan,     nan, -2.6733,     nan, -2.6474, -4.1683, -1.4241, -2.8964,\n             nan,     nan],\n        [    nan, -2.9127, -3.0024,     nan,     nan,     nan,     nan, -3.2747,\n             nan,     nan],\n        [    nan, -2.8069, -1.9924, -2.7662, -2.5264,     nan, -1.6340, -2.4261,\n             nan,     nan],\n        [-2.2407,     nan, -1.5537,     nan,     nan,     nan, -1.9689, -3.7451,\n             nan,     nan],\n        [-3.7392, -2.6422, -2.2756,     nan, -4.8483,     nan, -3.1595, -2.9417,\n             nan,     nan],\n        [-7.3012, -2.6928, -1.1664,     nan, -3.9842,     nan, -3.1499,     nan,\n             nan, -5.2800],\n        [    nan,     nan, -2.0192,     nan, -2.5440,     nan, -2.1994, -2.4857,\n             nan,     nan],\n        [    nan, -2.2937, -3.2800,     nan,     nan, -4.0166, -1.5846, -5.4499,\n             nan,     nan],\n        [    nan, -3.1199, -1.5361,     nan, -2.6519, -7.7917, -1.9951,     nan,\n             nan,     nan],\n        [    nan, -3.1370,     nan,     nan, -4.2050,     nan, -2.7482, -2.8942,\n             nan,     nan],\n        [    nan,     nan, -5.4870,     nan, -3.2080,     nan, -1.6263, -3.3694,\n             nan,     nan],\n        [-4.4021, -4.9924,     nan,     nan, -6.6352,     nan, -1.3838,     nan,\n             nan,     nan],\n        [    nan, -6.3111, -1.9119,     nan,     nan,     nan, -2.1208, -3.2227,\n             nan, -4.9806],\n        [-3.5407,     nan, -1.5032,     nan, -3.3957,     nan, -2.2165, -3.6746,\n             nan,     nan],\n        [-3.8272,     nan, -2.4577,     nan, -3.4266,     nan, -1.7941, -2.7728,\n             nan,     nan],\n        [    nan, -2.4895, -1.8133,     nan,     nan,     nan, -2.0077, -1.8651,\n             nan,     nan],\n        [-3.9935, -3.7519, -2.3352, -2.5174, -2.5873,     nan, -2.9882, -2.1463,\n             nan,     nan],\n        [    nan, -5.4167, -2.0673,     nan, -2.9995,     nan, -2.0994, -2.8446,\n             nan,     nan]], grad_fn=<LogBackward0>)"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_logvar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}