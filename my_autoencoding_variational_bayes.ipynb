{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "absolute_path = os.path.join(os.getcwd(), '/mnist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "test_dataset  = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "one_image_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # For visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def plot_image(tensor):\n",
    "\n",
    "    if tensor.shape[0] == 1:\n",
    "        tensor = tensor.squeeze(0)\n",
    "\n",
    "    image = tensor.numpy()\n",
    "\n",
    "\n",
    "    plt.imshow(image, cmap='gray' if tensor.ndim == 2 else None)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_dataset[5][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class AutoEncodingVariationalBayes(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, inter_dim = 256):\n",
    "        super(AutoEncodingVariationalBayes, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.inter_dim = inter_dim\n",
    "\n",
    "        # Appendix C.2, switch-out Z with X as described ============================\n",
    "        self.encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, inter_dim), # Here are the weights W_3 that make up phi\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.latent_u_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_4 that make up phi\n",
    "        self.latent_sigma_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_5 that make up phi\n",
    "        # ===========================================================================\n",
    "\n",
    "        # Appendix C.2, implemented exactly in this way ---------------------------------\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_layers, inter_dim), # Here are the weights W_3 that make up theta\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.output_u_prediction_head = nn.Linear(inter_dim, input_dim) # Here are the weights W_4 that make up theta\n",
    "        self.output_sigma_prediction_head = nn.Linear(inter_dim, input_dim) # Here are the weights W_5 that make up theta\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "    def encoder_forward(self, X):\n",
    "        h = self.encoder_mlp(X)\n",
    "        latent_u = self.latent_u_prediction_head(h)\n",
    "        latent_sigma = self.latent_sigma_prediction_head(h)\n",
    "\n",
    "        return latent_u, latent_sigma # These are the estimates of parameters for q_phi(Z|X) ~ N(latent_u, latent_sigma)\n",
    "\n",
    "    def decoder_forward(self, latent_u, latent_sigma, sampled_error_e = 0):\n",
    "        Z = latent_u + latent_sigma*sampled_error_e # Latent variables Z\n",
    "        # We sample from prior only during training, so during prediction Z = latent_u\n",
    "\n",
    "        h = self.decoder_mlp(Z)\n",
    "        output_u = self.output_u_prediction_head(h) # This is the main prediction of X_hat\n",
    "        output_sigma = self.output_sigma_prediction_head(h) # Not needed to reconstruct, but gives us an estimate of the variance\n",
    "\n",
    "        return output_u, output_sigma # These are the estimates of parameters for p_theta(X|Z) ~ N(output_u, output_sigma)\n",
    "\n",
    "    def sample_from_prior(self):\n",
    "        samples = torch.randn(self.hidden_layers) # This is a prior p(Z) ~ N(0,1)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, X):\n",
    "        latent_u, latent_sigma = self.encoder_forward(X)\n",
    "        sampled_error_e = self.sample_from_prior()\n",
    "        X_hat, variance = self.decoder_forward(latent_u, latent_sigma, sampled_error_e)\n",
    "        return X_hat, variance, latent_u, latent_sigma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing whether the VAE works:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6.9816e-03,  4.6622e-02,  1.4694e-02,  1.5085e-01,  9.8173e-02,\n          7.9205e-02,  1.7749e-01, -7.1100e-02,  7.4495e-02, -2.6139e-02,\n          1.7785e-01,  5.7656e-02, -2.3225e-02,  1.3569e-01,  1.6880e-01,\n          6.9139e-02, -1.1027e-02,  6.2904e-02,  8.4998e-02,  1.2277e-01,\n         -3.9947e-02,  6.3507e-02,  4.3134e-02,  4.3275e-02, -7.6757e-02,\n          9.4930e-02, -1.2220e-01,  1.5831e-01,  3.6402e-02, -1.4646e-01,\n         -1.0014e-01,  2.8663e-01,  9.1254e-02, -6.7287e-03,  1.7323e-01,\n         -1.1768e-01,  7.7146e-02, -3.8736e-02, -1.6193e-03,  2.9758e-02,\n          1.8911e-01, -1.4827e-01, -4.3165e-02,  5.6692e-02, -3.5958e-02,\n          9.8506e-04, -1.3869e-01, -3.4476e-02, -1.2253e-01, -1.1235e-02,\n         -1.9221e-02,  1.1759e-01, -2.5989e-01,  3.6550e-02,  8.0421e-02,\n         -3.4533e-02,  2.1755e-01,  3.8334e-03, -2.7188e-04, -6.4183e-02,\n          4.0544e-02,  1.3493e-02, -1.2440e-01,  1.8107e-01, -2.1135e-01,\n         -2.6120e-02, -2.1133e-01,  1.4915e-02, -1.2386e-01, -1.0198e-01,\n          1.9261e-01,  1.9985e-01, -7.1814e-03,  2.3083e-01,  2.4562e-02,\n         -1.7713e-03,  7.3194e-02,  1.8437e-02,  2.4517e-02,  1.0329e-01,\n          1.1506e-01,  1.0430e-01,  1.8350e-01, -9.2983e-02, -1.1354e-01,\n         -2.7078e-01, -1.3689e-01,  1.1296e-02,  2.3025e-01,  1.7335e-02,\n          1.3597e-01, -1.0177e-01,  1.5327e-01, -1.7487e-01,  2.1367e-01,\n          2.6976e-02, -4.2362e-03, -1.0280e-01,  1.3006e-03, -1.1493e-01,\n         -1.0660e-01, -1.0844e-01,  2.8421e-02,  5.5013e-02,  6.3208e-02,\n         -4.4031e-02,  8.0533e-02, -4.6546e-02, -1.0771e-01, -2.6886e-02,\n         -1.4556e-03, -7.6250e-02, -5.8548e-02,  3.0755e-01,  5.7657e-02,\n         -1.5714e-02, -5.0817e-02, -5.6781e-02, -5.1267e-02,  1.4264e-01,\n         -1.1461e-01,  1.9179e-01,  3.8617e-02,  6.8042e-02,  7.4374e-02,\n         -9.6234e-02,  8.3230e-02, -2.2017e-02,  1.7126e-01,  5.0573e-02,\n          1.3545e-01,  1.2603e-01,  1.5983e-02,  6.0997e-02,  2.0373e-03,\n          8.6287e-02, -1.1054e-02,  7.3240e-02,  1.1772e-01,  6.1240e-02,\n         -2.6742e-01, -5.0536e-02, -1.0419e-02,  7.1277e-02,  5.4401e-02,\n         -2.0645e-01, -7.4812e-02, -6.0693e-03,  2.0826e-01, -1.7691e-02,\n         -1.0167e-01, -1.7220e-01, -1.0752e-01,  7.6206e-03,  1.2993e-01,\n          1.9072e-02, -6.8975e-02,  4.8180e-02, -1.6901e-02,  1.7138e-01,\n          1.4702e-01, -9.2855e-02,  3.1111e-02,  6.6813e-02, -7.1496e-02,\n          8.0087e-02,  3.6631e-02, -1.4390e-01,  5.1027e-02,  9.6940e-02,\n         -1.7169e-01, -1.6454e-02,  6.0276e-02, -7.5687e-03,  1.0862e-01,\n          3.1021e-02,  2.2619e-01, -1.3254e-01, -6.2388e-02, -1.2283e-01,\n          9.7647e-02, -8.3970e-02, -1.7245e-01, -2.2299e-01,  2.2847e-02,\n          2.3736e-02, -1.8447e-01, -7.3072e-03,  2.1151e-02, -1.0373e-02,\n         -9.9238e-02,  5.6042e-02, -2.4294e-01, -2.6108e-01, -2.9391e-02,\n          1.7153e-01, -7.0220e-03,  6.6737e-02,  1.5523e-01,  9.3686e-02,\n          2.8532e-02,  2.2200e-02,  6.1162e-02, -1.0246e-02,  3.5285e-02,\n         -2.1727e-02, -2.2611e-01,  1.0927e-01,  7.2906e-02,  3.0468e-02,\n          3.6635e-02,  2.6110e-02, -1.4274e-02,  1.2939e-01,  1.5504e-01,\n          1.7306e-01,  6.5846e-02,  1.5310e-01,  2.0494e-01, -1.7327e-01,\n         -1.2650e-01,  6.9643e-02,  2.8609e-01, -1.1571e-01,  1.0270e-01,\n          4.9157e-02, -7.4593e-02, -8.0933e-03,  1.5975e-01,  1.7793e-01,\n         -8.1230e-02, -3.3298e-02, -1.1632e-01, -1.3517e-01,  7.2371e-02,\n          3.6175e-02,  1.9562e-01,  4.5521e-02, -4.0886e-02,  7.6593e-02,\n          7.7133e-02,  5.2978e-02, -1.3881e-01, -4.6530e-02, -1.9086e-01,\n         -1.0447e-01,  7.3211e-02, -1.1294e-01,  2.0671e-01,  1.0273e-01,\n          6.0937e-02,  2.9767e-03,  1.1931e-01, -7.8526e-02, -6.0748e-02,\n         -3.2292e-03,  1.8070e-01,  7.7277e-02, -8.9931e-02, -7.1881e-02,\n          9.5898e-02,  1.5035e-01, -8.2538e-02, -2.2272e-02,  1.4413e-01,\n          4.7643e-02, -2.3760e-01,  7.7201e-02,  3.5512e-01,  1.7038e-03,\n          1.6253e-02, -2.0695e-01,  1.3169e-01,  2.0350e-02,  2.7552e-02,\n         -8.1787e-02,  1.1328e-01,  1.1499e-01, -1.3453e-01, -5.3831e-03,\n         -1.3997e-01, -1.1124e-01, -6.4022e-02,  4.4372e-02, -5.1960e-02,\n         -4.1957e-02,  2.8014e-02,  1.5506e-01,  8.9006e-02,  2.1617e-01,\n         -4.4353e-02, -7.8989e-02, -1.7451e-01, -8.1905e-02, -9.7434e-02,\n         -8.9055e-03, -3.6248e-01,  5.8929e-02,  7.3380e-02, -2.3607e-01,\n         -2.3077e-02, -2.8524e-02,  6.6085e-02,  5.8401e-02, -6.8193e-02,\n         -1.9961e-01, -7.1054e-02, -5.2256e-02, -6.6547e-02,  9.9271e-03,\n          1.1528e-02,  9.8496e-02, -5.4778e-02,  9.1493e-02,  9.3552e-03,\n         -6.4010e-02, -3.1269e-02,  7.9638e-02, -2.6389e-02, -1.0855e-01,\n          4.5825e-02, -5.5346e-02, -5.6318e-02, -1.0360e-01, -1.7124e-01,\n         -3.2725e-02,  1.6231e-02,  6.0644e-02, -4.3044e-02,  9.3696e-02,\n         -9.3498e-02, -2.2445e-01,  1.4259e-01,  7.2138e-03, -1.3794e-02,\n         -1.8615e-01, -1.4144e-01,  1.0749e-01,  2.3139e-02,  2.4160e-03,\n          1.7518e-01,  6.8243e-02,  1.0116e-01, -9.2621e-02,  1.3118e-01,\n         -1.3905e-01, -6.6096e-02,  5.2698e-02, -1.9186e-02, -1.0780e-01,\n         -1.1753e-02, -1.1223e-01,  1.1259e-02, -2.3865e-02,  3.3787e-02,\n         -1.9164e-01, -1.5287e-01, -4.7992e-02,  6.0749e-02, -1.2441e-01,\n          2.5116e-01,  4.6714e-02,  2.5512e-02,  1.4264e-02, -1.5319e-02,\n         -4.4081e-02,  2.8175e-02,  8.0219e-02, -4.9467e-02, -1.7897e-03,\n         -7.3262e-02, -8.0163e-03,  2.1967e-02, -3.3435e-02, -1.0036e-01,\n         -1.1785e-01, -5.7559e-02, -8.5629e-02,  8.5399e-02,  4.7710e-02,\n         -4.9781e-02,  1.7066e-01, -1.3500e-01, -8.4813e-02, -4.3243e-02,\n         -2.0373e-02,  1.6495e-02,  3.8961e-02, -3.0772e-02, -3.8094e-02,\n         -6.9167e-03, -7.0523e-02, -8.1423e-02,  2.4432e-02,  1.3448e-01,\n          3.5355e-02,  2.5065e-02, -1.5692e-01,  4.6864e-02,  1.5360e-01,\n         -1.0284e-01,  6.9309e-02, -2.8566e-02,  6.1569e-02, -7.8837e-02,\n          1.5093e-01, -3.1920e-02,  5.5004e-02, -7.0419e-02,  6.3186e-02,\n          6.4383e-02, -1.3500e-02, -6.5470e-02, -1.4673e-01,  1.0613e-01,\n         -4.1872e-02, -7.9166e-03,  1.0858e-01,  4.5742e-02,  8.3606e-02,\n          9.8293e-02, -7.7871e-02, -1.0280e-01,  1.8896e-01,  6.2960e-02,\n         -1.5610e-01,  8.1514e-02, -2.6215e-02, -2.9538e-02,  8.9469e-02,\n         -1.6506e-02,  1.1318e-01, -8.2862e-02,  9.7683e-02, -1.5242e-01,\n         -1.0565e-01, -2.7042e-02,  1.4035e-01, -1.3515e-02,  1.4892e-02,\n         -4.6780e-02, -9.4721e-02,  7.1624e-02, -3.9651e-02, -7.7587e-03,\n         -7.4522e-02,  5.1425e-02, -2.7348e-02,  6.6440e-02,  1.3065e-01,\n         -2.2706e-01,  4.1013e-02, -1.3611e-01, -4.7826e-02,  4.6643e-03,\n         -5.8036e-02,  1.6087e-02, -9.7713e-02,  4.9351e-03,  3.8735e-02,\n          1.2799e-01,  5.2101e-02,  1.5556e-01,  1.4157e-01,  9.3806e-02,\n          9.9890e-02, -1.2136e-01,  9.6399e-02,  1.3140e-01,  1.1751e-01,\n         -3.9809e-03,  1.0823e-01,  8.3962e-02, -6.5458e-03, -1.2106e-02,\n          1.1011e-01, -2.9127e-02, -6.7810e-02,  3.5243e-02,  9.9650e-02,\n          1.0006e-01,  1.1713e-01, -2.5631e-01,  1.4347e-01,  2.0157e-01,\n         -1.0227e-01,  2.4024e-01, -1.2834e-01,  1.8124e-02, -1.4984e-01,\n         -1.0020e-01, -1.0952e-01,  1.1219e-01,  3.7541e-02, -5.8472e-02,\n          1.5452e-02, -1.2728e-01, -1.5310e-01, -2.8471e-02, -6.0768e-02,\n         -1.9460e-02,  1.9858e-02, -7.1219e-02, -1.6910e-02,  1.4155e-01,\n         -1.1871e-01,  1.0317e-01,  1.1299e-01,  6.2512e-02,  7.3416e-02,\n          1.2404e-01,  1.3414e-01, -3.7324e-02, -1.2840e-01, -5.3372e-02,\n          7.4180e-02,  1.5172e-01, -4.0626e-02, -9.4244e-02, -1.0214e-01,\n          1.1611e-01,  2.8406e-01, -1.8707e-01,  7.6923e-02, -1.4529e-01,\n         -1.9499e-02, -7.6578e-02,  1.2698e-01,  3.4269e-02, -2.4430e-01,\n          4.3635e-02,  1.7029e-01,  1.9789e-02, -1.0739e-02, -3.0179e-02,\n          5.5033e-02, -8.7755e-02,  1.3973e-01,  2.3169e-02,  2.6000e-02,\n          7.3677e-02, -1.3945e-02,  1.4956e-01, -5.8325e-02,  1.5569e-01,\n          1.2420e-01,  9.9704e-05, -4.4915e-02,  1.1077e-01,  2.2721e-01,\n         -2.5405e-02,  6.7977e-03, -1.5140e-01, -9.1123e-02, -1.1894e-02,\n          2.7412e-02,  9.8105e-02, -3.4277e-02, -1.9431e-02, -1.2804e-01,\n          3.4224e-02,  1.8879e-01, -7.4608e-02,  1.3470e-02,  1.4400e-01,\n          6.9294e-02, -6.6920e-02, -3.3658e-02, -1.7740e-01,  2.7130e-01,\n          5.8136e-02,  3.3430e-02, -1.5497e-02, -4.5839e-02, -5.5492e-02,\n          1.4845e-01,  3.7864e-02,  1.1183e-02, -5.5785e-02, -7.3278e-02,\n          3.2961e-02, -4.2159e-02, -9.2809e-02,  1.0667e-01,  2.0494e-02,\n          2.5959e-01,  1.4576e-01,  7.0053e-03,  8.2968e-03, -1.7359e-01,\n          2.6357e-02, -7.7505e-02, -5.2741e-02, -5.0469e-02, -1.0556e-01,\n         -1.2378e-02,  3.1347e-02, -1.0013e-01,  6.2875e-02,  5.9170e-03,\n          5.0178e-02, -9.2333e-02,  2.5817e-02, -1.6355e-01,  1.4596e-02,\n          1.3864e-01, -1.7896e-01, -1.4132e-01, -1.0261e-01, -4.5133e-02,\n         -4.9621e-03, -3.0771e-02,  1.5006e-01,  3.5750e-02, -4.7690e-02,\n          1.2554e-01, -3.2357e-02,  8.3435e-02, -1.2984e-01,  7.2800e-02,\n         -1.1061e-01, -1.0035e-01,  5.7575e-03, -8.2112e-02,  3.0959e-02,\n         -2.8584e-02, -8.2026e-03,  3.5801e-02, -2.2845e-01,  1.8511e-01,\n         -6.8523e-02,  7.7744e-02,  1.6843e-01,  1.7967e-02, -3.8001e-02,\n         -1.2956e-01, -2.7967e-01,  3.2015e-01,  2.0284e-02, -1.0265e-04,\n          3.8305e-02,  8.0654e-02,  6.8624e-02,  1.6546e-01,  1.1862e-01,\n         -1.1398e-02, -5.8244e-02, -3.0133e-02,  1.5843e-01, -4.0895e-02,\n          1.2977e-01,  2.5534e-02,  1.4208e-01,  1.9136e-01, -2.6191e-02,\n          3.3300e-02,  2.1888e-01,  1.1660e-02,  3.9326e-02, -9.3559e-02,\n          3.1753e-02, -1.4743e-01, -1.2703e-01,  5.1046e-02,  1.8567e-02,\n          1.4814e-01,  3.5941e-02,  4.3284e-02,  2.1324e-02,  3.3548e-02,\n         -6.7955e-02, -2.1353e-01,  9.1722e-03, -2.8789e-02, -5.9053e-02,\n         -1.5030e-02, -1.3993e-01, -8.4878e-02,  9.4517e-02,  6.2587e-03,\n         -2.7035e-02, -9.3016e-02, -1.4902e-01, -1.5225e-01, -9.1018e-02,\n         -2.8002e-01,  1.0663e-03,  9.0716e-02, -4.7230e-02, -9.3636e-02,\n          1.4369e-01,  1.0894e-01, -1.4090e-01,  4.8967e-02, -2.0914e-02,\n         -8.7410e-02,  1.0061e-01, -1.1281e-01,  2.6905e-02,  1.3118e-01,\n         -2.1265e-02,  6.6144e-02, -5.6941e-02,  3.0764e-02,  1.5215e-01,\n          2.2634e-02,  3.2880e-02, -9.3291e-02, -1.5557e-01,  8.1900e-02,\n         -6.6599e-02, -1.3068e-01,  3.3835e-02,  1.0973e-01,  1.3581e-01,\n         -5.9485e-02,  5.6673e-02,  9.2815e-02,  1.4828e-01,  1.1115e-01,\n         -8.0810e-02, -4.7184e-02,  9.8145e-02, -1.1470e-01,  7.8985e-02,\n         -2.8008e-02, -9.6003e-02, -3.0757e-02,  9.6092e-02, -1.2825e-01,\n         -2.9587e-01,  1.1402e-01, -7.0305e-02,  1.2073e-02, -7.4666e-02,\n         -3.7543e-04, -7.4572e-04, -7.6173e-02, -1.0549e-01,  7.0049e-02,\n         -3.9415e-03, -1.8684e-01,  1.9431e-02,  6.6304e-03,  2.0792e-02,\n         -7.1999e-02, -1.1429e-01,  1.3179e-01,  6.3203e-02,  4.3021e-02,\n          7.7404e-02, -1.4149e-01,  3.2404e-01, -2.1346e-01, -8.3036e-03,\n          4.1934e-02, -1.0093e-02, -2.7780e-01, -1.1721e-02,  2.0296e-01,\n          1.0010e-01, -5.2731e-02,  1.2136e-01, -9.4547e-02, -4.3181e-02,\n         -6.0657e-02,  3.3215e-02, -7.1536e-02,  1.0246e-01, -2.9702e-03,\n          1.3192e-01, -3.0498e-01, -2.3222e-01, -4.9464e-03,  1.5363e-01,\n          8.8621e-02, -1.1656e-01,  2.3021e-02,  5.8622e-02, -2.2660e-02,\n          4.8129e-02, -6.1967e-02,  2.5179e-02,  7.8642e-03]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAE = AutoEncodingVariationalBayes(784, 2)\n",
    "VAE.apply(init_weights)\n",
    "\n",
    "\n",
    "single_image, _ = next(iter(one_image_loader))\n",
    "single_image = single_image.view(single_image.size(0),-1)\n",
    "\n",
    "VAE(single_image)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(X, X_hat, sigma):\n",
    "    sigma_squared = torch.square(sigma)\n",
    "    logvar = torch.log(sigma_squared)\n",
    "    reconstruction_loss = torch.sum(-0.5 * (torch.log(2 * torch.pi) + logvar + (X - X_hat) ** 2 / sigma_squared))\n",
    "    return reconstruction_loss / X.size(0)  # Normalize by batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}