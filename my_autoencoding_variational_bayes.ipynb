{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "absolute_path = os.path.join(os.getcwd(), '/mnist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "test_dataset  = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "one_image_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # For visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def plot_image(tensor):\n",
    "\n",
    "    if tensor.shape[0] == 1:\n",
    "        tensor = tensor.squeeze(0)\n",
    "\n",
    "    image = tensor.numpy()\n",
    "\n",
    "\n",
    "    plt.imshow(image, cmap='gray' if tensor.ndim == 2 else None)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_dataset[5][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class GaussianAutoEncodingVariationalBayes(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, inter_dim = 256, classification = False):\n",
    "        super(GaussianAutoEncodingVariationalBayes, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.inter_dim = inter_dim\n",
    "\n",
    "        # Appendix C.2, switch-out Z with X as described ============================\n",
    "        self.encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, inter_dim), # Here are the weights W_3 that make up phi\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.latent_u_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_4 that make up phi\n",
    "        self.latent_variance_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_5 that make up phi\n",
    "        # ===========================================================================\n",
    "\n",
    "\n",
    "        # Appendix C.2, implemented exactly in this way ---------------------------------\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_layers, inter_dim), # Here are the weights W_3 that make up theta\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        if classification:\n",
    "            self.output_u_prediction_head = nn.Sequential(\n",
    "            nn.Linear(inter_dim, input_dim), # Here are the weights W_4 that make up theta\n",
    "            nn.Sigmoid() # Page 6 last sentence explains this part\n",
    "            )\n",
    "        else:\n",
    "            nn.Linear(inter_dim,input_dim) # Here are the weights W_4 that make up theta\n",
    "\n",
    "        self.output_variance_prediction_head = nn.Linear(inter_dim, input_dim) # Here are the weights W_5 that make up theta\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "    def encoder_forward(self, X):\n",
    "        h = self.encoder_mlp(X)\n",
    "        latent_u = self.latent_u_prediction_head(h)\n",
    "        latent_variance = self.latent_variance_prediction_head(h)\n",
    "\n",
    "        latent_logvariance = torch.log(latent_variance)\n",
    "\n",
    "        return latent_u, latent_logvariance # These are the estimates of parameters for q_phi(Z|X) ~ N(latent_u, latent_variance)\n",
    "\n",
    "    def decoder_forward(self, latent_u, latent_logvariance, sampled_error_e = 0):\n",
    "        Z = latent_u + torch.sqrt(torch.exp(latent_logvariance))*sampled_error_e # Latent variables Z\n",
    "        # We sample from prior only during training, so during prediction Z = latent_u\n",
    "\n",
    "        h = self.decoder_mlp(Z)\n",
    "        output_u = self.output_u_prediction_head(h) # This is the main prediction of X_hat\n",
    "        output_variance = self.output_variance_prediction_head(h) # Not needed to reconstruct, but gives us an estimate of the variance\n",
    "\n",
    "        output_logvariance = torch.log(output_variance)\n",
    "\n",
    "        return output_u, output_logvariance # These are the estimates of parameters for p_theta(X|Z) ~ N(output_u, output_variance)\n",
    "\n",
    "    def sample_from_prior(self):\n",
    "        samples = torch.randn(self.hidden_layers) # This is a prior p(Z) ~ N(0,1)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, X):\n",
    "        latent_u, latent_logvariance = self.encoder_forward(X)\n",
    "        sampled_error_e = self.sample_from_prior()\n",
    "        X_hat, logvariance = self.decoder_forward(latent_u, latent_logvariance, sampled_error_e)\n",
    "        return X_hat, logvariance, latent_u, latent_logvariance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing whether the VAE works:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5029, 0.5131, 0.5011, 0.5015, 0.5033, 0.5036, 0.5029, 0.5021, 0.5033,\n         0.4980, 0.5043, 0.5048, 0.5006, 0.5030, 0.5076, 0.5026, 0.5001, 0.5016,\n         0.5022, 0.5024, 0.5005, 0.4977, 0.4997, 0.4991, 0.4968, 0.5035, 0.5041,\n         0.5034, 0.5010, 0.5012, 0.5025, 0.5032, 0.5033, 0.5037, 0.5020, 0.4981,\n         0.5006, 0.5024, 0.5073, 0.5063, 0.4997, 0.5021, 0.5002, 0.5006, 0.5058,\n         0.5005, 0.5046, 0.5006, 0.5017, 0.5064, 0.5066, 0.5025, 0.5007, 0.5024,\n         0.5019, 0.5000, 0.5047, 0.5029, 0.5023, 0.5049, 0.4975, 0.5056, 0.5064,\n         0.5023, 0.5035, 0.4984, 0.5014, 0.5021, 0.4982, 0.4977, 0.5036, 0.5005,\n         0.5065, 0.4987, 0.5026, 0.5049, 0.5054, 0.4999, 0.5020, 0.5037, 0.5051,\n         0.5051, 0.5042, 0.5058, 0.5003, 0.5049, 0.4999, 0.5047, 0.4989, 0.5030,\n         0.5026, 0.5035, 0.5060, 0.5031, 0.5059, 0.5006, 0.5033, 0.5065, 0.5013,\n         0.5021, 0.4997, 0.4997, 0.5061, 0.5056, 0.4999, 0.5036, 0.5018, 0.5007,\n         0.4991, 0.5020, 0.5018, 0.5080, 0.5006, 0.5060, 0.5016, 0.5037, 0.5049,\n         0.5032, 0.5025, 0.5067, 0.5007, 0.5010, 0.5013, 0.5046, 0.5002, 0.5010,\n         0.5003, 0.5089, 0.5063, 0.5053, 0.4996, 0.5019, 0.5006, 0.5043, 0.5017,\n         0.5023, 0.5039, 0.5012, 0.5061, 0.5015, 0.5017, 0.5035, 0.5049, 0.5013,\n         0.5030, 0.5018, 0.5007, 0.5074, 0.5029, 0.5049, 0.5056, 0.5027, 0.5034,\n         0.5004, 0.5026, 0.5022, 0.5032, 0.4985, 0.5072, 0.4986, 0.5026, 0.5025,\n         0.4987, 0.5041, 0.5008, 0.5030, 0.5018, 0.4997, 0.5062, 0.4969, 0.5055,\n         0.5051, 0.5013, 0.4995, 0.5019, 0.5032, 0.5014, 0.5015, 0.5042, 0.5047,\n         0.5002, 0.5059, 0.5039, 0.5025, 0.5070, 0.5022, 0.5052, 0.5044, 0.5026,\n         0.5027, 0.4989, 0.5035, 0.5032, 0.5042, 0.5000, 0.4983, 0.5003, 0.5015,\n         0.4996, 0.4964, 0.5022, 0.5017, 0.5001, 0.5060, 0.5024, 0.5016, 0.5003,\n         0.5036, 0.5051, 0.5030, 0.5033, 0.5081, 0.5028, 0.5049, 0.5016, 0.5028,\n         0.5019, 0.5051, 0.5039, 0.5041, 0.5047, 0.5000, 0.5059, 0.5058, 0.5029,\n         0.5065, 0.5013, 0.4978, 0.5045, 0.5037, 0.5035, 0.5052, 0.5000, 0.5015,\n         0.5005, 0.5024, 0.5032, 0.4986, 0.5013, 0.5034, 0.5008, 0.5060, 0.4997,\n         0.5063, 0.5010, 0.5046, 0.5013, 0.5056, 0.4999, 0.5038, 0.5022, 0.5058,\n         0.5073, 0.5033, 0.5030, 0.5026, 0.5026, 0.5063, 0.5045, 0.5029, 0.5013,\n         0.5081, 0.5025, 0.5055, 0.5021, 0.5062, 0.5023, 0.5023, 0.4990, 0.5033,\n         0.5069, 0.5051, 0.5023, 0.5047, 0.5004, 0.5034, 0.5043, 0.5007, 0.5064,\n         0.4999, 0.5014, 0.5032, 0.5011, 0.5028, 0.5031, 0.4976, 0.5022, 0.5024,\n         0.5023, 0.5001, 0.5027, 0.5065, 0.5011, 0.4988, 0.5026, 0.4992, 0.5012,\n         0.5021, 0.5027, 0.5034, 0.5032, 0.5002, 0.5020, 0.5007, 0.5034, 0.5010,\n         0.5034, 0.5038, 0.5078, 0.5067, 0.5012, 0.5045, 0.5025, 0.4995, 0.5046,\n         0.5006, 0.5040, 0.5033, 0.4987, 0.5001, 0.5033, 0.5058, 0.5042, 0.5051,\n         0.5024, 0.4971, 0.5033, 0.5045, 0.4996, 0.5017, 0.4988, 0.5027, 0.5019,\n         0.5037, 0.5006, 0.5019, 0.5030, 0.5032, 0.5022, 0.5074, 0.5020, 0.5011,\n         0.4974, 0.5027, 0.5058, 0.5023, 0.4990, 0.5016, 0.5029, 0.5065, 0.5041,\n         0.5018, 0.5063, 0.5017, 0.4994, 0.4994, 0.5073, 0.5049, 0.5027, 0.5002,\n         0.5052, 0.5043, 0.5029, 0.5048, 0.5024, 0.5026, 0.5009, 0.4958, 0.5052,\n         0.5030, 0.4979, 0.5019, 0.5012, 0.5020, 0.5088, 0.5036, 0.5036, 0.5044,\n         0.5031, 0.4990, 0.5045, 0.5002, 0.4994, 0.5052, 0.5008, 0.5053, 0.4993,\n         0.5017, 0.5029, 0.5063, 0.5057, 0.5061, 0.5030, 0.5009, 0.5011, 0.5047,\n         0.5040, 0.5013, 0.5030, 0.5050, 0.4973, 0.4994, 0.5048, 0.5014, 0.5014,\n         0.5033, 0.4982, 0.5005, 0.5031, 0.5036, 0.5039, 0.5026, 0.5051, 0.5018,\n         0.5067, 0.5010, 0.5024, 0.5018, 0.5020, 0.5008, 0.5000, 0.5059, 0.5016,\n         0.5000, 0.5023, 0.4966, 0.5003, 0.5013, 0.5047, 0.5003, 0.5046, 0.5017,\n         0.5004, 0.5060, 0.5003, 0.5047, 0.4995, 0.4954, 0.5058, 0.5059, 0.5077,\n         0.5022, 0.5050, 0.5032, 0.5014, 0.5043, 0.5049, 0.5095, 0.5081, 0.5011,\n         0.4995, 0.5028, 0.5009, 0.5032, 0.4997, 0.5043, 0.5050, 0.4998, 0.5040,\n         0.5050, 0.5062, 0.5079, 0.5027, 0.5066, 0.5023, 0.4969, 0.5006, 0.5045,\n         0.5067, 0.4971, 0.5071, 0.5000, 0.5017, 0.4970, 0.5043, 0.5028, 0.5013,\n         0.5016, 0.5011, 0.4981, 0.5025, 0.5032, 0.4995, 0.5006, 0.5050, 0.5032,\n         0.5029, 0.4966, 0.5016, 0.4940, 0.5070, 0.4993, 0.5048, 0.5055, 0.5027,\n         0.5015, 0.5020, 0.5036, 0.5038, 0.5021, 0.5030, 0.4998, 0.5063, 0.5038,\n         0.5021, 0.5052, 0.5010, 0.5019, 0.5013, 0.5002, 0.5097, 0.5017, 0.4990,\n         0.5033, 0.4989, 0.5055, 0.5050, 0.4989, 0.5041, 0.5032, 0.4975, 0.5013,\n         0.5013, 0.5067, 0.5011, 0.5061, 0.5075, 0.5072, 0.4990, 0.5027, 0.5089,\n         0.5051, 0.5023, 0.5057, 0.5039, 0.4997, 0.5038, 0.5028, 0.5019, 0.5018,\n         0.5031, 0.5049, 0.5018, 0.5027, 0.5034, 0.4991, 0.5012, 0.5013, 0.5032,\n         0.4981, 0.5035, 0.5052, 0.4967, 0.5052, 0.5001, 0.5058, 0.5033, 0.5028,\n         0.5045, 0.5037, 0.5059, 0.5003, 0.5002, 0.5025, 0.5023, 0.5030, 0.4996,\n         0.5020, 0.5022, 0.5022, 0.5012, 0.5062, 0.5022, 0.5056, 0.5023, 0.5047,\n         0.5022, 0.5022, 0.5007, 0.5023, 0.5016, 0.5046, 0.5026, 0.5029, 0.5063,\n         0.5065, 0.5023, 0.4998, 0.5075, 0.4996, 0.5011, 0.5043, 0.5059, 0.5039,\n         0.5005, 0.5034, 0.5041, 0.4993, 0.5053, 0.5022, 0.5004, 0.5011, 0.5031,\n         0.5023, 0.5009, 0.5031, 0.5025, 0.5007, 0.5050, 0.5013, 0.4994, 0.5041,\n         0.5056, 0.5008, 0.5059, 0.5029, 0.5027, 0.4989, 0.5027, 0.5071, 0.4994,\n         0.5009, 0.5079, 0.5039, 0.5024, 0.5053, 0.5049, 0.5036, 0.5052, 0.5009,\n         0.5080, 0.5023, 0.5040, 0.5045, 0.5050, 0.5021, 0.4971, 0.5074, 0.5027,\n         0.5025, 0.5000, 0.5024, 0.5015, 0.4971, 0.5035, 0.5031, 0.5055, 0.5017,\n         0.5021, 0.5010, 0.4976, 0.5025, 0.5043, 0.5084, 0.5002, 0.5009, 0.5028,\n         0.5052, 0.4988, 0.5068, 0.5028, 0.5022, 0.5033, 0.5022, 0.5039, 0.4973,\n         0.5005, 0.5029, 0.5048, 0.5055, 0.5044, 0.4998, 0.5012, 0.4987, 0.5057,\n         0.5035, 0.5037, 0.4983, 0.5040, 0.5002, 0.5045, 0.5008, 0.5011, 0.5000,\n         0.5024, 0.5006, 0.5005, 0.5026, 0.5051, 0.5015, 0.5050, 0.4993, 0.5029,\n         0.5026, 0.5037, 0.4991, 0.5016, 0.5025, 0.5007, 0.5033, 0.5044, 0.5027,\n         0.5035, 0.5079, 0.5023, 0.5001, 0.5022, 0.5050, 0.5020, 0.5044, 0.5029,\n         0.5015, 0.5055, 0.5012, 0.5047, 0.5030, 0.5063, 0.5053, 0.5046, 0.5023,\n         0.4984, 0.4974, 0.5030, 0.5050, 0.4995, 0.5006, 0.4978, 0.5036, 0.5044,\n         0.5044, 0.5041, 0.4984, 0.4995, 0.5000, 0.5038, 0.5035, 0.5060, 0.5068,\n         0.5025, 0.5053, 0.5043, 0.5027, 0.5016, 0.5041, 0.5018, 0.5018, 0.5032,\n         0.5074, 0.4983, 0.5020, 0.5002, 0.5033, 0.4990, 0.5000, 0.5024, 0.4999,\n         0.5034, 0.5013, 0.4979, 0.5035, 0.4994, 0.5027, 0.5041, 0.5002, 0.5009,\n         0.5012, 0.5020, 0.5008, 0.5037, 0.5054, 0.5002, 0.5002, 0.5062, 0.5037,\n         0.4957, 0.5007, 0.5030, 0.4974, 0.5084, 0.5034, 0.5021, 0.5051, 0.5056,\n         0.5016]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GVAE = GaussianAutoEncodingVariationalBayes(784, 2, classification=True)\n",
    "GVAE.apply(init_weights)\n",
    "\n",
    "\n",
    "single_image, _ = next(iter(one_image_loader))\n",
    "single_image = single_image.view(single_image.size(0),-1)\n",
    "\n",
    "GVAE(single_image)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.7020,\n         0.8000, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4157,\n         0.9961, 0.9137, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686,\n         0.8353, 0.9216, 0.3686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.7412, 0.9922, 0.7922, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.1216, 0.9451, 0.8431, 0.2353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.4000, 0.9961, 0.7569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.2157, 0.9451, 0.9176, 0.1804, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0745, 0.5059, 0.9922, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.2039, 0.9333, 0.9412, 0.2118, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.1490, 0.7608, 0.8118, 0.2745, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.5294, 1.0000, 0.6824, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0196, 0.3490, 0.6118, 0.6118, 0.7608, 0.5804, 0.0706, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0157, 0.5765, 0.9451, 0.3451, 0.0000, 0.0000,\n         0.0314, 0.3137, 0.6471, 0.9922, 0.9490, 0.9137, 0.9333, 0.9922, 0.5961,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.9922, 0.6471, 0.0000, 0.0000,\n         0.0000, 0.5020, 0.9961, 0.9922, 0.6588, 0.1412, 0.0000, 0.0706, 0.8863,\n         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6549, 0.9922, 0.4118, 0.0000,\n         0.0000, 0.5529, 0.9725, 0.9569, 0.1882, 0.0235, 0.0000, 0.0000, 0.4235,\n         0.9490, 0.1882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6824, 0.9922, 0.0902,\n         0.0000, 0.1216, 0.9451, 0.9922, 0.3255, 0.0000, 0.0000, 0.0000, 0.1490,\n         0.9059, 0.7843, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3098, 0.9961, 0.4667,\n         0.0000, 0.0196, 0.7608, 0.9961, 0.2275, 0.0000, 0.0000, 0.0000, 0.2431,\n         0.9961, 0.8549, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373, 0.9922,\n         0.5765, 0.0000, 0.1373, 0.9922, 0.8000, 0.0353, 0.0000, 0.0667, 0.1961,\n         0.8941, 0.9294, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4235,\n         0.9922, 0.6078, 0.0000, 0.5765, 0.9922, 0.3412, 0.2392, 0.5412, 0.9451,\n         0.9922, 0.7647, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.1882, 0.9608, 0.9922, 0.9216, 0.9529, 0.9922, 0.9529, 0.9725, 0.8392,\n         0.6588, 0.3412, 0.0353, 0.0000, 0.2431, 0.2314, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0706, 0.4353, 0.7569, 0.8471, 0.9922, 0.9922, 0.6353, 0.3137,\n         0.0000, 0.0000, 0.0000, 0.0627, 0.5294, 0.7569, 0.0941, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian Loss function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(X, X_hat, logvar):\n",
    "    reconstruction_loss = torch.sum(-0.5 * (torch.log(2 * torch.pi) + logvar + (X - X_hat) ** 2 / torch.exp(logvar)))\n",
    "    return reconstruction_loss / X.size(0)  # Normalize by batch size\n",
    "\n",
    "\n",
    "def kullback_leiber_divergence(latent_u, latent_logvar):\n",
    "    kl_div = -0.5 * torch.sum(1 + latent_logvar - latent_u.pow(2) - latent_logvar.exp())\n",
    "    return kl_div"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GVAE = GaussianAutoEncodingVariationalBayes(784, 2, classification = True)\n",
    "GVAE.apply(init_weights)\n",
    "epochs = 5-"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}