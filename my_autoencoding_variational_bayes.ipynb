{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "absolute_path = os.path.join(os.getcwd(), '/mnist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "test_dataset  = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "one_image_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # For visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def plot_image(tensor):\n",
    "\n",
    "    if tensor.shape[0] == 1:\n",
    "        tensor = tensor.squeeze(0)\n",
    "\n",
    "    image = tensor.numpy()\n",
    "\n",
    "\n",
    "    plt.imshow(image, cmap='gray' if tensor.ndim == 2 else None)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_dataset[5][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class AutoEncodingVariationalBayes(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, inter_dim = 256):\n",
    "        super(AutoEncodingVariationalBayes, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.inter_dim = inter_dim\n",
    "\n",
    "        # Appendix C.2, switch-out Z with X as described ============================\n",
    "        self.encoder_mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, inter_dim), # Here are the weights W_3 that make up phi\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.latent_u_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_4 that make up phi\n",
    "        self.latent_variance_prediction_head = nn.Linear(inter_dim, hidden_layers) # Here are the weights W_5 that make up phi\n",
    "        # ===========================================================================\n",
    "\n",
    "        # Appendix C.2, implemented exactly in this way ---------------------------------\n",
    "        self.decoder_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_layers, inter_dim), # Here are the weights W_3 that make up theta\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.output_u_prediction_head = nn.Linear(inter_dim, input_dim) # Here are the weights W_4 that make up theta\n",
    "        self.output_variance_prediction_head = nn.Linear(inter_dim, input_dim) # Here are the weights W_5 that make up theta\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "    def encoder_forward(self, X):\n",
    "        h = self.encoder_mlp(X)\n",
    "        latent_u = self.latent_u_prediction_head(h)\n",
    "        latent_variance = self.latent_variance_prediction_head(h)\n",
    "\n",
    "        latent_logvariance = torch.log(latent_variance)\n",
    "\n",
    "        return latent_u, latent_logvariance # These are the estimates of parameters for q_phi(Z|X) ~ N(latent_u, latent_variance)\n",
    "\n",
    "    def decoder_forward(self, latent_u, latent_logvariance, sampled_error_e = 0):\n",
    "        Z = latent_u + torch.sqrt(torch.exp(latent_logvariance))*sampled_error_e # Latent variables Z\n",
    "        # We sample from prior only during training, so during prediction Z = latent_u\n",
    "\n",
    "        h = self.decoder_mlp(Z)\n",
    "        output_u = self.output_u_prediction_head(h) # This is the main prediction of X_hat\n",
    "        output_variance = self.output_variance_prediction_head(h) # Not needed to reconstruct, but gives us an estimate of the variance\n",
    "\n",
    "        output_logvariance = torch.log(output_variance)\n",
    "\n",
    "        return output_u, output_logvariance # These are the estimates of parameters for p_theta(X|Z) ~ N(output_u, output_variance)\n",
    "\n",
    "    def sample_from_prior(self):\n",
    "        samples = torch.randn(self.hidden_layers) # This is a prior p(Z) ~ N(0,1)\n",
    "        return samples\n",
    "\n",
    "    def forward(self, X):\n",
    "        latent_u, latent_logvariance = self.encoder_forward(X)\n",
    "        sampled_error_e = self.sample_from_prior()\n",
    "        X_hat, logvariance = self.decoder_forward(latent_u, latent_logvariance, sampled_error_e)\n",
    "        return X_hat, logvariance, latent_u, latent_logvariance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing whether the VAE works:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-6.4504e-02, -9.2389e-02, -8.9336e-02, -4.8080e-02, -7.8155e-02,\n          1.3013e-02, -1.7102e-02, -1.3335e-02, -1.3939e-01,  2.6205e-03,\n         -3.1015e-02, -3.4672e-02, -2.1834e-03,  3.3580e-03,  2.3290e-02,\n          2.0854e-02,  1.1169e-01, -1.4209e-01,  5.0511e-02, -3.3842e-02,\n         -6.9132e-03, -4.4042e-02, -1.2945e-01,  7.0176e-02,  4.2429e-02,\n         -9.3620e-02,  3.1199e-03,  4.2646e-02, -1.2024e-01, -6.6701e-02,\n         -1.3897e-02,  3.2461e-02, -2.9942e-02,  5.7409e-02,  1.7352e-02,\n         -7.6230e-03,  8.3385e-03,  1.0128e-01,  4.6744e-02, -1.8123e-02,\n          3.4477e-02,  4.4814e-02,  1.8115e-01, -5.3046e-02,  4.9229e-02,\n          9.8609e-02, -1.4016e-02,  2.5415e-02, -5.8195e-02,  2.9485e-02,\n         -9.2156e-02,  5.2629e-02,  3.2637e-02,  6.8712e-02,  2.7292e-02,\n         -6.2049e-02,  1.5366e-02, -9.6113e-03,  1.6409e-02, -3.9217e-02,\n         -6.7798e-03,  5.6919e-02, -4.7586e-03,  6.0651e-02,  1.6275e-02,\n          1.2315e-01,  9.4532e-03, -8.1340e-02,  8.9155e-02,  5.0572e-02,\n          1.1731e-01, -2.4302e-02, -3.7679e-02,  6.7520e-02, -7.5544e-02,\n          1.3945e-02,  1.0207e-01, -8.5191e-02,  5.7299e-02, -9.3382e-02,\n          3.8410e-03,  1.4447e-01, -2.9169e-02, -1.4533e-02, -4.5745e-03,\n         -5.8549e-02, -8.7220e-02, -7.5259e-03, -6.2071e-02,  1.4147e-01,\n          2.4688e-02, -5.1767e-02,  2.6436e-02,  1.4303e-01,  5.2331e-03,\n          7.2374e-02, -5.2227e-02,  1.2621e-01, -1.2582e-01, -3.0601e-03,\n          1.2961e-01, -3.4332e-02,  7.1433e-02, -9.0168e-03,  9.3442e-02,\n          5.0044e-02,  5.3902e-02, -1.5005e-01,  8.0686e-02, -7.1386e-02,\n          2.6950e-02,  3.9375e-02, -5.1692e-02, -3.9279e-02,  6.1628e-02,\n         -3.2881e-02,  5.0237e-02, -2.1497e-03, -3.8570e-02,  3.7990e-02,\n         -6.7095e-02,  6.7236e-03,  7.2411e-02,  6.6105e-02,  6.2119e-02,\n         -4.1808e-02,  2.9298e-02,  4.4529e-02,  6.9319e-02,  2.5188e-02,\n         -3.6849e-02, -3.7079e-02, -7.6634e-02,  4.0660e-02,  1.1013e-02,\n         -8.2728e-02, -9.3427e-02, -4.4012e-02, -6.0506e-02, -1.1626e-01,\n         -4.3674e-02,  5.4637e-02,  5.7925e-02, -1.2771e-02,  4.3160e-02,\n         -2.6389e-02, -2.8758e-02,  1.1726e-01, -4.2494e-02,  2.8309e-02,\n         -1.3048e-01, -3.8619e-02, -2.9536e-02, -1.9355e-01,  8.3169e-02,\n         -5.2444e-02, -7.2543e-02,  6.0036e-02, -3.7236e-02,  1.2354e-01,\n          7.4830e-02,  2.0442e-02,  1.1795e-01, -9.3795e-03,  4.1093e-02,\n          8.8530e-02,  1.5392e-01, -5.5205e-02,  1.8523e-03,  1.9169e-02,\n         -1.7391e-02, -7.8947e-02, -3.2145e-02,  2.1510e-02, -3.4177e-02,\n          8.1810e-02,  1.0805e-01, -2.6434e-02,  3.7517e-02,  1.6887e-02,\n         -1.2112e-02, -4.6606e-02,  7.8721e-02,  3.7603e-02,  6.5964e-02,\n         -2.2929e-02, -7.2968e-02,  1.0264e-01,  3.5703e-02, -8.5684e-02,\n          7.7929e-02,  2.3044e-02,  1.8289e-02,  1.4898e-01, -2.4525e-02,\n          8.3046e-02,  1.2632e-02, -5.1452e-02,  7.1687e-02,  1.1373e-02,\n          2.3828e-02, -3.7237e-02, -2.5118e-02,  3.9808e-02,  8.1019e-02,\n         -7.5764e-02,  7.4796e-02,  3.4362e-02, -7.0548e-03,  2.3614e-02,\n         -2.4726e-02, -1.6227e-02,  1.6771e-02,  2.5266e-03, -1.1945e-01,\n          7.9602e-02, -3.7336e-03,  4.3003e-02, -8.2416e-02,  5.1038e-02,\n         -1.7202e-02,  9.1720e-02,  5.2939e-02, -9.9825e-03, -1.4146e-01,\n          1.1528e-01,  4.5337e-02,  8.6811e-02,  9.1038e-02,  1.3504e-01,\n          1.1017e-01,  8.6709e-02,  2.4849e-02,  2.5011e-02,  5.5585e-02,\n          7.1695e-02, -3.5986e-02,  1.8864e-02, -1.6713e-02,  1.4424e-02,\n         -2.0866e-02,  3.8330e-02,  8.6938e-02,  5.3079e-02,  4.2771e-02,\n         -7.8139e-04,  1.1557e-01,  4.6090e-02,  4.9512e-02, -3.2284e-02,\n         -5.9000e-02, -1.3851e-02, -5.0908e-02,  5.7854e-02,  9.8173e-02,\n         -1.5282e-02, -1.4243e-02, -3.3485e-02, -3.4749e-02,  4.2040e-02,\n          5.5370e-02, -1.8356e-03, -3.0994e-04,  2.9001e-02, -3.4725e-02,\n         -1.2145e-01, -2.9196e-02, -5.4913e-02, -2.3326e-02,  5.7980e-02,\n          1.3103e-01,  1.7904e-02, -3.4008e-02,  7.0359e-02, -4.3307e-03,\n          3.0215e-02, -5.9731e-02,  7.5614e-02,  7.2703e-02, -2.5745e-02,\n          5.9720e-02, -8.9845e-02,  3.9136e-02, -2.8047e-04,  8.9649e-02,\n          2.3998e-02, -2.3438e-02,  1.0365e-01,  1.4632e-02, -5.7827e-02,\n         -1.0676e-01,  8.1120e-02,  5.3313e-02, -3.3263e-02, -4.7721e-03,\n          1.1377e-01, -1.6362e-01, -4.1111e-02,  6.9521e-02, -4.9632e-02,\n          5.0651e-03,  6.8419e-02,  1.8914e-02, -1.4023e-01, -6.6127e-02,\n          2.4568e-02, -1.2640e-02, -1.5714e-02, -7.6289e-02,  7.3308e-02,\n          8.8215e-02,  5.4014e-02,  8.4948e-02,  5.1233e-02,  4.9696e-04,\n         -5.3220e-02,  5.5063e-02,  1.1419e-01, -1.2758e-02,  8.8433e-02,\n         -3.0202e-02, -6.0889e-02,  1.9005e-02,  1.9847e-03,  5.4882e-02,\n          3.0792e-02, -5.3582e-02,  1.0390e-01, -5.6094e-02,  5.6889e-02,\n          5.9544e-02,  5.1751e-02, -5.5788e-03, -1.6525e-02,  5.2394e-02,\n         -1.5183e-01, -8.8639e-02,  2.0974e-02, -2.8618e-02, -6.5153e-02,\n         -7.1505e-02,  1.0896e-03,  6.1530e-03,  1.1055e-01, -8.7747e-02,\n          6.2582e-02,  3.7978e-03,  2.8764e-02, -3.3642e-02,  4.1212e-02,\n         -6.4095e-02,  6.5190e-02, -4.8263e-02, -2.5381e-02,  1.9636e-02,\n          2.5679e-02, -1.1657e-02,  5.4304e-02,  8.7621e-02,  1.5929e-02,\n          3.2325e-02,  2.4293e-02,  1.5264e-02,  8.2105e-02,  1.4756e-01,\n          1.0100e-01,  1.1216e-01,  2.6421e-02, -2.7209e-02,  3.1187e-02,\n          2.5668e-02,  4.7650e-02, -1.5284e-01,  1.0653e-01, -4.1265e-02,\n         -8.9632e-03,  6.9575e-02,  4.4160e-02,  3.1603e-03,  6.1884e-02,\n          5.7885e-03, -9.9846e-03, -2.0542e-02,  1.3466e-02,  1.6046e-01,\n         -1.8401e-02, -3.8648e-02,  5.9183e-02,  1.0840e-01, -6.7982e-02,\n         -4.9810e-02, -2.3006e-02, -1.6367e-01, -4.9795e-02,  3.7882e-02,\n         -8.9023e-03,  8.1709e-02,  6.2573e-02, -2.7283e-02, -4.7259e-02,\n         -5.6775e-02,  4.9874e-02,  5.1400e-02, -9.7957e-02,  3.2993e-02,\n         -3.5350e-02,  4.6246e-02,  8.1082e-03,  4.9061e-02, -7.1421e-02,\n          3.8409e-02, -4.2239e-02,  3.8803e-02,  3.6518e-02, -7.8764e-02,\n          3.5464e-02,  8.8192e-02,  7.4870e-02,  4.8796e-02, -3.7044e-02,\n          6.8700e-03, -9.4158e-02, -5.3579e-02,  1.2788e-01, -3.9821e-02,\n         -4.9105e-02, -4.7823e-02, -2.6624e-02,  2.7245e-02, -1.0479e-01,\n          2.2199e-02, -5.0517e-02, -1.4261e-02, -7.1612e-02,  4.8795e-02,\n          1.5369e-02,  6.1468e-02,  1.9040e-02,  2.8714e-02, -9.7603e-02,\n          3.3216e-02,  1.0093e-01,  2.7578e-04, -2.5074e-02, -1.2322e-02,\n          1.1282e-01,  4.5669e-04,  1.4737e-01,  6.7478e-02, -4.0162e-02,\n          6.7239e-02, -2.0615e-02, -1.6944e-02,  8.6788e-03,  4.9568e-03,\n          5.3259e-02, -4.6179e-02,  1.2881e-01,  1.4569e-02,  5.3096e-02,\n          3.8248e-02,  1.7309e-02,  1.7223e-02,  4.3043e-02,  4.9218e-03,\n         -1.4894e-02, -4.1314e-02,  1.0667e-02, -2.6534e-02,  3.8040e-02,\n          1.2748e-01, -8.5322e-02,  4.6962e-02,  7.0052e-02, -8.5109e-02,\n          5.0141e-02, -5.7811e-02,  3.4837e-02,  6.4123e-03, -3.9847e-03,\n          9.9664e-02, -5.6790e-02,  2.3742e-02, -5.4592e-02,  1.0047e-01,\n         -1.5182e-03, -5.2662e-02, -6.9125e-02, -3.3352e-02, -3.8799e-02,\n          1.3826e-02, -1.5910e-02,  3.3554e-02,  7.0901e-02, -7.9861e-02,\n          6.6778e-02, -6.2293e-02, -1.4011e-02,  5.7097e-02,  9.5872e-02,\n         -7.5360e-02,  3.4715e-02,  2.6244e-02,  8.8672e-02, -4.7291e-02,\n          4.0069e-02,  2.8269e-02,  5.4012e-02,  2.0943e-03, -9.2377e-02,\n         -1.7387e-01,  1.6079e-02,  8.8562e-02,  7.8586e-02, -1.5657e-01,\n         -2.7679e-02, -2.5136e-02,  7.2705e-02,  6.0366e-02,  6.9580e-03,\n          3.9297e-02,  3.7443e-02,  1.0757e-03,  1.0723e-01, -1.0129e-01,\n          4.8941e-02, -6.5750e-02,  5.4138e-03,  3.3249e-02, -6.1594e-03,\n          2.6558e-02,  5.0971e-02,  1.6254e-02,  4.5968e-03, -2.3624e-02,\n          1.3835e-03, -7.2311e-02,  1.1839e-01,  5.2784e-02,  3.5775e-02,\n          6.4791e-02, -5.6723e-02, -1.4400e-01,  1.6570e-02,  6.9160e-02,\n          3.9897e-02, -9.0736e-02, -4.6222e-02,  6.9013e-02,  3.1841e-02,\n         -8.5669e-02,  7.5923e-02, -2.5268e-02,  2.7279e-02,  1.0894e-01,\n          4.8152e-02, -8.6574e-02, -1.6427e-02,  3.6283e-03, -1.7451e-03,\n          6.4875e-02, -7.3970e-02,  6.5048e-02, -6.5520e-02,  4.8325e-02,\n         -8.9144e-02, -9.0819e-02,  1.3916e-01,  4.8390e-02,  8.3948e-02,\n          7.4893e-03,  3.1710e-02,  1.1691e-01, -8.4423e-02, -1.8645e-02,\n          6.6241e-02, -7.4266e-02,  5.6112e-04, -9.4958e-03,  5.4471e-04,\n          1.9602e-02,  9.6173e-02, -1.3657e-01,  3.3142e-02,  6.1816e-02,\n          1.0609e-01,  1.0449e-01,  1.5314e-01,  7.0720e-02,  1.0317e-01,\n          6.2531e-02, -7.6163e-02,  1.2464e-01,  6.7323e-02, -1.1054e-01,\n         -1.0860e-02,  3.9506e-02,  7.1134e-02,  7.6454e-02, -1.3299e-02,\n          3.3043e-02, -3.7104e-02, -4.5868e-02,  5.1893e-02, -2.0266e-02,\n         -1.5540e-02, -2.0293e-02, -2.5371e-02, -2.7730e-02,  1.4807e-01,\n         -2.4450e-02,  7.6356e-02,  5.0151e-02,  3.9110e-02,  6.8271e-02,\n         -9.6505e-02,  1.5432e-02,  7.4725e-02,  4.0545e-02, -5.4527e-02,\n         -8.2266e-02,  6.8573e-02,  3.5986e-02, -1.4119e-03, -3.1729e-02,\n         -4.4009e-02, -4.8978e-03,  1.3427e-01, -7.0196e-02,  1.6201e-01,\n          8.3506e-02, -1.6127e-02, -2.4681e-02,  1.7056e-01, -4.5582e-03,\n          3.7961e-02, -7.9776e-02, -2.8209e-04,  8.8465e-02, -7.8661e-03,\n          3.2890e-03,  6.3523e-04,  9.7170e-02, -6.0237e-02, -1.5665e-01,\n         -3.8631e-02,  9.1691e-02,  7.3634e-02, -1.0276e-02,  2.6382e-02,\n          5.4855e-02,  1.0022e-02, -7.3911e-02,  4.8561e-02,  1.0100e-01,\n          2.6078e-02, -2.8804e-02, -2.2988e-02, -5.7229e-02,  8.1122e-02,\n          6.9687e-02,  9.7108e-02, -1.5583e-02,  1.3436e-02,  2.2699e-02,\n          3.3605e-02,  4.5224e-02, -1.9691e-02,  1.3331e-01,  7.5731e-02,\n         -8.1910e-02,  9.1373e-02, -2.9475e-02, -7.7049e-02, -1.5321e-02,\n          1.5839e-02,  4.1745e-02,  4.0567e-02, -3.4164e-02, -2.3298e-02,\n         -1.6775e-03, -1.1524e-01, -2.2033e-02,  5.4910e-02,  4.8358e-02,\n          5.2157e-02,  8.0906e-02,  1.1856e-01,  1.1908e-01,  2.4698e-02,\n         -2.4660e-03, -8.5253e-02, -2.0408e-02, -1.4958e-01, -3.1173e-02,\n         -5.6152e-02, -4.4152e-02,  5.6754e-02, -1.5593e-01,  1.2027e-01,\n          8.1880e-02,  6.4377e-02,  1.1395e-01,  3.3844e-02, -2.3418e-03,\n         -3.3213e-02,  3.5051e-02,  5.3470e-02,  5.6901e-02, -6.3700e-02,\n          2.1626e-02,  1.0834e-02,  7.8050e-02, -8.1946e-02, -1.4912e-02,\n          3.2956e-02,  7.1643e-02, -2.9523e-02,  1.1793e-02, -3.1443e-02,\n          6.3659e-02, -4.0627e-02, -2.7449e-02, -4.4547e-05, -7.9346e-02,\n         -1.0053e-01,  1.5550e-02,  8.3426e-02,  8.7462e-03,  5.2880e-02,\n         -3.4639e-02, -3.1805e-02,  7.4554e-03,  8.2835e-03, -2.6467e-02,\n         -5.3385e-02,  2.3287e-02,  1.4247e-02, -3.7403e-03,  7.4360e-02,\n          9.6401e-02,  9.6363e-03, -8.4446e-02,  5.4561e-02,  2.4742e-02,\n         -6.1200e-02,  3.4092e-03, -4.0836e-02, -4.0417e-02, -1.2524e-02,\n          4.6494e-02, -1.3569e-01, -3.3249e-02,  6.8538e-02,  1.3175e-03,\n          4.9731e-02, -4.6838e-02,  3.7631e-02, -4.0086e-02,  1.1055e-02,\n         -4.4394e-02,  1.7413e-01, -4.7415e-02,  1.2622e-01, -1.5590e-02,\n         -1.0311e-01,  5.2099e-03, -8.3640e-02, -1.4779e-02, -7.6850e-02,\n          3.2266e-02, -6.8703e-02, -1.1472e-02, -6.7315e-02,  8.0811e-03,\n          4.4947e-02,  5.2985e-02, -1.3228e-02,  4.0489e-03,  5.5716e-02,\n         -7.1382e-02, -4.7111e-02,  1.2939e-01, -7.3879e-03]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAE = AutoEncodingVariationalBayes(784, 2)\n",
    "VAE.apply(init_weights)\n",
    "\n",
    "\n",
    "single_image, _ = next(iter(one_image_loader))\n",
    "single_image = single_image.view(single_image.size(0),-1)\n",
    "\n",
    "VAE(single_image)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(X, X_hat, logvar):\n",
    "    reconstruction_loss = torch.sum(-0.5 * (torch.log(2 * torch.pi) + logvar + (X - X_hat) ** 2 / torch.exp(logvar)))\n",
    "    return reconstruction_loss / X.size(0)  # Normalize by batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def kullback_leiber_divergence(latent_u, latent_logvar):\n",
    "    kl_div = -0.5 * torch.sum(1 + latent_logvar - latent_u.pow(2) - latent_logvar.exp())\n",
    "    return kl_div"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}