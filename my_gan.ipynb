{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "absolute_path = os.path.join(os.getcwd(), '/mnist')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "test_dataset  = MNIST(os.getcwd(), transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "one_image_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) # For visualising"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels, leaky_relu_parameter = 0.1):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.leaky_relu_parameter = leaky_relu_parameter\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64*7*7),\n",
    "            nn.BatchNorm1d(64*7*7),\n",
    "            nn.LeakyReLU(self.leaky_relu_parameter),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(self.leaky_relu_parameter),\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "generator = Generator(64, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "random_vars = torch.randn(64).unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 28, 28])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.eval()\n",
    "generator(random_vars).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discriminator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1), # 28x28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 2), #14x14\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 2), #7x7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discriminator = Discriminator(2)\n",
    "discriminator.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "example_img = next(iter(one_image_loader))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discriminator(example_img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "generator = Generator(64, 1)\n",
    "discriminator = Discriminator(output_dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator.apply(initialize_weights)\n",
    "discriminator.apply(initialize_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/1875], D Loss: 1.4377, G Loss: 0.7539\n",
      "Epoch [1/50], Step [200/1875], D Loss: 1.3946, G Loss: 0.7458\n",
      "Epoch [1/50], Step [300/1875], D Loss: 1.3462, G Loss: 0.7564\n",
      "Epoch [1/50], Step [400/1875], D Loss: 1.3158, G Loss: 0.7722\n",
      "Epoch [1/50], Step [500/1875], D Loss: 1.2724, G Loss: 0.8109\n",
      "Epoch [1/50], Step [600/1875], D Loss: 1.2743, G Loss: 0.8314\n",
      "Epoch [1/50], Step [700/1875], D Loss: 1.2585, G Loss: 0.8436\n",
      "Epoch [1/50], Step [800/1875], D Loss: 1.2764, G Loss: 0.8803\n",
      "Epoch [1/50], Step [900/1875], D Loss: 1.2027, G Loss: 0.8632\n",
      "Epoch [1/50], Step [1000/1875], D Loss: 1.1746, G Loss: 0.8886\n",
      "Epoch [1/50], Step [1100/1875], D Loss: 1.1172, G Loss: 0.9249\n",
      "Epoch [1/50], Step [1200/1875], D Loss: 1.1797, G Loss: 0.9159\n",
      "Epoch [1/50], Step [1300/1875], D Loss: 1.1305, G Loss: 0.9786\n",
      "Epoch [1/50], Step [1400/1875], D Loss: 1.1118, G Loss: 0.9885\n",
      "Epoch [1/50], Step [1500/1875], D Loss: 1.0294, G Loss: 1.0222\n",
      "Epoch [1/50], Step [1600/1875], D Loss: 1.0553, G Loss: 1.0645\n",
      "Epoch [1/50], Step [1700/1875], D Loss: 1.0102, G Loss: 1.0358\n",
      "Epoch [1/50], Step [1800/1875], D Loss: 0.9707, G Loss: 1.1398\n",
      "Epoch [2/50], Step [100/1875], D Loss: 0.9320, G Loss: 1.1941\n",
      "Epoch [2/50], Step [200/1875], D Loss: 0.8878, G Loss: 1.1934\n",
      "Epoch [2/50], Step [300/1875], D Loss: 0.8795, G Loss: 1.3100\n",
      "Epoch [2/50], Step [400/1875], D Loss: 0.8591, G Loss: 1.2596\n",
      "Epoch [2/50], Step [500/1875], D Loss: 0.8351, G Loss: 1.3133\n",
      "Epoch [2/50], Step [600/1875], D Loss: 0.7749, G Loss: 1.3717\n",
      "Epoch [2/50], Step [700/1875], D Loss: 0.8044, G Loss: 1.3755\n",
      "Epoch [2/50], Step [800/1875], D Loss: 0.7600, G Loss: 1.4455\n",
      "Epoch [2/50], Step [900/1875], D Loss: 0.7846, G Loss: 1.4289\n",
      "Epoch [2/50], Step [1000/1875], D Loss: 0.7623, G Loss: 1.5048\n",
      "Epoch [2/50], Step [1100/1875], D Loss: 0.8275, G Loss: 1.4994\n",
      "Epoch [2/50], Step [1200/1875], D Loss: 0.7568, G Loss: 1.6158\n",
      "Epoch [2/50], Step [1300/1875], D Loss: 0.7891, G Loss: 1.4525\n",
      "Epoch [2/50], Step [1400/1875], D Loss: 0.7499, G Loss: 1.6468\n",
      "Epoch [2/50], Step [1500/1875], D Loss: 0.7435, G Loss: 1.5804\n",
      "Epoch [2/50], Step [1600/1875], D Loss: 0.8126, G Loss: 1.6618\n",
      "Epoch [2/50], Step [1700/1875], D Loss: 0.7080, G Loss: 1.6777\n",
      "Epoch [2/50], Step [1800/1875], D Loss: 0.7884, G Loss: 1.6647\n",
      "Epoch [3/50], Step [100/1875], D Loss: 0.7296, G Loss: 1.8215\n",
      "Epoch [3/50], Step [200/1875], D Loss: 0.6890, G Loss: 1.8468\n",
      "Epoch [3/50], Step [300/1875], D Loss: 0.7271, G Loss: 1.7612\n",
      "Epoch [3/50], Step [400/1875], D Loss: 0.7003, G Loss: 1.7935\n",
      "Epoch [3/50], Step [500/1875], D Loss: 0.8114, G Loss: 1.8556\n",
      "Epoch [3/50], Step [600/1875], D Loss: 0.6965, G Loss: 1.8889\n",
      "Epoch [3/50], Step [700/1875], D Loss: 0.7062, G Loss: 1.8424\n",
      "Epoch [3/50], Step [800/1875], D Loss: 0.7248, G Loss: 1.8811\n",
      "Epoch [3/50], Step [900/1875], D Loss: 0.7095, G Loss: 1.9016\n",
      "Epoch [3/50], Step [1000/1875], D Loss: 0.7034, G Loss: 1.8222\n",
      "Epoch [3/50], Step [1100/1875], D Loss: 0.6823, G Loss: 1.8837\n",
      "Epoch [3/50], Step [1200/1875], D Loss: 0.6949, G Loss: 1.9356\n",
      "Epoch [3/50], Step [1300/1875], D Loss: 0.7145, G Loss: 1.9151\n",
      "Epoch [3/50], Step [1400/1875], D Loss: 0.7062, G Loss: 1.9007\n",
      "Epoch [3/50], Step [1500/1875], D Loss: 0.7125, G Loss: 1.7559\n",
      "Epoch [3/50], Step [1600/1875], D Loss: 0.7086, G Loss: 1.8646\n",
      "Epoch [3/50], Step [1700/1875], D Loss: 0.6985, G Loss: 1.8297\n",
      "Epoch [3/50], Step [1800/1875], D Loss: 0.6791, G Loss: 1.8672\n",
      "Epoch [4/50], Step [100/1875], D Loss: 0.6872, G Loss: 1.9565\n",
      "Epoch [4/50], Step [200/1875], D Loss: 0.6798, G Loss: 1.9211\n",
      "Epoch [4/50], Step [300/1875], D Loss: 0.6731, G Loss: 1.9301\n",
      "Epoch [4/50], Step [400/1875], D Loss: 0.6725, G Loss: 1.8732\n",
      "Epoch [4/50], Step [500/1875], D Loss: 0.7066, G Loss: 1.9704\n",
      "Epoch [4/50], Step [600/1875], D Loss: 0.6721, G Loss: 1.9330\n",
      "Epoch [4/50], Step [700/1875], D Loss: 0.6767, G Loss: 1.9895\n",
      "Epoch [4/50], Step [800/1875], D Loss: 0.6858, G Loss: 1.9487\n",
      "Epoch [4/50], Step [900/1875], D Loss: 0.6724, G Loss: 1.9247\n",
      "Epoch [4/50], Step [1000/1875], D Loss: 0.6740, G Loss: 1.8774\n",
      "Epoch [4/50], Step [1100/1875], D Loss: 0.6790, G Loss: 1.9775\n",
      "Epoch [4/50], Step [1200/1875], D Loss: 0.6752, G Loss: 2.0917\n",
      "Epoch [4/50], Step [1300/1875], D Loss: 0.6886, G Loss: 2.1628\n",
      "Epoch [4/50], Step [1400/1875], D Loss: 0.7176, G Loss: 1.9263\n",
      "Epoch [4/50], Step [1500/1875], D Loss: 0.6838, G Loss: 1.9118\n",
      "Epoch [4/50], Step [1600/1875], D Loss: 0.6813, G Loss: 1.8536\n",
      "Epoch [4/50], Step [1700/1875], D Loss: 0.7068, G Loss: 1.9107\n",
      "Epoch [4/50], Step [1800/1875], D Loss: 0.7004, G Loss: 2.2299\n",
      "Epoch [5/50], Step [100/1875], D Loss: 0.6857, G Loss: 2.0257\n",
      "Epoch [5/50], Step [200/1875], D Loss: 0.6826, G Loss: 1.8762\n",
      "Epoch [5/50], Step [300/1875], D Loss: 0.6987, G Loss: 1.7653\n",
      "Epoch [5/50], Step [400/1875], D Loss: 0.6751, G Loss: 1.9817\n",
      "Epoch [5/50], Step [500/1875], D Loss: 0.6756, G Loss: 2.1303\n",
      "Epoch [5/50], Step [600/1875], D Loss: 0.6823, G Loss: 1.9895\n",
      "Epoch [5/50], Step [700/1875], D Loss: 0.6889, G Loss: 2.1816\n",
      "Epoch [5/50], Step [800/1875], D Loss: 0.7068, G Loss: 1.9641\n",
      "Epoch [5/50], Step [900/1875], D Loss: 0.6912, G Loss: 1.9729\n",
      "Epoch [5/50], Step [1000/1875], D Loss: 0.7159, G Loss: 2.0060\n",
      "Epoch [5/50], Step [1100/1875], D Loss: 0.6862, G Loss: 2.0507\n",
      "Epoch [5/50], Step [1200/1875], D Loss: 0.6762, G Loss: 1.9404\n",
      "Epoch [5/50], Step [1300/1875], D Loss: 0.6718, G Loss: 2.0089\n",
      "Epoch [5/50], Step [1400/1875], D Loss: 0.6907, G Loss: 1.8140\n",
      "Epoch [5/50], Step [1500/1875], D Loss: 0.6755, G Loss: 2.0331\n",
      "Epoch [5/50], Step [1600/1875], D Loss: 0.6711, G Loss: 1.8883\n",
      "Epoch [5/50], Step [1700/1875], D Loss: 0.6845, G Loss: 1.8916\n",
      "Epoch [5/50], Step [1800/1875], D Loss: 0.6600, G Loss: 2.0263\n",
      "Epoch [6/50], Step [100/1875], D Loss: 0.6684, G Loss: 1.9136\n",
      "Epoch [6/50], Step [200/1875], D Loss: 0.6609, G Loss: 2.0478\n",
      "Epoch [6/50], Step [300/1875], D Loss: 0.6721, G Loss: 1.8419\n",
      "Epoch [6/50], Step [400/1875], D Loss: 0.6595, G Loss: 2.0850\n",
      "Epoch [6/50], Step [500/1875], D Loss: 0.6597, G Loss: 1.9201\n",
      "Epoch [6/50], Step [600/1875], D Loss: 0.6654, G Loss: 1.9150\n",
      "Epoch [6/50], Step [700/1875], D Loss: 0.6628, G Loss: 2.0579\n",
      "Epoch [6/50], Step [800/1875], D Loss: 0.6574, G Loss: 2.0287\n",
      "Epoch [6/50], Step [900/1875], D Loss: 0.6597, G Loss: 1.9607\n",
      "Epoch [6/50], Step [1000/1875], D Loss: 0.6585, G Loss: 2.0192\n",
      "Epoch [6/50], Step [1100/1875], D Loss: 0.6658, G Loss: 1.9818\n",
      "Epoch [6/50], Step [1200/1875], D Loss: 0.6605, G Loss: 2.0251\n",
      "Epoch [6/50], Step [1300/1875], D Loss: 0.6945, G Loss: 1.9647\n",
      "Epoch [6/50], Step [1400/1875], D Loss: 0.6719, G Loss: 1.9987\n",
      "Epoch [6/50], Step [1500/1875], D Loss: 0.6610, G Loss: 2.0394\n",
      "Epoch [6/50], Step [1600/1875], D Loss: 0.6610, G Loss: 2.0329\n",
      "Epoch [6/50], Step [1700/1875], D Loss: 0.6666, G Loss: 2.1372\n",
      "Epoch [6/50], Step [1800/1875], D Loss: 0.6729, G Loss: 1.9029\n",
      "Epoch [7/50], Step [100/1875], D Loss: 0.6754, G Loss: 1.9175\n",
      "Epoch [7/50], Step [200/1875], D Loss: 0.6587, G Loss: 1.9656\n",
      "Epoch [7/50], Step [300/1875], D Loss: 0.6605, G Loss: 2.1134\n",
      "Epoch [7/50], Step [400/1875], D Loss: 0.6647, G Loss: 2.0245\n",
      "Epoch [7/50], Step [500/1875], D Loss: 0.6754, G Loss: 2.0376\n",
      "Epoch [7/50], Step [600/1875], D Loss: 0.7549, G Loss: 2.1978\n",
      "Epoch [7/50], Step [700/1875], D Loss: 0.6677, G Loss: 1.8129\n",
      "Epoch [7/50], Step [800/1875], D Loss: 0.6717, G Loss: 1.9125\n",
      "Epoch [7/50], Step [900/1875], D Loss: 0.6630, G Loss: 2.1184\n",
      "Epoch [7/50], Step [1000/1875], D Loss: 0.7050, G Loss: 2.1729\n",
      "Epoch [7/50], Step [1100/1875], D Loss: 0.6724, G Loss: 1.9246\n",
      "Epoch [7/50], Step [1200/1875], D Loss: 0.6936, G Loss: 2.1441\n",
      "Epoch [7/50], Step [1300/1875], D Loss: 0.6688, G Loss: 2.1126\n",
      "Epoch [7/50], Step [1400/1875], D Loss: 0.6618, G Loss: 2.0154\n",
      "Epoch [7/50], Step [1500/1875], D Loss: 0.6581, G Loss: 1.9576\n",
      "Epoch [7/50], Step [1600/1875], D Loss: 0.6599, G Loss: 2.0082\n",
      "Epoch [7/50], Step [1700/1875], D Loss: 0.6589, G Loss: 1.9236\n",
      "Epoch [7/50], Step [1800/1875], D Loss: 0.6791, G Loss: 2.0407\n",
      "Epoch [8/50], Step [100/1875], D Loss: 0.6648, G Loss: 2.0495\n",
      "Epoch [8/50], Step [200/1875], D Loss: 0.6581, G Loss: 1.9706\n",
      "Epoch [8/50], Step [300/1875], D Loss: 0.6579, G Loss: 2.0345\n",
      "Epoch [8/50], Step [400/1875], D Loss: 0.6563, G Loss: 2.0135\n",
      "Epoch [8/50], Step [500/1875], D Loss: 0.6598, G Loss: 2.0243\n",
      "Epoch [8/50], Step [600/1875], D Loss: 0.6670, G Loss: 2.1035\n",
      "Epoch [8/50], Step [700/1875], D Loss: 0.6560, G Loss: 2.1201\n",
      "Epoch [8/50], Step [800/1875], D Loss: 0.6541, G Loss: 2.0404\n",
      "Epoch [8/50], Step [900/1875], D Loss: 0.6635, G Loss: 2.0371\n",
      "Epoch [8/50], Step [1000/1875], D Loss: 0.6798, G Loss: 2.1328\n",
      "Epoch [8/50], Step [1100/1875], D Loss: 0.6576, G Loss: 2.0703\n",
      "Epoch [8/50], Step [1200/1875], D Loss: 0.6616, G Loss: 2.1169\n",
      "Epoch [8/50], Step [1300/1875], D Loss: 0.6647, G Loss: 2.0569\n",
      "Epoch [8/50], Step [1400/1875], D Loss: 0.6728, G Loss: 2.0816\n",
      "Epoch [8/50], Step [1500/1875], D Loss: 0.6622, G Loss: 2.1414\n",
      "Epoch [8/50], Step [1600/1875], D Loss: 0.6624, G Loss: 2.0555\n",
      "Epoch [8/50], Step [1700/1875], D Loss: 0.6967, G Loss: 2.1829\n",
      "Epoch [8/50], Step [1800/1875], D Loss: 0.6810, G Loss: 1.9492\n",
      "Epoch [9/50], Step [100/1875], D Loss: 0.6708, G Loss: 2.1043\n",
      "Epoch [9/50], Step [200/1875], D Loss: 0.6616, G Loss: 1.8288\n",
      "Epoch [9/50], Step [300/1875], D Loss: 0.6700, G Loss: 1.7959\n",
      "Epoch [9/50], Step [400/1875], D Loss: 0.6633, G Loss: 1.9309\n",
      "Epoch [9/50], Step [500/1875], D Loss: 0.7031, G Loss: 1.8301\n",
      "Epoch [9/50], Step [600/1875], D Loss: 0.6638, G Loss: 1.8790\n",
      "Epoch [9/50], Step [700/1875], D Loss: 0.6589, G Loss: 2.0130\n",
      "Epoch [9/50], Step [800/1875], D Loss: 0.6651, G Loss: 1.9992\n",
      "Epoch [9/50], Step [900/1875], D Loss: 0.6835, G Loss: 2.1295\n",
      "Epoch [9/50], Step [1000/1875], D Loss: 0.6791, G Loss: 2.0159\n",
      "Epoch [9/50], Step [1100/1875], D Loss: 0.6565, G Loss: 2.0865\n",
      "Epoch [9/50], Step [1200/1875], D Loss: 0.6593, G Loss: 2.0287\n",
      "Epoch [9/50], Step [1300/1875], D Loss: 0.6579, G Loss: 2.0421\n",
      "Epoch [9/50], Step [1400/1875], D Loss: 0.6622, G Loss: 1.9303\n",
      "Epoch [9/50], Step [1500/1875], D Loss: 0.6654, G Loss: 2.0233\n",
      "Epoch [9/50], Step [1600/1875], D Loss: 0.6566, G Loss: 2.0452\n",
      "Epoch [9/50], Step [1700/1875], D Loss: 0.6677, G Loss: 2.0102\n",
      "Epoch [9/50], Step [1800/1875], D Loss: 0.6584, G Loss: 2.0027\n",
      "Epoch [10/50], Step [100/1875], D Loss: 0.6604, G Loss: 2.0878\n",
      "Epoch [10/50], Step [200/1875], D Loss: 0.6774, G Loss: 2.0057\n",
      "Epoch [10/50], Step [300/1875], D Loss: 0.6612, G Loss: 2.0078\n",
      "Epoch [10/50], Step [400/1875], D Loss: 0.6577, G Loss: 2.1113\n",
      "Epoch [10/50], Step [500/1875], D Loss: 0.6545, G Loss: 2.0508\n",
      "Epoch [10/50], Step [600/1875], D Loss: 0.6562, G Loss: 2.0575\n",
      "Epoch [10/50], Step [700/1875], D Loss: 0.6576, G Loss: 2.0468\n",
      "Epoch [10/50], Step [800/1875], D Loss: 0.6588, G Loss: 1.9870\n",
      "Epoch [10/50], Step [900/1875], D Loss: 0.6612, G Loss: 1.9428\n",
      "Epoch [10/50], Step [1000/1875], D Loss: 0.6580, G Loss: 1.9691\n",
      "Epoch [10/50], Step [1100/1875], D Loss: 0.6614, G Loss: 1.9402\n",
      "Epoch [10/50], Step [1200/1875], D Loss: 0.6618, G Loss: 1.9066\n",
      "Epoch [10/50], Step [1300/1875], D Loss: 0.6590, G Loss: 2.0240\n",
      "Epoch [10/50], Step [1400/1875], D Loss: 0.6663, G Loss: 1.9663\n",
      "Epoch [10/50], Step [1500/1875], D Loss: 0.6560, G Loss: 2.0381\n",
      "Epoch [10/50], Step [1600/1875], D Loss: 0.6593, G Loss: 2.0179\n",
      "Epoch [10/50], Step [1700/1875], D Loss: 0.6748, G Loss: 2.0248\n",
      "Epoch [10/50], Step [1800/1875], D Loss: 0.6640, G Loss: 2.0507\n",
      "Epoch [11/50], Step [100/1875], D Loss: 0.6566, G Loss: 2.0618\n",
      "Epoch [11/50], Step [200/1875], D Loss: 0.6565, G Loss: 2.0120\n",
      "Epoch [11/50], Step [300/1875], D Loss: 0.6674, G Loss: 2.0273\n",
      "Epoch [11/50], Step [400/1875], D Loss: 0.7811, G Loss: 2.0194\n",
      "Epoch [11/50], Step [500/1875], D Loss: 0.6568, G Loss: 2.0524\n",
      "Epoch [11/50], Step [600/1875], D Loss: 0.6563, G Loss: 2.0103\n",
      "Epoch [11/50], Step [700/1875], D Loss: 0.6553, G Loss: 2.0513\n",
      "Epoch [11/50], Step [800/1875], D Loss: 0.6558, G Loss: 2.0493\n",
      "Epoch [11/50], Step [900/1875], D Loss: 0.6573, G Loss: 2.0522\n",
      "Epoch [11/50], Step [1000/1875], D Loss: 0.6549, G Loss: 2.0046\n",
      "Epoch [11/50], Step [1100/1875], D Loss: 0.6624, G Loss: 2.0650\n",
      "Epoch [11/50], Step [1200/1875], D Loss: 0.6602, G Loss: 2.0287\n",
      "Epoch [11/50], Step [1300/1875], D Loss: 0.6576, G Loss: 2.0402\n",
      "Epoch [11/50], Step [1400/1875], D Loss: 0.6571, G Loss: 2.0548\n",
      "Epoch [11/50], Step [1500/1875], D Loss: 0.6563, G Loss: 2.0519\n",
      "Epoch [11/50], Step [1600/1875], D Loss: 0.6610, G Loss: 2.1345\n",
      "Epoch [11/50], Step [1700/1875], D Loss: 0.6571, G Loss: 2.0501\n",
      "Epoch [11/50], Step [1800/1875], D Loss: 0.6606, G Loss: 2.2049\n",
      "Epoch [12/50], Step [100/1875], D Loss: 0.6626, G Loss: 2.0617\n",
      "Epoch [12/50], Step [200/1875], D Loss: 0.6628, G Loss: 2.0852\n",
      "Epoch [12/50], Step [300/1875], D Loss: 0.6681, G Loss: 2.0463\n",
      "Epoch [12/50], Step [400/1875], D Loss: 0.6592, G Loss: 2.0484\n",
      "Epoch [12/50], Step [500/1875], D Loss: 0.6541, G Loss: 2.0567\n",
      "Epoch [12/50], Step [600/1875], D Loss: 0.6572, G Loss: 2.0334\n",
      "Epoch [12/50], Step [700/1875], D Loss: 0.6663, G Loss: 2.0528\n",
      "Epoch [12/50], Step [800/1875], D Loss: 0.6587, G Loss: 2.0607\n",
      "Epoch [12/50], Step [900/1875], D Loss: 0.6558, G Loss: 2.0384\n",
      "Epoch [12/50], Step [1000/1875], D Loss: 0.6831, G Loss: 2.0386\n",
      "Epoch [12/50], Step [1100/1875], D Loss: 0.6583, G Loss: 2.0343\n",
      "Epoch [12/50], Step [1200/1875], D Loss: 0.6637, G Loss: 2.1472\n",
      "Epoch [12/50], Step [1300/1875], D Loss: 0.6683, G Loss: 2.1255\n",
      "Epoch [12/50], Step [1400/1875], D Loss: 0.6621, G Loss: 2.0885\n",
      "Epoch [12/50], Step [1500/1875], D Loss: 0.6654, G Loss: 2.0491\n",
      "Epoch [12/50], Step [1600/1875], D Loss: 0.6576, G Loss: 2.0433\n",
      "Epoch [12/50], Step [1700/1875], D Loss: 0.6567, G Loss: 2.0078\n",
      "Epoch [12/50], Step [1800/1875], D Loss: 0.6578, G Loss: 2.0570\n",
      "Epoch [13/50], Step [100/1875], D Loss: 0.6575, G Loss: 2.0907\n",
      "Epoch [13/50], Step [200/1875], D Loss: 0.6603, G Loss: 2.0491\n",
      "Epoch [13/50], Step [300/1875], D Loss: 0.6643, G Loss: 2.0613\n",
      "Epoch [13/50], Step [400/1875], D Loss: 0.6612, G Loss: 2.0828\n",
      "Epoch [13/50], Step [500/1875], D Loss: 0.6608, G Loss: 2.0786\n",
      "Epoch [13/50], Step [600/1875], D Loss: 0.6576, G Loss: 2.0201\n",
      "Epoch [13/50], Step [700/1875], D Loss: 0.6620, G Loss: 2.0966\n",
      "Epoch [13/50], Step [800/1875], D Loss: 0.6571, G Loss: 2.1225\n",
      "Epoch [13/50], Step [900/1875], D Loss: 0.6606, G Loss: 1.9213\n",
      "Epoch [13/50], Step [1000/1875], D Loss: 0.7362, G Loss: 1.9614\n",
      "Epoch [13/50], Step [1100/1875], D Loss: 0.6557, G Loss: 1.9925\n",
      "Epoch [13/50], Step [1200/1875], D Loss: 0.6567, G Loss: 2.0585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m discriminator\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     27\u001B[0m generator\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 29\u001B[0m outputs_real \u001B[38;5;241m=\u001B[39m \u001B[43mdiscriminator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreal_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m loss_real \u001B[38;5;241m=\u001B[39m criterion(outputs_real, real_labels)\n\u001B[1;32m     33\u001B[0m noise \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, latent_dim)\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[40], line 24\u001B[0m, in \u001B[0;36mDiscriminator.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    164\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    166\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001B[0m, in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2475\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m   2476\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m-> 2478\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2479\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[1;32m   2480\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.0002\n",
    "batch_size = train_loader.batch_size\n",
    "latent_dim = generator.latent_dim\n",
    "num_epochs = 50\n",
    "\n",
    "smooth_factor = 0.1\n",
    "real_label_smoothed = 1 - smooth_factor\n",
    "fake_label_smoothed = smooth_factor\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        # Real images to device\n",
    "        # real_images = real_images.to(device)\n",
    "        real_labels = torch.full((batch_size, 1), real_label_smoothed)\n",
    "        fake_labels = torch.full((batch_size, 1), fake_label_smoothed)\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        generator.zero_grad()\n",
    "\n",
    "        outputs_real = discriminator(real_images)\n",
    "        loss_real = criterion(outputs_real, real_labels)\n",
    "\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "\n",
    "        outputs_fake = discriminator(fake_images.detach())\n",
    "        loss_fake = criterion(outputs_fake, fake_labels)\n",
    "\n",
    "\n",
    "        loss_d = loss_real + loss_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "\n",
    "\n",
    "        fake_images = generator(noise)\n",
    "        outputs_fake = discriminator(fake_images)\n",
    "        loss_g = criterion(outputs_fake, real_labels)\n",
    "\n",
    "\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "                    f'D Loss: {loss_d.item():.4f}, G Loss: {loss_g.item():.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}