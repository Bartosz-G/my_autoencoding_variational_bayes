{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def Attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, d_k: int):\n",
    "    QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "    QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "    softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "    return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch. matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 16])\n",
      "Shape of K: torch.Size([2, 5, 16])\n",
      "Shape of V: torch.Size([2, 5, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[ 2.4445e+34, -7.4946e+34,  3.4983e+33, -3.0524e+34,  1.9479e+34,\n          -4.8222e+34,         nan,         nan, -6.0450e+32,  2.0500e+33,\n           1.3955e+34, -5.6239e+34,  2.1966e+33,  1.8136e+34,  9.8687e+33,\n          -5.0540e+33,  1.5466e+35,  3.4836e+33,  1.5569e+34,  1.4061e+34,\n           2.2541e+34,  1.1064e+34, -4.2650e+34, -2.4469e+34,  9.3073e+33,\n           2.9667e+34, -1.8876e+35, -3.0091e+34,  2.9198e+34,  4.1485e+34,\n          -8.6919e+33,  5.4249e+33,  1.7572e+34, -2.3438e+34,  2.3730e+34,\n           1.1159e+34,  7.0735e+34,  6.0478e+34,  1.1153e+34, -5.6251e+33,\n          -5.7578e+33,  2.9731e+34, -9.1963e+33, -5.9002e+33, -2.2500e+34,\n           5.6736e+33, -5.2378e+34,  2.0040e+34,  1.3603e+32,  5.9965e+34,\n          -1.0792e+34,  2.2398e+33,  2.3582e+34,  1.7505e+34,  4.0903e+34,\n           2.6814e+32, -7.3109e+33, -1.4660e+34,  3.3339e+34, -1.3406e+34,\n           1.6280e+34,  1.5884e+34, -2.6358e+33,  2.9108e+34],\n         [ 4.0159e+34, -7.0553e+34, -1.7225e+33, -2.2597e+34,  3.6996e+34,\n          -4.0930e+34,         nan,         nan,  1.7707e+34, -6.4596e+33,\n           1.8615e+34, -1.0884e+35,  1.6791e+33,  1.4483e+34,  2.3232e+33,\n           1.0918e+33,  1.6601e+35, -8.7392e+33,  8.4386e+33,  1.8314e+34,\n           4.8274e+34,  1.4833e+34, -5.8625e+34, -2.6722e+34,  1.6488e+34,\n           2.8299e+34, -1.2428e+35, -1.9087e+34,  3.2710e+34,  5.0332e+34,\n          -1.9438e+34,  1.0004e+34,  3.1936e+34, -3.2015e+34,  3.0888e+34,\n          -2.7629e+33,  6.0279e+34,  3.6894e+34,  1.9388e+34,  7.2967e+33,\n          -1.0363e+34,  1.4358e+34, -3.4766e+33, -1.4820e+34, -2.8098e+34,\n          -1.5359e+33, -5.1013e+34,  1.3747e+34,  2.5090e+32,  3.7759e+34,\n          -1.9611e+34,  2.1855e+34,  3.3100e+34,  1.1661e+34,  4.3109e+34,\n          -5.9242e+33, -2.9537e+33, -1.1329e+34,  3.1677e+34, -2.5599e+34,\n           9.9988e+33,  1.6722e+34,  2.3132e+34,  2.2783e+34],\n         [ 3.8668e+34, -1.0514e+35,  5.5168e+33, -2.8683e+34,  2.7278e+34,\n          -3.8236e+34,         nan,         nan, -2.9995e+34,  2.1298e+34,\n           1.7500e+34, -7.2735e+34,  3.3421e+32,  3.5185e+34,  2.3196e+34,\n          -5.9764e+33,  1.2372e+35,  7.8483e+33,  3.2806e+34,  2.0387e+34,\n           3.1243e+34,  1.2116e+34,  1.5471e+33, -2.3723e+34,  3.8332e+34,\n           3.1553e+34, -3.3154e+35,  5.9569e+33,  2.1513e+34,  5.1346e+34,\n          -1.4577e+34,  4.6622e+31,  1.5283e+34, -7.5574e+33,  7.9419e+33,\n          -2.2175e+34,  7.8434e+34,  1.0311e+35, -1.9337e+33, -3.3853e+34,\n          -9.1191e+33,  1.8585e+34, -9.5671e+33, -4.9706e+33, -3.9062e+33,\n           1.1458e+34, -4.5651e+34,  3.3881e+34,  1.9164e+32,  1.0521e+35,\n          -1.9454e+34, -3.4688e+34,  3.7965e+34,  1.7510e+34,  4.1443e+34,\n          -4.3325e+33,  4.0059e+32, -1.3830e+34,  2.5160e+34, -3.4945e+32,\n           2.9862e+34,  1.5665e+34, -3.4309e+34,  1.5701e+34],\n         [ 4.2798e+34, -6.4647e+34, -1.7922e+34, -3.4476e+34,  4.7294e+34,\n          -3.5847e+34,         nan,         nan, -8.6982e+33,  3.8111e+32,\n           1.9119e+34, -9.5241e+34, -1.6271e+34,  1.2944e+34,  3.5436e+33,\n          -2.9964e+33,  1.4691e+35, -3.3864e+33,  1.6506e+34,  1.2120e+34,\n           4.4399e+34,  1.1191e+34, -5.1067e+34, -2.4794e+34,  1.2798e+34,\n           2.8460e+34, -1.1839e+35, -3.0153e+34,  3.1874e+34,  4.3243e+34,\n          -1.4612e+34,  4.7024e+34,  2.3196e+34, -3.5655e+34,  1.7794e+34,\n           1.6918e+34,  4.8520e+34,  8.5109e+34,  2.1118e+34,  4.0156e+33,\n           2.1902e+34,  2.1350e+34, -2.0944e+34, -1.6322e+34, -1.8417e+34,\n          -2.8183e+33, -5.4293e+34, -7.5867e+33,  3.0022e+31,  8.6832e+34,\n          -1.8717e+34,  8.1132e+33,  3.1037e+34,  9.6479e+33,  4.4521e+34,\n           5.6007e+31, -3.4489e+33, -1.5595e+34,  3.4047e+34, -2.9742e+34,\n          -1.0725e+34, -8.3580e+33, -4.7596e+33,  2.7158e+34],\n         [ 3.3986e+34, -7.6963e+34, -2.7333e+34,  2.2609e+34,  2.4800e+34,\n          -3.5242e+34,         nan,         nan, -1.8800e+34,  1.5210e+33,\n           1.3556e+34, -1.1453e+35, -8.9309e+33,  6.9654e+33,  5.4978e+33,\n          -5.1587e+33,  1.3298e+35,  3.5127e+33,  2.3386e+34, -1.2725e+33,\n           3.6981e+34,  2.0622e+33, -4.5576e+34, -1.9320e+34,  2.0482e+34,\n           3.2656e+34, -1.7203e+35, -1.6301e+34,  2.8612e+34,  2.3949e+34,\n          -1.2521e+34,  1.9025e+34,  2.7939e+34, -4.0264e+34,  1.9568e+34,\n          -1.1903e+34,  3.2118e+34,  7.8368e+34,  2.1207e+34,  1.9626e+33,\n           1.2345e+34,  1.6205e+34, -1.0027e+34, -1.7474e+34, -2.2796e+34,\n           8.7496e+33, -3.6562e+34,  6.3163e+33,  9.3911e+31,  7.8535e+34,\n          -1.3938e+34, -1.2300e+34,  3.1401e+34,  1.4366e+34,  4.2844e+34,\n          -2.7321e+34,  4.3281e+33, -1.8839e+34,  4.1031e+33, -1.5734e+34,\n           2.5475e+33, -2.6393e+34, -2.5984e+34, -2.6713e+33]],\n\n        [[ 5.6896e+34, -1.6306e+34, -5.4439e+34,  2.1292e+34, -1.6459e+34,\n          -2.9054e+34,         nan,         nan, -1.0393e+34,  3.2788e+34,\n          -7.4878e+33,  1.4149e+35, -2.1454e+34,  1.9070e+34,  4.4909e+32,\n           2.0668e+34, -6.9809e+34,  4.2245e+34,  4.5973e+34,  4.9927e+34,\n           2.9049e+34, -1.2588e+34, -1.1487e+34, -1.6871e+34,  2.3673e+34,\n           6.2975e+33, -1.5826e+35,  1.0095e+35, -3.0580e+34,  9.0662e+33,\n           3.3168e+34,  2.8622e+34,  1.6824e+34, -1.3880e+34,  2.4760e+33,\n          -5.1503e+34, -2.2325e+34, -4.4134e+34,  1.7375e+34, -6.8418e+34,\n           9.6138e+33, -5.9360e+34, -4.5402e+34, -1.0188e+34, -9.2945e+33,\n          -3.1041e+34, -1.6276e+33, -6.8241e+33,  6.1308e+32, -3.5214e+34,\n          -2.9595e+32, -6.0228e+34,  6.0477e+33,  1.4135e+34, -2.7462e+34,\n           5.8297e+34,  1.9286e+34,  1.2806e+34, -3.0662e+34, -1.5436e+34,\n          -1.2321e+34,  1.9205e+35, -2.6677e+34,  2.1363e+34],\n         [ 4.8186e+34, -2.2690e+34, -9.6374e+33,  2.8831e+33, -3.3117e+34,\n          -5.1838e+34,         nan,         nan, -1.7077e+34,  5.0827e+34,\n          -1.3705e+34,  9.2815e+34, -7.1211e+33,  6.5783e+34, -2.3538e+33,\n           8.8623e+33, -5.3347e+34,  2.2465e+34,  4.6261e+34,  7.7615e+34,\n           8.7082e+33, -2.7776e+34, -4.0336e+34, -7.0016e+33,  6.1049e+34,\n           1.2395e+34, -1.2314e+35, -2.9686e+33, -3.1102e+34,  1.5973e+34,\n           2.8873e+34,  4.5229e+34,  1.8026e+34,  1.0927e+33, -9.2240e+33,\n           2.0668e+34,  9.4322e+33,  2.0658e+33,  1.9972e+34, -4.6202e+34,\n           1.4474e+34, -2.8779e+34, -7.1686e+33,  2.0909e+34, -1.3905e+34,\n          -2.3385e+34, -9.2845e+33, -2.3204e+34,  4.1749e+32,  1.4273e+34,\n          -5.8309e+33, -4.7992e+34,  8.7898e+33,  1.5304e+34, -2.2877e+34,\n           8.2762e+34,  3.2522e+34,  2.1233e+34, -9.7090e+33, -1.0324e+34,\n          -2.8517e+34,  2.3661e+35, -4.6938e+32,  2.3423e+33],\n         [ 4.1325e+34, -1.1489e+34, -8.2820e+34,  4.4605e+34, -1.2809e+34,\n          -4.2510e+34,         nan,         nan, -1.3401e+34,  2.3521e+34,\n          -1.1129e+34,  9.4261e+34, -1.4915e+34,  9.3106e+34, -1.5974e+32,\n          -4.8766e+33, -6.6688e+34,  3.3317e+34,  4.9147e+34,  5.6970e+34,\n           1.0745e+35, -4.9497e+33, -4.4478e+34, -3.4356e+34,  4.4164e+34,\n           1.9168e+34, -1.1018e+35,  2.9224e+34, -3.4518e+34,  1.6462e+34,\n           2.1596e+34,  4.2029e+34,  2.9890e+34,  2.4813e+34, -5.0052e+33,\n           5.2621e+34, -1.8695e+33, -5.5229e+33,  2.3694e+34, -4.7084e+34,\n           1.7185e+33, -4.4813e+34,  1.3796e+34,  2.2912e+34,  1.3218e+33,\n          -3.5270e+34, -3.0914e+34,  4.6187e+33,  6.4071e+32,  4.0118e+33,\n           4.3225e+33, -3.2774e+34,  2.6041e+34,  4.0206e+33, -1.6743e+34,\n           1.4068e+35,  2.3722e+34,  2.2876e+34, -3.8947e+34, -1.4555e+34,\n          -3.0855e+32,  3.1275e+35, -2.7016e+33,  1.9485e+34],\n         [ 4.0954e+34, -1.2545e+34, -2.4865e+34,  3.0296e+32,  3.1209e+33,\n          -4.1963e+34,         nan,         nan, -1.0562e+34,  4.5665e+34,\n          -6.7313e+33,  1.2036e+35, -6.9735e+33,  1.3136e+35, -4.8912e+33,\n           1.5950e+33, -1.3922e+34,  1.7822e+34,  4.6771e+34,  7.4513e+34,\n          -1.6852e+33, -1.9795e+34, -3.9255e+34, -2.3278e+33,  5.6388e+34,\n           2.4213e+34, -1.1161e+35,  8.3254e+34, -2.9118e+34,  2.1676e+34,\n           3.0337e+34,  2.0944e+34,  1.2231e+34,  8.1802e+33, -7.5475e+33,\n           6.4214e+34, -3.2031e+32, -5.5306e+33,  3.2130e+34, -5.3622e+34,\n           1.3399e+34, -4.7837e+34, -1.9734e+34,  7.0607e+33, -9.9106e+33,\n          -2.2759e+34,  4.9989e+33, -1.8204e+34,  5.5458e+32,  5.3405e+33,\n          -6.6896e+33, -3.9012e+34,  1.5805e+34,  1.2529e+34, -1.1015e+34,\n           4.5439e+34,  2.6806e+34,  2.1820e+34, -1.7522e+34, -1.1349e+34,\n          -2.4406e+34,  2.0414e+35, -6.4334e+33,  7.0119e+33],\n         [ 3.9802e+34, -1.0090e+34, -4.0537e+34,  1.2173e+34, -2.5821e+34,\n          -4.4523e+34,         nan,         nan, -1.0695e+34,  3.1249e+34,\n          -1.4486e+34,  1.2399e+35, -3.8663e+33, -3.2688e+34, -6.3002e+32,\n          -3.0830e+33, -7.7988e+34,  2.1740e+34,  4.5695e+34,  6.1506e+34,\n           2.2379e+34, -1.6366e+34, -3.6065e+34, -1.3353e+34,  4.4152e+34,\n          -2.8170e+33, -7.7441e+34,  2.6458e+34, -2.8554e+34,  1.0747e+34,\n           6.2257e+33,  3.3577e+34,  1.5293e+34,  2.7806e+34, -3.1505e+33,\n           1.3818e+34, -1.4373e+34, -2.8081e+34,  2.0542e+34, -4.2165e+34,\n           6.5476e+33, -5.6996e+34, -5.2707e+33,  2.2853e+34, -1.3785e+33,\n          -2.1202e+34, -4.3806e+33, -8.0260e+33,  4.8970e+32, -1.7553e+34,\n           1.7499e+34, -3.9337e+34,  2.2060e+34,  1.8835e+34, -2.9568e+34,\n           8.6569e+34,  1.8631e+34,  2.1900e+34, -1.9014e+34, -1.4556e+34,\n          -1.3396e+34,  2.0207e+35, -9.7557e+33,  2.5721e+34]]],\n       grad_fn=<UnsafeViewBackward0>)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "model_dim = 64    # Dimension of the model\n",
    "d_k = 16          # Dimension of the keys (and queries)\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, 4)\n",
    "multi_head_attention(Q, K, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}