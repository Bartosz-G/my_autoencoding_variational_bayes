{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch.matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention1.set_mask(mask)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttentionIsAllYouNeed(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, number_of_encoder_stacks: int, number_of_decoder_stacks: int):\n",
    "        super(AttentionIsAllYouNeed, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "\n",
    "        encoder_list = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_encoder_stacks):\n",
    "            encoder_list.append(EncoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_list)\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_decoder_stacks):\n",
    "            self.decoder.append(DecoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        for layer in self.decoder:\n",
    "            layer.set_mask(mask)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[-5.1103e-02, -1.5536e-01,  8.9463e-02, -1.3167e+00,  2.7258e+00,\n",
      "          -9.3958e-01,  9.1497e-01,  1.2106e+00,  3.7205e-01,  3.5637e-01,\n",
      "          -9.8485e-01,  8.5567e-01,  4.6632e-01,  1.9161e+00, -8.8724e-01,\n",
      "           2.8030e-01, -7.9871e-01,  1.5041e+00,  1.3065e+00, -2.2650e-01,\n",
      "          -5.8580e-01, -5.5293e-01, -5.7641e-01,  1.6829e+00, -8.5277e-01,\n",
      "          -2.0138e+00, -9.7452e-01,  3.1980e-01, -8.7095e-01, -6.4506e-01,\n",
      "          -5.8573e-01, -1.3367e+00,  5.3541e-01,  5.7812e-01, -9.1546e-01,\n",
      "          -4.1406e-01,  1.4907e-01, -5.6542e-01,  1.1760e+00,  9.9776e-01,\n",
      "          -3.5892e-01,  3.2156e-01, -7.2271e-01,  2.7709e+00,  2.0121e-01,\n",
      "           5.6360e-01, -3.7481e-01, -1.1145e+00, -7.7653e-01, -2.0065e-01,\n",
      "           4.9055e-01, -1.0025e+00,  1.6620e+00,  1.1149e+00, -9.8769e-01,\n",
      "           2.2635e-01,  3.1325e-01,  6.6318e-01, -1.5772e+00, -2.2373e-01,\n",
      "          -1.1477e+00, -9.7737e-01,  4.6852e-01, -5.1943e-01],\n",
      "         [-6.8841e-01, -8.6490e-01, -1.3724e+00, -4.5800e-01, -6.8802e-01,\n",
      "          -1.4863e-01, -1.4646e+00, -1.8452e-01,  2.5537e+00,  8.0596e-01,\n",
      "          -8.3441e-01,  1.7598e-01, -4.7733e-01,  8.8868e-01, -8.2976e-02,\n",
      "           1.2537e+00, -9.0478e-01,  7.9458e-01,  5.8841e-01,  4.8570e-01,\n",
      "          -2.1299e-01,  1.5128e+00, -8.6960e-02,  1.1995e+00, -6.9278e-01,\n",
      "          -1.0320e+00, -1.0820e+00,  3.1746e-01,  2.7974e-01, -5.1187e-01,\n",
      "          -2.9694e-01, -8.0212e-01, -1.2737e+00, -1.9297e+00,  2.5961e+00,\n",
      "          -4.3026e-01,  2.4280e+00,  4.1778e-01,  7.1219e-01,  4.2987e-01,\n",
      "          -8.2258e-02, -2.1841e-01, -6.8880e-01, -1.6487e+00,  6.0924e-03,\n",
      "          -5.3685e-02,  1.6131e+00,  1.3495e+00,  9.4533e-01,  1.1775e+00,\n",
      "          -1.2649e+00, -7.8668e-01,  6.7297e-01, -5.3733e-02,  5.0657e-01,\n",
      "          -1.3611e+00,  3.5836e-01,  3.8559e-01, -9.4717e-01, -5.4370e-01,\n",
      "           3.3911e-01, -1.0620e+00,  1.0050e+00, -5.6778e-01],\n",
      "         [-5.6074e-01,  1.1817e+00,  1.1648e+00,  1.0533e+00,  7.2197e-01,\n",
      "          -1.5081e+00,  2.5600e-01,  1.1274e+00, -1.2847e+00, -1.5076e+00,\n",
      "          -1.8919e-01, -1.2851e+00,  4.5195e-01,  9.1747e-01, -4.6730e-01,\n",
      "           3.7822e-01, -3.7757e-01,  3.9612e-01,  6.7636e-01, -7.0749e-01,\n",
      "          -1.1521e+00, -8.7779e-01,  3.8576e-01, -2.1262e-01,  1.4575e-01,\n",
      "           7.3122e-01,  1.9435e-01,  7.8982e-01,  3.1689e-01, -5.6578e-02,\n",
      "           1.4717e+00,  1.2090e-01,  4.5074e-01,  1.1758e+00, -1.4989e+00,\n",
      "          -9.7437e-01, -1.8728e-01,  1.0688e+00, -1.7491e+00, -8.0438e-01,\n",
      "          -4.4129e-01,  1.2983e+00,  9.6388e-01,  6.9503e-01,  3.6560e-01,\n",
      "          -2.6073e+00, -7.6265e-01,  3.6382e-01, -1.2655e+00,  1.2118e+00,\n",
      "           3.2539e-01,  8.9628e-01,  5.8750e-01, -8.1674e-01,  2.1598e+00,\n",
      "          -1.6869e+00, -2.1023e+00,  8.2708e-01, -1.3012e+00, -6.1135e-01,\n",
      "           8.2081e-01,  2.1578e-01,  6.7225e-01,  4.1620e-01],\n",
      "         [ 1.3344e+00, -9.9155e-01, -1.0183e+00, -9.0660e-01,  4.7829e-01,\n",
      "          -4.4327e-01,  5.7788e-01,  1.4208e+00,  4.9137e-01, -1.1404e+00,\n",
      "           1.4168e+00,  2.7640e+00,  6.1314e-01,  1.1116e+00, -2.5898e-01,\n",
      "          -1.8487e+00, -8.7567e-02, -1.2134e+00,  4.6382e-01,  3.4479e-01,\n",
      "          -1.2833e-01, -2.1320e-01, -4.9333e-01, -1.0221e+00,  3.8596e-01,\n",
      "          -1.7833e+00,  3.6969e-01,  3.6498e-02, -9.3343e-01, -6.6096e-01,\n",
      "           1.7100e-01, -1.6527e+00, -4.0085e-01,  5.9080e-01,  7.5305e-01,\n",
      "           1.5781e+00, -7.5856e-01, -6.0144e-01, -6.4685e-02,  1.4061e+00,\n",
      "          -1.3402e+00, -3.8414e-02,  7.1120e-01,  8.4655e-01, -1.8073e+00,\n",
      "           2.4917e-01,  7.0173e-01, -1.0959e+00, -8.8336e-01,  1.5854e+00,\n",
      "          -5.5009e-03,  1.5087e-01,  8.9697e-01,  1.5465e+00,  6.5289e-01,\n",
      "          -8.2981e-01,  8.1235e-01,  5.1695e-01,  5.7412e-01,  4.1909e-01,\n",
      "          -1.4766e+00,  6.4033e-01, -1.6288e+00, -8.8484e-01],\n",
      "         [ 1.2215e+00,  1.8750e+00, -7.1294e-01,  3.0638e-01,  1.6902e+00,\n",
      "          -7.5369e-01,  5.8235e-01,  1.7376e+00,  9.6239e-02,  9.6842e-01,\n",
      "          -4.7307e-03,  1.2772e+00,  7.1318e-01, -1.4355e+00, -1.4250e+00,\n",
      "          -8.4113e-01, -5.6904e-01, -6.1144e-01, -9.9198e-01,  1.3705e+00,\n",
      "          -1.2725e-01, -1.3785e+00,  9.2965e-01,  4.5105e-01, -7.8754e-01,\n",
      "           2.8032e-01, -7.3398e-01, -8.8950e-01,  2.9220e-01, -1.2704e+00,\n",
      "           1.2726e+00,  1.2171e+00, -9.7024e-02, -2.8957e-01,  1.2203e+00,\n",
      "          -5.2449e-01, -1.0267e-01, -1.8073e+00,  5.3403e-01,  8.0702e-01,\n",
      "           1.1324e+00,  8.1842e-02, -1.6323e+00,  3.3268e-01, -2.7161e-01,\n",
      "           7.9035e-02,  1.2970e-02,  1.2345e+00,  2.9339e-01, -3.8973e-01,\n",
      "           9.9472e-01, -7.0655e-01, -1.6877e+00,  6.0203e-01,  9.5932e-01,\n",
      "          -7.4737e-01,  5.4794e-01, -8.5321e-01, -2.7987e+00, -9.3975e-01,\n",
      "           6.4218e-01, -1.1206e+00,  8.7382e-01, -1.2826e-01]],\n",
      "\n",
      "        [[-4.3689e-01,  1.8104e+00, -3.2786e-01,  4.1239e-02, -3.1979e-01,\n",
      "          -4.0830e-01,  4.9674e-01, -1.9661e-01,  6.7335e-01, -2.6229e-01,\n",
      "           7.9543e-01,  1.4057e+00,  1.0084e+00,  1.8963e-01, -1.9172e-01,\n",
      "          -1.5564e+00, -1.0866e+00,  1.9105e-01, -1.5438e+00, -1.6614e-01,\n",
      "           6.0393e-01,  4.6219e-01,  2.4772e+00, -1.4566e+00, -2.0114e+00,\n",
      "          -1.2725e+00, -8.5913e-02, -1.2158e-01,  4.6649e-01,  2.3234e-02,\n",
      "          -6.1917e-01,  1.3519e+00,  1.7427e-01,  8.1013e-01,  1.6215e-01,\n",
      "           2.4445e+00, -8.7535e-01,  2.3522e-01, -9.0293e-01,  3.8988e-01,\n",
      "           5.1139e-01, -2.1139e+00, -7.2428e-01,  5.5917e-01, -1.4615e-01,\n",
      "           1.0273e+00,  5.0105e-01, -2.3834e-01, -1.5746e+00, -3.9510e-01,\n",
      "           2.1203e-01, -8.3569e-01,  6.1036e-01, -3.7024e-01, -6.1946e-02,\n",
      "           2.5475e-01,  5.2293e-01, -1.7165e+00,  7.0524e-01, -2.8540e-01,\n",
      "          -3.5665e-01,  1.3752e+00, -1.8428e+00,  2.0110e+00],\n",
      "         [-1.7987e+00,  1.1532e+00, -2.6385e-02, -1.2169e+00,  7.3232e-01,\n",
      "          -9.8096e-01, -3.4256e-01,  6.6256e-01, -3.0432e-01,  4.3574e-01,\n",
      "           1.2595e+00,  1.9819e+00, -1.0547e+00, -1.4394e+00, -4.4312e-01,\n",
      "           1.3983e+00,  1.0254e+00, -2.2235e-01, -2.7041e-01, -1.3931e+00,\n",
      "           9.2148e-01,  9.8867e-01, -5.9349e-01,  7.0632e-01,  8.6180e-03,\n",
      "           2.0746e-01,  1.9331e-01,  4.7602e-01,  4.2822e-01, -1.2815e-01,\n",
      "           1.6158e+00, -1.6276e-01, -7.3339e-01,  1.0137e+00,  1.3996e+00,\n",
      "           1.9505e+00,  7.4951e-02, -4.8732e-01,  5.9566e-01, -3.4725e-01,\n",
      "          -4.2250e-01, -1.0374e+00,  2.0247e-02, -8.3887e-01,  9.6252e-01,\n",
      "           1.6417e-01,  1.3117e-01,  4.6495e-01, -2.6396e-01,  1.6125e+00,\n",
      "          -1.3834e+00, -2.0260e+00, -3.2183e-01, -3.2585e-02, -1.3379e-01,\n",
      "          -2.4259e+00,  1.1696e+00, -9.2065e-01,  5.7924e-01,  1.4224e+00,\n",
      "          -1.4383e+00, -1.2999e+00,  2.0261e-03, -1.2677e+00],\n",
      "         [ 9.0101e-03, -4.7617e-01, -7.8876e-01,  1.1760e+00,  7.2330e-01,\n",
      "           2.2311e-01,  1.6259e-01,  7.4508e-01,  7.6537e-01, -1.3616e-01,\n",
      "           1.2425e-01,  6.6473e-01, -1.4456e+00,  1.4376e-01, -1.0407e+00,\n",
      "          -1.1087e+00, -8.2233e-01, -5.0064e-02, -1.5613e+00,  1.5183e+00,\n",
      "          -7.1175e-01,  3.0856e-01, -1.5725e-02, -7.3902e-01,  1.9968e+00,\n",
      "          -3.7742e-02, -3.0898e-01, -1.5067e+00, -1.1500e-02,  1.4069e+00,\n",
      "           1.4156e-01,  4.1666e-01,  5.0860e-01,  1.0464e+00,  2.8535e-01,\n",
      "           2.3581e+00, -6.4296e-01, -1.3110e+00, -3.1099e-01,  2.4063e-01,\n",
      "           3.1355e-01,  3.5219e-01, -2.3965e-01,  8.7400e-01, -4.3404e-01,\n",
      "           4.1649e-01, -1.6186e-01,  1.6079e-01,  8.2099e-01, -2.9523e-01,\n",
      "          -1.5538e+00,  5.2492e-01, -8.7833e-01, -4.0458e-01, -2.8767e-01,\n",
      "          -1.1729e+00,  2.3063e+00, -2.3676e+00,  2.3186e+00, -6.0889e-01,\n",
      "          -1.8221e+00,  1.1428e-01, -1.2683e+00,  1.3540e+00],\n",
      "         [ 2.8551e+00,  1.1616e+00, -2.0922e-01,  1.3300e+00, -1.3336e+00,\n",
      "          -1.7139e-02,  3.0502e-01,  9.5228e-02, -6.5361e-01, -8.1473e-01,\n",
      "           1.8364e+00, -3.4062e-02, -8.8814e-01,  1.5563e+00,  1.5594e-01,\n",
      "           1.3765e-01,  1.3086e+00, -1.0239e+00,  1.4242e+00,  7.4025e-02,\n",
      "          -7.1301e-01, -1.2659e-01,  3.7532e-01,  1.1019e+00,  1.3118e+00,\n",
      "          -4.6781e-01,  1.3762e+00, -1.0566e+00, -3.1918e-01, -9.4817e-01,\n",
      "          -4.0215e-01, -5.9567e-01,  4.9674e-01, -9.8181e-01, -9.2894e-02,\n",
      "           3.0167e-01,  8.3294e-01,  1.3463e+00, -4.0568e-01, -1.6191e+00,\n",
      "          -4.4816e-01, -2.5594e-01, -4.7000e-01, -1.1460e+00,  1.7500e+00,\n",
      "           8.2338e-02,  1.1057e+00, -4.0195e-01,  4.2444e-01, -1.5171e+00,\n",
      "          -7.1684e-01, -1.6696e+00, -1.7902e+00, -2.2767e-01, -1.5874e+00,\n",
      "           4.1878e-02, -8.9869e-01,  2.7091e-01,  6.9793e-01,  5.2538e-01,\n",
      "          -1.5216e+00,  4.1654e-01, -4.3933e-01,  1.0956e+00],\n",
      "         [-1.2209e-01, -1.0250e+00, -1.6323e+00, -7.0465e-01, -6.7891e-01,\n",
      "          -2.1575e-01,  9.7989e-01, -6.3390e-01, -3.1567e-01, -3.2046e-01,\n",
      "          -2.1357e-02,  1.2205e-01,  2.5867e+00,  2.3145e-01, -6.9201e-01,\n",
      "           1.6710e+00,  6.3202e-01,  7.5485e-01,  1.5967e-01,  6.0119e-01,\n",
      "          -5.0484e-01, -1.1671e+00,  2.8733e-01,  4.5709e-01, -3.6638e+00,\n",
      "          -9.7129e-01,  8.3796e-01,  1.5706e-01,  1.1226e+00, -1.7337e-01,\n",
      "           1.1265e+00,  9.5722e-01,  1.1006e+00, -1.9463e+00, -4.0635e-01,\n",
      "           5.3671e-02, -2.3444e+00,  1.1326e+00, -9.5963e-01, -6.5375e-01,\n",
      "           6.4256e-01, -2.5472e-01, -1.2341e-01,  1.4197e+00,  1.0841e+00,\n",
      "          -3.3315e-01, -4.9617e-01,  1.8849e-02,  1.6534e-02,  3.1816e-01,\n",
      "          -5.6606e-02,  1.2983e+00,  1.1845e+00,  3.8288e-01,  1.6913e-01,\n",
      "          -1.2735e-01,  5.2979e-01, -1.6966e-01,  1.2077e+00, -1.5494e-01,\n",
      "          -3.8811e-01,  3.7566e-01, -8.3431e-01, -1.5281e+00]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[-0.0553, -0.5405, -0.0787, -1.1386,  3.2929, -0.8569,  1.0018,\n",
      "           1.0260,  0.3173,  0.9808, -1.0841,  0.8368,  0.0544,  1.6189,\n",
      "          -0.7436,  0.1179, -0.1419,  1.7244,  1.7081, -0.4420, -0.8642,\n",
      "          -0.7075, -0.3510,  1.3777, -0.6152, -1.7694, -1.0887, -0.0660,\n",
      "          -0.9641, -0.8317, -1.1228, -1.3365, -0.1299,  0.5359, -1.1189,\n",
      "          -0.1141,  0.1088, -0.5681,  0.8904,  0.6167, -0.3779,  0.5211,\n",
      "          -0.7390,  2.5329,  0.2582,  1.2149, -0.4170, -1.5689, -0.4364,\n",
      "          -0.2533,  0.3883, -0.8144,  0.7542,  1.1015, -0.4885, -0.1257,\n",
      "           0.7466,  1.1807, -1.3835,  0.0727, -1.1045, -0.5672,  0.6248,\n",
      "          -0.5989],\n",
      "         [ 0.0453, -1.3570, -1.0942, -0.6251, -0.3720, -0.0241, -1.5228,\n",
      "          -0.6315,  3.1421,  1.5561, -1.1906,  0.0214, -0.2957,  0.1801,\n",
      "           0.1137,  1.3456, -1.0721,  1.0356,  0.5130,  0.1852, -0.2766,\n",
      "           1.1702,  0.3333,  0.7300, -0.2384, -0.7054, -0.6570,  0.3771,\n",
      "           0.3538, -1.5277, -0.2292, -0.6932, -1.0551, -1.4147,  2.8197,\n",
      "          -0.5175,  1.6599,  0.3196,  0.0936, -0.0612, -0.0517, -0.1345,\n",
      "          -0.6346, -2.1623, -0.0560,  0.1988,  1.3134,  0.6577,  1.2242,\n",
      "           1.3037, -1.3862, -0.5313, -0.2187,  0.1791,  0.3922, -1.3335,\n",
      "           0.6032,  0.2279, -0.5945, -0.1435,  0.3782, -0.7415,  1.6215,\n",
      "          -0.5459],\n",
      "         [ 0.0993,  0.7359,  0.9183,  1.3103,  1.2538, -0.9052,  0.1420,\n",
      "           1.0976, -1.0708, -1.7744, -0.1256, -1.1781,  0.2305,  0.1322,\n",
      "          -0.3484,  0.3143,  0.2176,  0.1915,  1.3259, -0.3866, -1.8451,\n",
      "          -1.3371,  0.7574,  0.0451,  0.3879,  0.1932,  0.2546,  0.6935,\n",
      "          -0.1851, -0.0729,  1.5546, -0.3709,  0.3745,  1.2515, -1.7674,\n",
      "          -0.3668, -0.6747,  1.0070, -1.7193, -0.9091, -0.3433,  1.5385,\n",
      "           0.9114,  0.6773,  0.1734, -2.4196, -0.4702,  0.1166, -1.0691,\n",
      "           1.4112,  0.1374,  0.5021,  0.1659, -0.8661,  1.5624, -1.4794,\n",
      "          -2.0877,  0.9120, -1.4187, -0.5312,  1.2374,  0.3959,  1.4484,\n",
      "           0.0443],\n",
      "         [ 0.8522, -0.8676, -1.3277, -1.1879,  0.8217, -0.4563,  0.6180,\n",
      "           1.6094,  0.3656, -0.6941,  1.7086,  2.6753,  0.0678,  0.5963,\n",
      "           0.2435, -2.0769,  0.7169, -1.3074,  0.6609,  0.2714, -0.2022,\n",
      "          -0.4309, -0.4743, -0.8259,  0.8144, -0.9651,  0.2038, -0.0947,\n",
      "          -1.0873, -1.0830, -0.2615, -1.6265, -0.9596,  0.4655,  0.3275,\n",
      "           1.7851, -1.2580, -1.1063,  0.2830,  1.4642, -1.2042,  0.4860,\n",
      "           0.8585,  0.8612, -1.6795,  0.2155,  1.0206, -1.1489, -0.9943,\n",
      "           1.3767, -0.1750,  0.1942,  0.2469,  1.6752,  0.7239, -0.6865,\n",
      "           0.7691,  0.3786,  0.7648,  0.3372, -0.5218,  0.5864, -1.2754,\n",
      "          -1.0670],\n",
      "         [ 1.5441,  1.5562, -0.8276, -0.1642,  1.9329, -0.2419,  0.5722,\n",
      "           1.8866,  0.3633,  1.3898, -0.3938,  1.1738,  0.6233, -2.0070,\n",
      "          -0.9569, -0.9429, -0.5513, -0.5803, -0.5547,  1.1912, -0.1439,\n",
      "          -1.4513,  1.3730,  0.4328, -0.7824, -0.1669, -0.2733, -1.1229,\n",
      "           0.0288, -1.1324,  0.9211,  0.9229, -0.4977, -0.1653,  1.3137,\n",
      "          -0.1485, -0.1896, -1.9127,  0.2093,  0.4598,  1.2347,  0.8043,\n",
      "          -1.3042, -0.4284, -0.4229,  0.2656, -0.1729,  0.6832,  0.3436,\n",
      "           0.1559,  1.1051, -0.9242, -2.3234,  0.5598,  0.5166, -0.6582,\n",
      "           0.7630, -0.6809, -2.2349, -0.7335,  0.7981, -1.0835,  1.2706,\n",
      "          -0.2206]],\n",
      "\n",
      "        [[-0.6468,  1.9367, -0.0594, -0.3905, -0.1342,  0.2948,  0.4725,\n",
      "          -0.2776,  0.1353,  0.0737,  0.5158,  1.1426,  0.6548, -0.0776,\n",
      "           0.1456, -1.4938, -1.4644, -0.0692, -1.3039,  0.2410,  0.3065,\n",
      "           0.0416,  2.5215, -1.4486, -2.0912, -0.9152, -0.2250, -0.1639,\n",
      "           0.1232,  0.3912, -0.8650,  1.6261,  0.0611,  0.4650,  0.3990,\n",
      "           1.9706, -0.7176, -0.5119, -1.1055,  0.9577,  0.8786, -1.6129,\n",
      "          -1.3380,  0.7634, -0.5100,  1.3526,  1.2720, -0.2112, -1.7717,\n",
      "          -0.4950,  0.6687, -1.0288,  0.5705, -0.2921, -0.4566,  0.2741,\n",
      "           0.6963, -1.8761,  1.0194, -0.2283,  0.5384,  1.7120, -1.4454,\n",
      "           1.0056],\n",
      "         [-2.0964,  1.0116, -0.1979, -1.9188,  1.3334, -1.0995, -0.2740,\n",
      "           1.1042, -0.6121,  0.3629,  0.3905,  2.0424, -1.2821, -1.5402,\n",
      "           0.2192,  1.3633,  0.6598, -0.6967,  0.0755, -1.5402,  0.7270,\n",
      "           0.8211, -0.3745,  0.5372, -0.0067,  0.7003, -0.2103,  0.1603,\n",
      "          -0.0136,  0.0308,  1.5208, -0.4853, -0.4253,  0.7113,  1.5598,\n",
      "           1.6953,  0.0551, -0.5836,  0.9512, -0.6034, -0.5645, -0.8669,\n",
      "          -0.1052, -0.7270,  1.0058, -0.2414,  0.3280,  0.5508,  0.0167,\n",
      "           2.1245, -0.8064, -1.9247, -0.5456,  0.2220, -0.3042, -2.1909,\n",
      "           1.1114, -1.1459,  0.7989,  1.0711, -0.3828, -0.8808,  0.5688,\n",
      "          -1.1840],\n",
      "         [ 0.1094, -0.6121, -0.9312,  0.6980,  0.8353,  0.1936,  0.5291,\n",
      "           0.7400,  0.6464, -0.0979, -0.0940,  0.3283, -1.8783, -0.1980,\n",
      "          -0.3020, -1.0647, -0.8149, -0.3299, -1.6089,  1.5243, -0.7702,\n",
      "          -0.3358, -0.0404, -1.2149,  2.4992,  0.1013, -0.3926, -1.2971,\n",
      "           0.2029,  1.3906, -0.2022,  0.4669,  0.2139,  1.2140,  0.4301,\n",
      "           2.0992, -0.7504, -1.6352,  0.1990,  0.7656,  0.4659,  0.8779,\n",
      "          -0.2666,  1.0396, -0.5770,  0.2845, -0.1601, -0.1817,  1.1181,\n",
      "          -0.1731, -1.0526,  0.4307, -1.0137, -0.5639, -0.0494, -1.2936,\n",
      "           2.2243, -2.2617,  2.2687, -0.7948, -1.1544,  0.4547, -1.0063,\n",
      "           0.7680],\n",
      "         [ 2.8046,  0.9964, -0.1763,  0.7809, -1.1356,  0.2677,  0.3455,\n",
      "           0.5941, -0.5687, -0.8327,  1.3114,  0.0198, -1.4477,  1.2788,\n",
      "           0.7120,  0.8788,  0.1660, -0.9515,  1.4154, -0.0647, -0.4943,\n",
      "          -0.0599, -0.3969,  1.3629,  1.4545, -0.9766,  0.6431, -1.1733,\n",
      "          -0.5105, -0.7595, -0.2550, -0.3758,  0.7042, -0.7752,  0.2557,\n",
      "          -0.4686,  1.8806,  1.4820, -0.6265, -1.4110, -0.3117, -0.1275,\n",
      "          -0.8129, -0.6734,  1.6678,  0.1935,  1.6031, -0.6138,  0.1727,\n",
      "          -0.5299, -0.8958, -1.5131, -2.6410,  0.2171, -1.5467,  0.5855,\n",
      "          -1.1607,  0.3038,  0.5210,  0.5032, -1.1764,  0.3693, -0.5956,\n",
      "           0.5673],\n",
      "         [-0.4132, -1.3426, -1.8727, -1.0442, -0.7984,  0.6435,  0.4887,\n",
      "          -1.0027, -0.3570, -0.4024, -0.2013,  0.1011,  2.7555, -0.0513,\n",
      "          -0.3900,  2.0331,  0.6333,  0.4062,  0.0974,  0.5514, -0.3193,\n",
      "          -1.4045,  0.1887,  0.2361, -3.3590, -0.7900,  0.5599,  0.1604,\n",
      "           0.6070, -0.2309,  1.0748,  0.7694,  0.7071, -1.4363, -0.4025,\n",
      "          -0.5799, -2.2298,  1.0588, -1.3153, -0.6855,  0.7083,  0.2502,\n",
      "          -0.2598,  1.7555,  0.7688, -0.1599,  0.1996,  0.0465, -0.1659,\n",
      "           0.4983, -0.2782,  1.5622,  0.9616,  0.3129,  0.0047, -0.1339,\n",
      "           0.9507, -0.1645,  0.9832,  0.5714,  0.1907,  0.8246, -0.1605,\n",
      "          -1.7102]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}