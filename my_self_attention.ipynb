{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch.matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention1.set_mask(mask)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, max_len=1000, dropout = 0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough `P`\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1765611388.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[8], line 34\u001B[0;36m\u001B[0m\n\u001B[0;31m    self.embedding.data[x] = torch.randn(embedding_dim) for x in special_tokens\u001B[0m\n\u001B[0m                                                        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class AttentionIsAllYouNeed(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, number_of_encoder_stacks: int, number_of_decoder_stacks: int):\n",
    "        super(AttentionIsAllYouNeed, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.embedding = None\n",
    "        self.positional_encoding = None\n",
    "\n",
    "        encoder_list = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_encoder_stacks):\n",
    "            encoder_list.append(EncoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_list)\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_decoder_stacks):\n",
    "            self.decoder.append(DecoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.final_layer = nn.Linear(d_v, d_v)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def init_embedding(self, pretrained_embeddings, freeze_embeddings = False, sparse = False):\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze = freeze_embeddings, sparse=sparse)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        for layer in self.decoder:\n",
    "            layer.set_mask(mask)\n",
    "\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        X_encoded = self.encoder(X)\n",
    "        for stack in self.decoder:\n",
    "            Y = stack(Y, X_encoded, X_encoded)\n",
    "\n",
    "        Y_hat = self.final_layer(Y)\n",
    "\n",
    "        return Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_glove_pre_trained_with_special_tokens(dim = 100):\n",
    "    glove_vectors = GloVe(name='6B', dim=dim)\n",
    "\n",
    "    pad_token = \"<pad>\"\n",
    "    pad_index = 0\n",
    "\n",
    "    eos_token = \"<eos>\"\n",
    "    eos_index = 1\n",
    "\n",
    "    bos_token = \"<bos>\"\n",
    "    bos_index = 2\n",
    "\n",
    "    glove_vocab = vocab(glove_vectors.stoi)\n",
    "\n",
    "    glove_vocab.insert_token(pad_token, pad_index)\n",
    "    glove_vocab.insert_token(eos_token, eos_index)\n",
    "    glove_vocab.insert_token(bos_token, bos_index)\n",
    "\n",
    "    glove_vocab.set_default_index(bos_index)\n",
    "\n",
    "    pretrained_embeddings = glove_vectors.vectors\n",
    "    pretrained_embeddings = torch.cat((torch.randn(3,pretrained_embeddings.shape[1]),pretrained_embeddings))\n",
    "    pretrained_embeddings[0] = torch.zeros(pretrained_embeddings.shape[1]) # Setting padding token embedding as 0s\n",
    "\n",
    "    return pretrained_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_vectors = GloVe(name='6B', dim=100)\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "pad_index = 0\n",
    "\n",
    "eos_token = \"<eos>\"\n",
    "eos_index = 1\n",
    "\n",
    "bos_token = \"<bos>\"\n",
    "bos_index = 2\n",
    "\n",
    "glove_vocab = vocab(glove_vectors.stoi)\n",
    "\n",
    "glove_vocab.insert_token(pad_token, pad_index)\n",
    "# glove_vocab.set_default_index(pad_index)\n",
    "\n",
    "glove_vocab.insert_token(eos_token, eos_index)\n",
    "# glove_vocab.set_default_index(eos_index)\n",
    "\n",
    "glove_vocab.insert_token(bos_token, bos_index)\n",
    "# glove_vocab.set_default_index(bos_index)\n",
    "\n",
    "glove_vocab.set_default_index(bos_index)\n",
    "\n",
    "pretrained_embeddings = glove_vectors.vectors\n",
    "pretrained_embeddings = torch.cat((torch.randn(3,pretrained_embeddings.shape[1]),pretrained_embeddings))\n",
    "pretrained_embeddings[0] = torch.zeros(pretrained_embeddings.shape[1]) # Setting padding token embedding as 0s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pretrained_embeddings[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_vocab.get_itos()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n         [ 1.0925e+00,  2.0762e+00,  1.1140e+00, -3.6582e-01,  1.4256e+00,\n           1.8005e-01,  1.8415e+00, -5.7231e-01,  2.5149e-01,  1.7394e-01,\n          -1.0700e+00,  3.6377e-01,  2.0141e+00,  2.2822e-01,  1.1281e+00,\n           1.3456e-02, -1.0482e-01,  2.5672e+00,  1.1370e+00,  1.1357e+00,\n          -6.8090e-01,  1.5728e+00,  8.9112e-01, -4.2575e-01,  7.1284e-01,\n           1.8635e+00, -8.8090e-01,  5.6104e-01,  5.5713e-01,  1.3830e+00,\n          -1.1469e+00, -7.0841e-01, -9.5328e-01, -1.0426e-01,  8.2447e-01,\n           1.9476e+00,  4.9286e-01,  1.0591e-01, -2.4496e-01,  3.8890e-01,\n          -5.5786e-01,  2.1935e+00, -5.5729e-01, -1.0753e+00, -6.5333e-01,\n           7.5299e-02,  5.2578e-02, -4.1617e-01, -2.1431e+00, -2.7614e-02,\n           4.0962e-01,  2.4454e+00,  9.0703e-02,  1.4453e+00, -1.0706e+00,\n          -1.9794e-02, -1.6528e+00,  1.7690e+00, -1.5916e+00,  9.1966e-01,\n           5.4821e-01,  2.5323e+00, -3.5183e-01,  2.1138e+00,  1.2109e+00,\n           7.3016e-01,  2.0564e-02, -3.5774e-01, -5.4468e-01,  5.5756e-01,\n           7.4085e-01,  9.9527e-01,  8.5064e-01,  1.4876e+00, -4.7829e-01,\n           1.1823e+00,  6.1663e-01,  2.4063e+00, -7.4562e-01, -7.6876e-01,\n           4.5865e-01,  1.5512e+00,  1.3772e+00,  1.3480e+00, -3.5806e-01,\n           2.4302e+00, -7.2539e-02,  1.4877e+00,  1.0680e+00,  8.4839e-01,\n          -8.8289e-01,  1.4389e+00, -2.2001e+00,  1.7769e+00, -8.8425e-01,\n           1.3107e+00,  2.1233e-02,  1.1637e+00, -6.0939e-01,  2.9489e+00],\n         [-4.4656e-01,  1.1719e+00,  8.4363e-01,  1.3858e-01, -2.3328e-01,\n          -5.3259e-01,  8.1917e-01, -4.7006e-01,  8.8459e-01,  1.5878e+00,\n           1.6327e+00,  8.6351e-01,  7.6309e-01, -2.3439e-01, -6.7029e-02,\n           4.6599e-01,  1.8623e+00, -3.1894e-02,  1.4087e+00, -3.1653e-01,\n          -4.4525e-01,  2.9896e+00, -5.7595e-01,  2.8731e+00,  1.5945e+00,\n           1.2998e+00,  1.7249e+00,  6.4619e-01,  1.9290e-04,  1.5912e+00,\n           7.1524e-01, -1.6069e+00, -1.7227e-01,  5.3258e-01,  1.3292e+00,\n          -1.9762e+00, -3.9484e-01,  1.0376e+00, -3.8016e-01, -2.6562e-01,\n           1.8333e+00,  1.5221e+00,  2.5888e+00,  8.1354e-01,  8.8342e-01,\n           1.4043e+00, -7.7523e-01,  1.2933e+00,  6.8377e-01,  1.1327e+00,\n           4.1289e-01,  1.0972e+00,  3.3950e-01,  6.8776e-01,  1.9017e+00,\n           8.9674e-02,  1.3326e-01, -6.2480e-01, -9.0837e-01, -5.9533e-01,\n           1.3096e+00,  1.5751e+00, -6.1045e-01,  1.3460e+00,  9.2291e-01,\n           4.0729e+00,  1.1478e+00, -4.2338e-01,  1.2782e+00,  8.9571e-01,\n          -9.6944e-01,  8.9813e-01,  1.1847e+00,  7.2071e-02, -1.7156e+00,\n           1.4379e+00, -2.5751e-01,  6.9968e-01,  9.0181e-02, -5.5956e-01,\n          -1.2684e+00,  1.7066e+00, -1.2453e-01,  1.8260e+00,  7.5367e-01,\n           2.4442e+00,  7.1456e-02,  1.0857e+00,  1.1434e-01,  1.2704e+00,\n          -3.4453e-01,  2.0361e+00,  6.4492e-01, -2.5086e-01, -1.0526e+00,\n           1.6010e+00, -3.4192e-01,  1.2125e+00,  6.9874e-01,  7.3702e-01],\n         [ 1.4112e-01, -9.8999e-01,  6.0224e-01, -7.9832e-01,  8.7532e-01,\n          -4.8354e-01,  9.8793e-01, -1.5490e-01,  9.9091e-01,  1.3450e-01,\n           9.2997e-01,  3.6764e-01,  8.3788e-01,  5.4585e-01,  7.3541e-01,\n           6.7762e-01,  6.3442e-01,  7.7299e-01,  5.4101e-01,  8.4102e-01,\n           4.5775e-01,  8.8908e-01,  3.8525e-01,  9.2281e-01,  3.2304e-01,\n           9.4638e-01,  2.7020e-01,  9.6280e-01,  2.2561e-01,  9.7422e-01,\n           1.8816e-01,  9.8214e-01,  1.5679e-01,  9.8763e-01,  1.3058e-01,\n           9.9144e-01,  1.0871e-01,  9.9407e-01,  9.0475e-02,  9.9590e-01,\n           7.5285e-02,  9.9716e-01,  6.2638e-02,  9.9804e-01,  5.2110e-02,\n           9.9864e-01,  4.3350e-02,  9.9906e-01,  3.6060e-02,  9.9935e-01,\n           2.9995e-02,  9.9955e-01,  2.4950e-02,  9.9969e-01,  2.0753e-02,\n           9.9978e-01,  1.7262e-02,  9.9985e-01,  1.4358e-02,  9.9990e-01,\n           1.1943e-02,  9.9993e-01,  9.9338e-03,  9.9995e-01,  8.2626e-03,\n           9.9997e-01,  6.8725e-03,  9.9998e-01,  5.7164e-03,  9.9998e-01,\n           4.7547e-03,  9.9999e-01,  3.9548e-03,  9.9999e-01,  3.2894e-03,\n           9.9999e-01,  2.7360e-03,  1.0000e+00,  2.2757e-03,  1.0000e+00,\n           1.8929e-03,  1.0000e+00,  1.5744e-03,  1.0000e+00,  1.3095e-03,\n           1.0000e+00,  1.0892e-03,  1.0000e+00,  9.0599e-04,  1.0000e+00,\n           7.5357e-04,  1.0000e+00,  6.2679e-04,  1.0000e+00,  5.2134e-04,\n           1.0000e+00,  4.3363e-04,  1.0000e+00,  3.6068e-04,  1.0000e+00],\n         [-7.5680e-01, -6.5364e-01, -1.8440e-01, -9.8285e-01,  3.6559e-01,\n          -9.3078e-01,  7.4453e-01, -6.6759e-01,  9.4151e-01, -3.3700e-01,\n           9.9977e-01, -2.1631e-02,  9.6983e-01,  2.4379e-01,  8.9197e-01,\n           4.5209e-01,  7.9338e-01,  6.0872e-01,  6.9050e-01,  7.2333e-01,\n           5.9234e-01,  8.0569e-01,  5.0320e-01,  8.6417e-01,  4.2466e-01,\n           9.0535e-01,  3.5677e-01,  9.3419e-01,  2.9880e-01,  9.5432e-01,\n           2.4971e-01,  9.6832e-01,  2.0838e-01,  9.7805e-01,  1.7372e-01,\n           9.8480e-01,  1.4472e-01,  9.8947e-01,  1.2050e-01,  9.9271e-01,\n           1.0031e-01,  9.9496e-01,  8.3475e-02,  9.9651e-01,  6.9456e-02,\n           9.9758e-01,  5.7785e-02,  9.9833e-01,  4.8072e-02,  9.9884e-01,\n           3.9989e-02,  9.9920e-01,  3.3264e-02,  9.9945e-01,  2.7670e-02,\n           9.9962e-01,  2.3016e-02,  9.9974e-01,  1.9144e-02,  9.9982e-01,\n           1.5924e-02,  9.9987e-01,  1.3245e-02,  9.9991e-01,  1.1017e-02,\n           9.9994e-01,  9.1633e-03,  9.9996e-01,  7.6218e-03,  9.9997e-01,\n           6.3395e-03,  9.9998e-01,  5.2730e-03,  9.9999e-01,  4.3859e-03,\n           9.9999e-01,  3.6480e-03,  9.9999e-01,  3.0343e-03,  1.0000e+00,\n           2.5238e-03,  1.0000e+00,  2.0992e-03,  1.0000e+00,  1.7461e-03,\n           1.0000e+00,  1.4523e-03,  1.0000e+00,  1.2080e-03,  1.0000e+00,\n           1.0048e-03,  1.0000e+00,  8.3572e-04,  1.0000e+00,  6.9512e-04,\n           1.0000e+00,  5.7818e-04,  1.0000e+00,  4.8091e-04,  1.0000e+00]],\n\n        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n           0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n           1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n         [ 1.0925e+00,  2.0762e+00,  1.1140e+00, -3.6582e-01,  1.4256e+00,\n           1.8005e-01,  1.8415e+00, -5.7231e-01,  2.5149e-01,  1.7394e-01,\n          -1.0700e+00,  3.6377e-01,  2.0141e+00,  2.2822e-01,  1.1281e+00,\n           1.3456e-02, -1.0482e-01,  2.5672e+00,  1.1370e+00,  1.1357e+00,\n          -6.8090e-01,  1.5728e+00,  8.9112e-01, -4.2575e-01,  7.1284e-01,\n           1.8635e+00, -8.8090e-01,  5.6104e-01,  5.5713e-01,  1.3830e+00,\n          -1.1469e+00, -7.0841e-01, -9.5328e-01, -1.0426e-01,  8.2447e-01,\n           1.9476e+00,  4.9286e-01,  1.0591e-01, -2.4496e-01,  3.8890e-01,\n          -5.5786e-01,  2.1935e+00, -5.5729e-01, -1.0753e+00, -6.5333e-01,\n           7.5299e-02,  5.2578e-02, -4.1617e-01, -2.1431e+00, -2.7614e-02,\n           4.0962e-01,  2.4454e+00,  9.0703e-02,  1.4453e+00, -1.0706e+00,\n          -1.9794e-02, -1.6528e+00,  1.7690e+00, -1.5916e+00,  9.1966e-01,\n           5.4821e-01,  2.5323e+00, -3.5183e-01,  2.1138e+00,  1.2109e+00,\n           7.3016e-01,  2.0564e-02, -3.5774e-01, -5.4468e-01,  5.5756e-01,\n           7.4085e-01,  9.9527e-01,  8.5064e-01,  1.4876e+00, -4.7829e-01,\n           1.1823e+00,  6.1663e-01,  2.4063e+00, -7.4562e-01, -7.6876e-01,\n           4.5865e-01,  1.5512e+00,  1.3772e+00,  1.3480e+00, -3.5806e-01,\n           2.4302e+00, -7.2539e-02,  1.4877e+00,  1.0680e+00,  8.4839e-01,\n          -8.8289e-01,  1.4389e+00, -2.2001e+00,  1.7769e+00, -8.8425e-01,\n           1.3107e+00,  2.1233e-02,  1.1637e+00, -6.0939e-01,  2.9489e+00],\n         [-4.4656e-01,  1.1719e+00,  8.4363e-01,  1.3858e-01, -2.3328e-01,\n          -5.3259e-01,  8.1917e-01, -4.7006e-01,  8.8459e-01,  1.5878e+00,\n           1.6327e+00,  8.6351e-01,  7.6309e-01, -2.3439e-01, -6.7029e-02,\n           4.6599e-01,  1.8623e+00, -3.1894e-02,  1.4087e+00, -3.1653e-01,\n          -4.4525e-01,  2.9896e+00, -5.7595e-01,  2.8731e+00,  1.5945e+00,\n           1.2998e+00,  1.7249e+00,  6.4619e-01,  1.9290e-04,  1.5912e+00,\n           7.1524e-01, -1.6069e+00, -1.7227e-01,  5.3258e-01,  1.3292e+00,\n          -1.9762e+00, -3.9484e-01,  1.0376e+00, -3.8016e-01, -2.6562e-01,\n           1.8333e+00,  1.5221e+00,  2.5888e+00,  8.1354e-01,  8.8342e-01,\n           1.4043e+00, -7.7523e-01,  1.2933e+00,  6.8377e-01,  1.1327e+00,\n           4.1289e-01,  1.0972e+00,  3.3950e-01,  6.8776e-01,  1.9017e+00,\n           8.9674e-02,  1.3326e-01, -6.2480e-01, -9.0837e-01, -5.9533e-01,\n           1.3096e+00,  1.5751e+00, -6.1045e-01,  1.3460e+00,  9.2291e-01,\n           4.0729e+00,  1.1478e+00, -4.2338e-01,  1.2782e+00,  8.9571e-01,\n          -9.6944e-01,  8.9813e-01,  1.1847e+00,  7.2071e-02, -1.7156e+00,\n           1.4379e+00, -2.5751e-01,  6.9968e-01,  9.0181e-02, -5.5956e-01,\n          -1.2684e+00,  1.7066e+00, -1.2453e-01,  1.8260e+00,  7.5367e-01,\n           2.4442e+00,  7.1456e-02,  1.0857e+00,  1.1434e-01,  1.2704e+00,\n          -3.4453e-01,  2.0361e+00,  6.4492e-01, -2.5086e-01, -1.0526e+00,\n           1.6010e+00, -3.4192e-01,  1.2125e+00,  6.9874e-01,  7.3702e-01],\n         [ 1.4112e-01, -9.8999e-01,  6.0224e-01, -7.9832e-01,  8.7532e-01,\n          -4.8354e-01,  9.8793e-01, -1.5490e-01,  9.9091e-01,  1.3450e-01,\n           9.2997e-01,  3.6764e-01,  8.3788e-01,  5.4585e-01,  7.3541e-01,\n           6.7762e-01,  6.3442e-01,  7.7299e-01,  5.4101e-01,  8.4102e-01,\n           4.5775e-01,  8.8908e-01,  3.8525e-01,  9.2281e-01,  3.2304e-01,\n           9.4638e-01,  2.7020e-01,  9.6280e-01,  2.2561e-01,  9.7422e-01,\n           1.8816e-01,  9.8214e-01,  1.5679e-01,  9.8763e-01,  1.3058e-01,\n           9.9144e-01,  1.0871e-01,  9.9407e-01,  9.0475e-02,  9.9590e-01,\n           7.5285e-02,  9.9716e-01,  6.2638e-02,  9.9804e-01,  5.2110e-02,\n           9.9864e-01,  4.3350e-02,  9.9906e-01,  3.6060e-02,  9.9935e-01,\n           2.9995e-02,  9.9955e-01,  2.4950e-02,  9.9969e-01,  2.0753e-02,\n           9.9978e-01,  1.7262e-02,  9.9985e-01,  1.4358e-02,  9.9990e-01,\n           1.1943e-02,  9.9993e-01,  9.9338e-03,  9.9995e-01,  8.2626e-03,\n           9.9997e-01,  6.8725e-03,  9.9998e-01,  5.7164e-03,  9.9998e-01,\n           4.7547e-03,  9.9999e-01,  3.9548e-03,  9.9999e-01,  3.2894e-03,\n           9.9999e-01,  2.7360e-03,  1.0000e+00,  2.2757e-03,  1.0000e+00,\n           1.8929e-03,  1.0000e+00,  1.5744e-03,  1.0000e+00,  1.3095e-03,\n           1.0000e+00,  1.0892e-03,  1.0000e+00,  9.0599e-04,  1.0000e+00,\n           7.5357e-04,  1.0000e+00,  6.2679e-04,  1.0000e+00,  5.2134e-04,\n           1.0000e+00,  4.3363e-04,  1.0000e+00,  3.6068e-04,  1.0000e+00],\n         [-7.5680e-01, -6.5364e-01, -1.8440e-01, -9.8285e-01,  3.6559e-01,\n          -9.3078e-01,  7.4453e-01, -6.6759e-01,  9.4151e-01, -3.3700e-01,\n           9.9977e-01, -2.1631e-02,  9.6983e-01,  2.4379e-01,  8.9197e-01,\n           4.5209e-01,  7.9338e-01,  6.0872e-01,  6.9050e-01,  7.2333e-01,\n           5.9234e-01,  8.0569e-01,  5.0320e-01,  8.6417e-01,  4.2466e-01,\n           9.0535e-01,  3.5677e-01,  9.3419e-01,  2.9880e-01,  9.5432e-01,\n           2.4971e-01,  9.6832e-01,  2.0838e-01,  9.7805e-01,  1.7372e-01,\n           9.8480e-01,  1.4472e-01,  9.8947e-01,  1.2050e-01,  9.9271e-01,\n           1.0031e-01,  9.9496e-01,  8.3475e-02,  9.9651e-01,  6.9456e-02,\n           9.9758e-01,  5.7785e-02,  9.9833e-01,  4.8072e-02,  9.9884e-01,\n           3.9989e-02,  9.9920e-01,  3.3264e-02,  9.9945e-01,  2.7670e-02,\n           9.9962e-01,  2.3016e-02,  9.9974e-01,  1.9144e-02,  9.9982e-01,\n           1.5924e-02,  9.9987e-01,  1.3245e-02,  9.9991e-01,  1.1017e-02,\n           9.9994e-01,  9.1633e-03,  9.9996e-01,  7.6218e-03,  9.9997e-01,\n           6.3395e-03,  9.9998e-01,  5.2730e-03,  9.9999e-01,  4.3859e-03,\n           9.9999e-01,  3.6480e-03,  9.9999e-01,  3.0343e-03,  1.0000e+00,\n           2.5238e-03,  1.0000e+00,  2.0992e-03,  1.0000e+00,  1.7461e-03,\n           1.0000e+00,  1.4523e-03,  1.0000e+00,  1.2080e-03,  1.0000e+00,\n           1.0048e-03,  1.0000e+00,  8.3572e-04,  1.0000e+00,  6.9512e-04,\n           1.0000e+00,  5.7818e-04,  1.0000e+00,  4.8091e-04,  1.0000e+00]]])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000],\n         [ 0.2510,  1.5359,  0.3749, -1.0394,  0.7876, -0.5900,  1.2973,\n          -1.4113, -0.2091, -0.7137, -1.4576, -0.5580,  1.6890, -0.7175,\n           0.8561, -0.9489, -0.3319,  1.5933,  0.9476,  0.1538, -0.8387,\n           0.5853,  0.7597, -1.4171,  0.6034,  0.8695, -0.9720, -0.4348,\n           0.4813,  0.3859, -1.2099, -1.7064, -1.0057, -1.1029,  0.7808,\n           0.9486,  0.4566, -0.8934, -0.2752, -0.6106, -0.5830,  1.1938,\n          -0.5782, -2.0751, -0.6707, -0.9246,  0.0381, -1.4161, -2.1551,\n          -1.0275,  0.3996,  1.4455,  0.0824,  0.4453, -1.0776, -1.0198,\n          -1.6586,  0.7690, -1.5964, -0.0803,  0.5442,  1.5323, -0.3551,\n           1.1138,  1.2081, -0.2698,  0.0183, -1.3577, -0.5466, -0.4424,\n           0.7393, -0.0047,  0.8493,  0.4876, -0.4794,  0.1823,  0.6157,\n           1.4063, -0.7464, -1.7688,  0.4580,  0.5512,  1.3766,  0.3480,\n          -0.3585,  1.4302, -0.0729,  0.4877,  1.0677, -0.1516, -0.8831,\n           0.4389, -2.2003,  0.7769, -0.8844,  0.3107,  0.0211,  0.1637,\n          -0.6095,  1.9489],\n         [-1.3559,  1.5881, -0.1521,  0.2312, -1.2158, -0.7186, -0.0940,\n          -0.8777,  0.0670,  1.0121,  0.9180,  0.1641,  0.1482, -1.0230,\n          -0.5904, -0.3861,  1.4200, -0.9288,  1.0368, -1.2448, -0.7569,\n           2.0394, -0.8366,  1.9077,  1.3770,  0.3237,  1.5435, -0.3372,\n          -0.1509,  0.6026,  0.5894, -2.5990, -0.2770, -0.4619,  1.2420,\n          -2.9724, -0.4674,  0.0403, -0.4405, -1.2638,  1.7831,  0.5233,\n           2.5470, -0.1856,  0.8487,  0.4049, -0.8041,  0.2938,  0.6597,\n           0.1330,  0.3929,  0.0974,  0.3229, -0.3121,  1.8879, -0.9102,\n           0.1218, -1.6247, -0.9179, -1.5953,  1.3017,  0.5752, -0.6171,\n           0.3460,  0.9174,  3.0730,  1.1433, -1.4234,  1.2744, -0.1043,\n          -0.9726, -0.1019,  1.1821, -0.9279, -1.7178,  0.4379, -0.2593,\n          -0.3003,  0.0887, -1.5596, -1.2696,  0.7066, -0.1256,  0.8260,\n           0.7528,  1.4442,  0.0707,  0.0857,  0.1137,  0.2704, -0.3450,\n           1.0361,  0.6445, -1.2509, -1.0530,  0.6010, -0.3422,  0.2125,\n           0.6985, -0.2630],\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000],\n         [ 0.2510,  1.5359,  0.3749, -1.0394,  0.7876, -0.5900,  1.2973,\n          -1.4113, -0.2091, -0.7137, -1.4576, -0.5580,  1.6890, -0.7175,\n           0.8561, -0.9489, -0.3319,  1.5933,  0.9476,  0.1538, -0.8387,\n           0.5853,  0.7597, -1.4171,  0.6034,  0.8695, -0.9720, -0.4348,\n           0.4813,  0.3859, -1.2099, -1.7064, -1.0057, -1.1029,  0.7808,\n           0.9486,  0.4566, -0.8934, -0.2752, -0.6106, -0.5830,  1.1938,\n          -0.5782, -2.0751, -0.6707, -0.9246,  0.0381, -1.4161, -2.1551,\n          -1.0275,  0.3996,  1.4455,  0.0824,  0.4453, -1.0776, -1.0198,\n          -1.6586,  0.7690, -1.5964, -0.0803,  0.5442,  1.5323, -0.3551,\n           1.1138,  1.2081, -0.2698,  0.0183, -1.3577, -0.5466, -0.4424,\n           0.7393, -0.0047,  0.8493,  0.4876, -0.4794,  0.1823,  0.6157,\n           1.4063, -0.7464, -1.7688,  0.4580,  0.5512,  1.3766,  0.3480,\n          -0.3585,  1.4302, -0.0729,  0.4877,  1.0677, -0.1516, -0.8831,\n           0.4389, -2.2003,  0.7769, -0.8844,  0.3107,  0.0211,  0.1637,\n          -0.6095,  1.9489],\n         [-1.3559,  1.5881, -0.1521,  0.2312, -1.2158, -0.7186, -0.0940,\n          -0.8777,  0.0670,  1.0121,  0.9180,  0.1641,  0.1482, -1.0230,\n          -0.5904, -0.3861,  1.4200, -0.9288,  1.0368, -1.2448, -0.7569,\n           2.0394, -0.8366,  1.9077,  1.3770,  0.3237,  1.5435, -0.3372,\n          -0.1509,  0.6026,  0.5894, -2.5990, -0.2770, -0.4619,  1.2420,\n          -2.9724, -0.4674,  0.0403, -0.4405, -1.2638,  1.7831,  0.5233,\n           2.5470, -0.1856,  0.8487,  0.4049, -0.8041,  0.2938,  0.6597,\n           0.1330,  0.3929,  0.0974,  0.3229, -0.3121,  1.8879, -0.9102,\n           0.1218, -1.6247, -0.9179, -1.5953,  1.3017,  0.5752, -0.6171,\n           0.3460,  0.9174,  3.0730,  1.1433, -1.4234,  1.2744, -0.1043,\n          -0.9726, -0.1019,  1.1821, -0.9279, -1.7178,  0.4379, -0.2593,\n          -0.3003,  0.0887, -1.5596, -1.2696,  0.7066, -0.1256,  0.8260,\n           0.7528,  1.4442,  0.0707,  0.0857,  0.1137,  0.2704, -0.3450,\n           1.0361,  0.6445, -1.2509, -1.0530,  0.6010, -0.3422,  0.2125,\n           0.6985, -0.2630],\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n           0.0000,  0.0000]]])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_dim = 100\n",
    "embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze = True, sparse=True)\n",
    "embedding.eval()\n",
    "pos_encoding = PositionalEncoding(encoding_dim)\n",
    "pos_encoding.eval()\n",
    "\n",
    "X = embedding(torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 0, 0]]))\n",
    "X_pos = pos_encoding(X)\n",
    "display(X_pos)\n",
    "display(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1000, 100])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding.P.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 60, 32])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n           0.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  ...,  1.0000e+00,\n           1.7783e-04,  1.0000e+00],\n         [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  ...,  1.0000e+00,\n           3.5566e-04,  1.0000e+00],\n         ...,\n         [ 4.3616e-01,  8.9987e-01,  5.9521e-01,  ...,  9.9984e-01,\n           1.0136e-02,  9.9995e-01],\n         [ 9.9287e-01,  1.1918e-01,  9.3199e-01,  ...,  9.9983e-01,\n           1.0314e-02,  9.9995e-01],\n         [ 6.3674e-01, -7.7108e-01,  9.8174e-01,  ...,  9.9983e-01,\n           1.0492e-02,  9.9994e-01]]])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_dim, num_steps = 32, 60\n",
    "pos_encoding = PositionalEncoding(encoding_dim)\n",
    "pos_encoding.eval()\n",
    "X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))\n",
    "P = pos_encoding.P[:, :X.shape[1], :]\n",
    "display(P.shape)\n",
    "display(P)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[        nan,  4.0096e+37,  6.2351e+37, -4.2478e+36, -2.0465e+37,\n",
      "          -4.1833e+36,         nan,         nan, -7.8783e+36,  1.0117e+37,\n",
      "          -9.4423e+35,  2.0174e+36, -5.8751e+36, -3.1546e+36, -3.0629e+36,\n",
      "          -1.6364e+37, -1.8854e+36, -5.5364e+35,  3.2821e+36, -3.7999e+36,\n",
      "          -8.9872e+35, -2.5219e+36,  3.8568e+36, -5.8303e+36,  1.0008e+37,\n",
      "           4.1014e+35, -5.0816e+36, -1.4643e+35, -4.0584e+37, -1.0357e+38,\n",
      "                  nan, -3.9588e+36,  1.1894e+36,  9.2627e+36,         nan,\n",
      "           3.2849e+36, -2.2424e+36,  6.2773e+36,  6.5724e+36, -2.8173e+36,\n",
      "          -1.7648e+37, -4.5027e+37,  9.5633e+35, -3.9691e+37,  2.6361e+36,\n",
      "           6.8147e+36,  2.2129e+37,  4.7346e+36,  3.1635e+36,  1.1845e+36,\n",
      "          -9.1324e+36,         nan,  3.2725e+37, -2.1265e+37, -2.5488e+37,\n",
      "                  nan, -1.6300e+37, -9.5616e+36,         nan, -1.2026e+37,\n",
      "          -3.9547e+36, -6.9687e+36,         nan, -1.0574e+37],\n",
      "         [        nan,  4.5371e+37,  4.7310e+37,  5.4921e+35, -1.1456e+37,\n",
      "          -5.6767e+36,         nan,         nan, -1.7323e+37,  1.1040e+37,\n",
      "          -7.2874e+35,  2.3083e+35, -3.2761e+36, -1.7506e+36, -2.2463e+36,\n",
      "          -1.1745e+37, -2.2174e+36, -1.2140e+36,  3.5099e+36, -4.3533e+36,\n",
      "          -1.3467e+36, -2.7611e+36,  6.2095e+36, -5.3817e+36,  8.0789e+36,\n",
      "           1.7980e+36, -1.5847e+37, -7.4729e+35, -2.5404e+37, -7.9355e+37,\n",
      "                  nan, -8.5253e+35,  1.1945e+37,  6.3805e+36,         nan,\n",
      "          -4.4083e+35, -2.8747e+36,  6.0402e+36, -2.0758e+37, -2.3559e+36,\n",
      "          -1.8084e+37, -5.2724e+37,  1.5476e+36, -2.5422e+37,  1.9936e+36,\n",
      "           4.9472e+36,  2.3681e+37,  3.7612e+36,  2.3868e+36,  1.3099e+36,\n",
      "          -1.7271e+37,         nan,  3.9787e+37, -2.3462e+37, -3.1127e+37,\n",
      "                  nan, -7.4143e+36, -5.7982e+36,         nan, -9.0302e+36,\n",
      "          -2.3564e+36, -2.4159e+37,         nan, -1.0047e+37],\n",
      "         [        nan,  4.9118e+37,  2.8799e+37, -2.3690e+36, -1.3319e+37,\n",
      "          -6.3740e+36,         nan,         nan, -1.2797e+37,  1.8175e+37,\n",
      "           6.8685e+35,  1.9020e+36, -4.2108e+36, -4.3006e+35, -2.8865e+36,\n",
      "          -1.1824e+37, -1.8071e+36, -5.8824e+35,  7.4211e+36, -2.5073e+36,\n",
      "          -2.7051e+35, -3.2702e+36,  3.5792e+36, -5.2558e+36,  9.0414e+36,\n",
      "           3.1522e+36, -1.2399e+37, -1.6784e+35, -3.4883e+37, -4.8289e+37,\n",
      "                  nan, -3.7508e+35, -4.6989e+36,  5.9225e+36,         nan,\n",
      "           4.4778e+36, -3.5246e+36,  5.1440e+36, -3.7284e+37, -1.4928e+36,\n",
      "          -1.9490e+37, -2.9684e+37,  2.5439e+36, -3.7528e+37,  2.1151e+36,\n",
      "           4.8459e+36,  3.1743e+37,  5.6058e+36,  2.5647e+36,  1.6227e+36,\n",
      "          -2.3841e+37,         nan,  7.2269e+36, -2.1338e+37, -4.1046e+37,\n",
      "                  nan, -7.5939e+36, -7.1677e+36,         nan, -9.5367e+36,\n",
      "          -4.7869e+35, -2.0132e+37,         nan, -1.3228e+37],\n",
      "         [        nan,  4.2701e+37,  3.1278e+37, -3.4594e+36, -1.3173e+37,\n",
      "          -4.4869e+36,         nan,         nan, -2.0866e+37,  1.4397e+37,\n",
      "           9.2448e+35,  1.7016e+36, -3.9572e+36, -2.2918e+36, -3.2081e+36,\n",
      "          -1.2101e+37, -3.4974e+36, -1.4807e+35,  6.6339e+36, -3.5434e+36,\n",
      "          -5.6288e+35, -2.7698e+36,  3.6293e+36, -5.5168e+36,  9.3535e+36,\n",
      "           9.5465e+35, -2.0705e+37, -1.5194e+36, -2.7286e+37, -2.0721e+37,\n",
      "                  nan, -1.8357e+36,  3.2049e+37,  7.1670e+36,         nan,\n",
      "           4.6845e+36, -4.2058e+36,  6.2472e+36,  2.4124e+36, -2.6578e+36,\n",
      "          -2.2141e+37, -2.8636e+37, -4.1369e+35, -3.0506e+37,  1.9976e+36,\n",
      "           5.8713e+36,  3.0222e+37,  5.1576e+36,  3.6138e+36,  1.6406e+36,\n",
      "          -2.7575e+37,         nan, -4.5744e+36, -2.1539e+37, -2.7313e+37,\n",
      "                  nan, -8.7414e+36, -7.4574e+36,         nan, -8.2005e+36,\n",
      "           3.8115e+35, -2.6973e+37,         nan, -1.3051e+37],\n",
      "         [        nan,  2.8682e+37,  6.1103e+37, -2.6967e+36, -1.5549e+37,\n",
      "          -5.8104e+36,         nan,         nan, -1.0133e+37,  6.5978e+36,\n",
      "          -8.2512e+35,  1.2320e+36, -4.6589e+36, -3.4753e+36, -2.1134e+36,\n",
      "          -1.4487e+37, -3.2187e+36,  7.7062e+35,  3.4364e+36, -4.9016e+36,\n",
      "          -8.6521e+35, -4.1841e+36,  2.6152e+36, -7.9143e+36,  8.8688e+36,\n",
      "           2.0476e+36, -4.5180e+36,  1.5368e+35, -2.7410e+37, -9.4083e+37,\n",
      "                  nan, -2.1257e+36,  9.6171e+36,  8.8870e+36,         nan,\n",
      "           3.1058e+36, -5.2376e+36,  4.9070e+36,  3.7013e+37, -3.1266e+36,\n",
      "          -2.1147e+37, -3.5390e+37, -2.3136e+35, -3.0475e+37,  1.3876e+36,\n",
      "           5.2787e+36,  2.8096e+37,  6.7754e+36,  3.1925e+36,  1.3650e+36,\n",
      "          -2.2867e+37,         nan, -2.0622e+36, -2.0660e+37, -3.1112e+37,\n",
      "                  nan, -1.1481e+37, -8.1132e+36,         nan, -1.1277e+37,\n",
      "          -4.6646e+35, -9.7187e+36,         nan, -8.9855e+36]],\n",
      "\n",
      "        [[        nan, -6.7382e+37, -4.2625e+37, -2.0470e+36, -1.2120e+37,\n",
      "           6.9545e+36,         nan,         nan,  4.5927e+36, -4.6779e+36,\n",
      "           3.6045e+36,  3.8402e+36, -3.9906e+35,  4.0838e+36, -4.7419e+35,\n",
      "          -5.7820e+36, -3.2959e+36,  2.5794e+36,  1.2670e+37,  2.2927e+36,\n",
      "          -6.5024e+36,  3.9236e+36, -4.9213e+36,  4.4147e+36, -2.3720e+36,\n",
      "           8.0673e+34,  3.2192e+36,  8.3080e+36, -2.1957e+36,  1.1026e+38,\n",
      "                  nan, -4.4608e+36,  2.4272e+37,  1.6779e+36,         nan,\n",
      "           1.2393e+37, -3.2946e+36, -1.3854e+36,  1.0151e+38, -2.3405e+36,\n",
      "          -8.0749e+34,  6.3272e+37, -5.7680e+35, -2.0210e+37,  1.2589e+36,\n",
      "           3.9819e+36,  1.5783e+37, -1.7765e+37,  2.7154e+36,  5.9493e+34,\n",
      "          -7.5366e+37,         nan,  5.1530e+37, -1.2407e+37,  9.6505e+37,\n",
      "                  nan, -1.2620e+37, -4.3970e+36,         nan, -6.5372e+36,\n",
      "          -1.5062e+36,  5.9815e+37,         nan, -1.8907e+36],\n",
      "         [        nan, -4.7847e+37, -7.2829e+37, -4.0247e+36, -9.4325e+36,\n",
      "           3.2184e+36,         nan,         nan,  2.0933e+36, -4.4742e+36,\n",
      "           3.3286e+36,  3.7517e+36, -7.4822e+35,  3.7271e+36,  5.6983e+34,\n",
      "          -5.2446e+36, -5.6281e+35,  3.2002e+36,  1.6434e+37,  4.0984e+36,\n",
      "          -6.8866e+36,  9.5101e+35, -8.3455e+36,  3.3271e+36, -3.5104e+36,\n",
      "          -9.1216e+35, -1.4527e+36,  1.0200e+37, -9.2892e+36,  1.0743e+38,\n",
      "                  nan, -4.9266e+36,  2.4732e+37,  2.8699e+35,         nan,\n",
      "           1.8105e+37, -3.9720e+36, -4.2993e+36,  1.2451e+38, -9.3465e+35,\n",
      "           5.9127e+35,  9.2176e+37,  1.0130e+36, -1.8885e+37,  8.3359e+35,\n",
      "           2.3037e+36,  5.7416e+37, -2.1029e+37,  2.3048e+36, -8.0261e+35,\n",
      "          -6.6816e+37,         nan,  3.6136e+37, -6.5235e+36,  8.6220e+37,\n",
      "                  nan, -1.1122e+37, -4.8830e+36,         nan, -8.2444e+36,\n",
      "          -1.5866e+36,  5.5652e+37,         nan, -4.3248e+35],\n",
      "         [        nan, -7.7807e+37, -8.4701e+37, -1.5271e+35, -1.0586e+37,\n",
      "           4.2972e+36,         nan,         nan,  8.2462e+36, -7.3330e+36,\n",
      "           3.9321e+36,  3.5485e+36, -1.1267e+36,  4.2229e+36, -5.0422e+34,\n",
      "          -5.1021e+36, -1.5769e+36,  2.3742e+36,  2.1336e+37,  3.6011e+36,\n",
      "          -7.0191e+36,  5.1292e+36, -6.4630e+36,  4.9931e+36, -1.5905e+36,\n",
      "          -2.1721e+35,  5.5742e+36,  8.1162e+36, -1.1983e+37,  1.0641e+38,\n",
      "                  nan, -5.1742e+36,  2.3310e+37,  2.7715e+36,         nan,\n",
      "           1.9239e+37, -2.8520e+36,  4.8822e+35,  1.4534e+38, -2.0154e+36,\n",
      "          -4.1000e+35,  8.3717e+37,  2.9008e+36, -1.7059e+37,  3.5598e+36,\n",
      "           5.8412e+36,  5.2233e+35, -1.8547e+37,  3.5854e+36,  5.3653e+35,\n",
      "          -8.1522e+37,         nan,  6.3159e+37, -1.2448e+37,  8.7578e+37,\n",
      "                  nan, -1.1996e+37, -2.9783e+36,         nan, -5.9547e+36,\n",
      "          -3.2199e+35,  6.0936e+37,         nan,  1.4890e+35],\n",
      "         [        nan, -4.0866e+37, -8.1404e+35, -3.8771e+36, -1.2921e+37,\n",
      "           9.6882e+36,         nan,         nan,  7.4035e+35, -1.1025e+36,\n",
      "           9.3987e+35,  3.4053e+36, -1.0225e+36,  2.9446e+36,  4.6211e+35,\n",
      "          -6.9564e+36,  8.7896e+35,  1.3443e+36,  2.3434e+36,  2.0795e+36,\n",
      "          -7.1560e+36,  6.1440e+35, -6.0527e+36,  1.5701e+36, -5.3739e+36,\n",
      "           7.3904e+35,  8.8220e+35,  7.2547e+36,  3.1726e+36,  7.9941e+37,\n",
      "                  nan, -1.9231e+36,  2.5558e+37,  1.0059e+36,         nan,\n",
      "           6.1404e+36, -3.1408e+36, -2.3816e+36,  6.9959e+37, -1.8776e+36,\n",
      "          -3.5930e+35,  4.1430e+37,  4.6489e+36, -1.5787e+37, -1.7887e+36,\n",
      "           4.5677e+36,  5.2622e+37, -1.7067e+37,  1.2779e+36,  1.3825e+35,\n",
      "          -6.5667e+37,         nan,  4.8443e+37, -4.4556e+36,  7.9856e+37,\n",
      "                  nan, -1.3251e+37, -3.3514e+36,         nan, -8.7271e+36,\n",
      "          -1.4011e+36,  6.8786e+37,         nan,  3.4693e+36],\n",
      "         [        nan, -5.0943e+37, -2.2526e+37, -3.2216e+36, -1.1693e+37,\n",
      "           6.9668e+36,         nan,         nan,  1.3614e+37, -4.3622e+36,\n",
      "           3.3956e+36,  3.7700e+36, -5.9977e+35,  4.0461e+36,  3.0312e+35,\n",
      "          -6.0069e+36,  1.1386e+36,  2.6835e+36,  1.1526e+37,  3.6401e+36,\n",
      "          -6.6442e+36,  1.1696e+36, -6.7622e+36,  3.2996e+36, -3.9768e+36,\n",
      "          -1.0856e+36,  1.2781e+37,  8.2962e+36, -1.5192e+37,  1.4187e+38,\n",
      "                  nan, -5.2713e+36,  1.8595e+37,  1.2509e+36,         nan,\n",
      "           1.1996e+37, -2.0380e+36, -9.9744e+35,  1.1079e+38, -1.6160e+36,\n",
      "           1.6075e+36,  6.6058e+37,  1.6729e+36, -2.0665e+37,  2.1890e+36,\n",
      "           3.2580e+36,  4.0523e+37, -1.9129e+37,  3.1845e+36, -2.2442e+35,\n",
      "          -6.7577e+37,         nan,  4.7854e+37, -7.3919e+36,  9.2492e+37,\n",
      "                  nan, -1.2633e+37, -3.5836e+36,         nan, -7.0847e+36,\n",
      "          -2.8390e+36,  6.1240e+37,         nan,  9.7956e+35]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)\n",
    "Y = torch.randn(batch_size, seq_length, d_k)\n",
    "\n",
    "attention_is_all_you_need = AttentionIsAllYouNeed(model_dim, d_k, model_dim, h, 1, 1)\n",
    "output = attention_is_all_you_need(X, Y)\n",
    "print(output.shape)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}