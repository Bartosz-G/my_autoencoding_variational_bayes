{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch.matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention1.set_mask(mask)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, max_len=1000, dropout = 0):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough `P`\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class AttentionIsAllYouNeed(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, number_of_encoder_stacks: int, number_of_decoder_stacks: int):\n",
    "        super(AttentionIsAllYouNeed, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.embedding = None\n",
    "        self.positional_encoding = None\n",
    "\n",
    "        encoder_list = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_encoder_stacks):\n",
    "            encoder_list.append(EncoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_list)\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_decoder_stacks):\n",
    "            self.decoder.append(DecoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.final_layer = nn.Linear(d_v, d_v)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def init_embedding(self, pretrained_embeddings, freeze_embeddings = False, sparse = False):\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze = freeze_embeddings, sparse=sparse)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        for layer in self.decoder:\n",
    "            layer.set_mask(mask)\n",
    "\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        X_encoded = self.encoder(X)\n",
    "        for stack in self.decoder:\n",
    "            Y = stack(Y, X_encoded, X_encoded)\n",
    "\n",
    "        Y_hat = self.final_layer(Y)\n",
    "\n",
    "        return Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "glove_vectors = GloVe(name='6B', dim=100)\n",
    "\n",
    "start_token = \"</s>\"\n",
    "start_index = 0\n",
    "\n",
    "glove_vocab = vocab(glove_vectors.stoi)\n",
    "glove_vocab.insert_token(start_token, start_index)\n",
    "glove_vocab.set_default_index(start_index)\n",
    "pretrained_embeddings = glove_vectors.vectors\n",
    "pretrained_embeddings = torch.cat((torch.zeros(1,pretrained_embeddings.shape[1]),pretrained_embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([400001, 100])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "487"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vocab[\"november\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 8.5703e-02,  7.7799e-01,  1.6569e-01,  1.1337e+00,  3.8239e-01,\n           1.3540e+00,  1.2870e-02,  1.2246e+00, -4.3817e-01,  1.5016e+00,\n          -3.5874e-01,  6.5017e-01,  5.5156e-02,  1.6965e+00, -1.7958e-01,\n           1.0679e+00,  3.9101e-01,  1.1604e+00, -2.6635e-01,  7.8862e-01,\n           5.3698e-01,  1.4938e+00,  9.3660e-01,  1.6690e+00,  2.1793e-01,\n           5.3358e-01,  2.2383e-01,  6.3796e-01, -1.7656e-01,  1.1748e+00,\n          -2.0367e-01,  1.1393e+00,  1.9832e-02,  8.9587e-01, -2.0244e-01,\n           1.5500e+00, -1.5460e-01,  1.9865e+00, -2.6863e-01,  7.0910e-01,\n          -3.2866e-01,  6.5812e-01, -1.6943e-01,  5.7999e-01, -4.6727e-02,\n           8.3673e-01,  7.0824e-01,  2.5089e-01, -9.1559e-02,  3.8220e-02,\n          -1.9747e-01,  1.1028e+00,  5.5221e-01,  2.3816e+00, -6.5636e-01,\n          -2.2502e+00, -3.1556e-01, -2.0550e-01,  1.7709e+00,  1.4026e+00,\n          -7.9827e-01,  2.1597e+00, -3.3042e-01,  1.3138e+00,  7.7386e-01,\n           1.2260e+00,  5.2471e-01,  9.6595e-01,  3.2048e-01,  1.0799e+00,\n           1.7752e-01,  5.0574e-01, -7.0045e-01,  5.5431e-01,  1.7244e-01,\n           1.2028e+00,  2.3292e-02,  7.9323e-01, -1.0158e+00,  1.1832e+00,\n           5.6752e-01,  1.3182e+00, -6.5011e-01,  1.6828e+00, -8.6585e-01,\n           9.4061e-01, -2.9264e-01,  4.4332e-01, -3.4705e-01,  6.7105e-01,\n           4.0215e-01,  8.7254e-01, -2.0228e-01,  1.8737e+00, -5.4500e-01,\n           1.7921e+00, -2.0695e-01,  9.2573e-01,  7.5808e-01,  6.5757e-01],\n         [ 1.4300e+00,  3.3780e-01,  1.4739e+00, -9.8069e-03,  4.4120e-01,\n           5.8988e-01,  1.5243e-01,  1.1807e+00, -1.4505e-01,  1.5258e+00,\n           1.2072e-01,  1.2867e+00, -7.8677e-02,  8.3228e-01, -3.1523e-01,\n           1.2461e+00,  1.0296e+00,  6.2084e-01,  4.9023e-01,  1.0608e+00,\n           6.0199e-01,  5.2841e-01,  9.2438e-01,  1.4950e+00,  4.3748e-01,\n           1.2743e+00, -4.0223e-01,  6.1102e-01,  3.6501e-02,  7.4882e-01,\n          -1.3575e-01,  2.1449e+00,  1.8474e-01,  1.9155e+00, -3.2375e-01,\n           1.8933e+00,  5.7890e-01,  1.6167e+00, -5.9186e-01,  6.8822e-01,\n          -4.8421e-01,  1.2330e+00,  1.1035e+00,  9.5514e-01, -1.1029e-01,\n           1.2761e+00, -1.8163e-02,  7.2593e-01,  7.8966e-01,  4.9132e-01,\n           4.8307e-02,  6.6316e-01,  4.3176e-01,  2.2271e+00, -5.3134e-01,\n          -2.2411e+00,  4.3201e-01,  1.0252e+00,  1.3996e+00,  1.6508e+00,\n           3.7231e-02,  1.3714e+00,  4.0771e-01,  1.3556e+00,  9.8540e-01,\n           3.8276e-01,  5.4130e-01,  1.7622e+00,  3.0880e-01,  1.3306e+00,\n           3.1114e-01,  8.4839e-01, -1.1181e-01,  1.8719e-01,  6.1560e-01,\n           5.5659e-01, -1.9072e-01,  9.1045e-01, -1.5919e+00,  1.3740e+00,\n           8.5920e-01,  1.5461e+00, -3.1876e-01,  1.5260e+00, -1.4798e+00,\n           2.0690e-02, -2.9354e-01,  8.5276e-01,  2.5833e-01,  8.1830e-01,\n           1.0152e+00,  1.7765e+00,  1.2619e-01,  1.5478e+00, -1.0314e+00,\n           1.0646e+00, -3.7509e-01,  5.5250e-02,  6.1814e-01,  1.3959e+00],\n         [ 1.2432e+00, -9.3751e-01,  1.2642e+00,  8.1562e-02,  1.1406e+00,\n           1.1417e+00,  5.1908e-01,  1.1610e+00,  6.9329e-01,  1.2212e+00,\n           5.8623e-01,  1.3097e+00,  7.6284e-01,  1.3500e+00,  4.0863e-01,\n           6.1874e-01,  6.8690e-01,  8.6684e-01,  2.6254e-01,  1.3478e+00,\n           1.3180e-01,  4.8553e-01,  7.2682e-01,  1.8514e+00,  1.0545e+00,\n           1.7362e+00, -4.7898e-01, -7.0829e-01,  9.1111e-01,  9.4435e-01,\n           2.0393e-01,  1.6815e+00,  7.5536e-01,  1.2529e+00, -5.4944e-01,\n           1.3277e+00, -1.4943e-01,  1.1193e+00, -5.5698e-02,  1.2337e+00,\n          -4.6832e-01,  8.6424e-01,  5.0634e-01,  6.3049e-01, -7.2691e-02,\n           1.1467e+00, -3.3902e-01,  8.6051e-01,  7.9476e-01,  5.2286e-01,\n          -1.1726e-02,  1.0599e+00,  9.2576e-01,  2.6531e+00,  5.8146e-02,\n          -2.1637e+00,  3.5031e-01,  5.6529e-01,  1.1541e+00,  1.2624e+00,\n           4.6014e-01,  2.5068e+00,  2.1731e-01,  1.4872e+00,  5.0037e-01,\n           9.2943e-01,  1.0577e+00,  1.5153e+00,  1.9612e-01,  1.2047e+00,\n          -4.1228e-01,  1.6355e+00,  7.9711e-02,  5.7925e-01,  3.6986e-01,\n           1.9389e+00, -5.8940e-01,  2.9052e-01, -8.7345e-01,  6.0177e-01,\n           2.4206e-01,  1.6995e+00,  4.0039e-01,  1.2958e+00, -1.8021e+00,\n          -2.9610e-01,  7.3217e-02,  1.2799e+00, -7.9476e-01,  3.7113e-01,\n           7.9454e-02,  1.0140e+00,  1.1706e+00,  1.4514e+00, -4.1685e-01,\n           6.5130e-01, -8.0271e-01,  6.8602e-01,  3.1932e-01,  1.6206e+00],\n         [ 1.0293e-01, -1.2349e+00,  1.3304e+00, -1.1979e+00,  9.5849e-01,\n          -4.3959e-01,  5.9652e-01,  1.7950e-01,  4.1546e-01,  2.2196e-01,\n           1.2178e+00,  3.0033e-01,  1.1469e+00,  2.8201e-01,  6.0310e-01,\n           4.7005e-01,  9.6837e-01,  4.3451e-01,  2.2358e-01,  3.5766e-01,\n           6.0415e-01,  5.1604e-01,  7.3102e-01,  9.7485e-01,  7.7250e-01,\n           4.7667e-01,  2.9648e-01,  4.2125e-01,  7.0434e-02,  8.3315e-01,\n           1.4844e-01,  1.2649e+00,  3.0072e-01,  1.2223e+00, -1.7963e-01,\n           1.0776e+00,  3.1268e-01,  1.5203e+00,  2.6211e-01,  9.1352e-01,\n          -6.4258e-01,  5.8185e-01,  2.6599e-01,  8.7041e-01,  4.6578e-01,\n           1.5505e+00,  6.2243e-01,  6.6429e-01, -3.2953e-01,  4.5078e-01,\n          -3.2896e-02,  1.2654e+00,  3.2700e-01,  1.9974e+00, -7.8406e-01,\n          -2.0245e+00,  2.9802e-02,  6.3043e-01,  2.2311e+00,  1.7219e+00,\n          -2.3784e-01,  1.9213e+00,  4.4448e-02,  1.4674e+00,  1.1162e+00,\n           8.0639e-01, -6.7702e-02,  1.2335e+00, -4.6346e-02,  7.7954e-01,\n           6.1917e-02,  8.4193e-01, -3.0403e-01,  5.8374e-01,  3.8301e-01,\n           1.1501e+00, -5.2938e-01,  7.9450e-01, -1.2503e+00,  1.0716e+00,\n           7.0754e-01,  1.4974e+00, -4.1906e-01,  1.2615e+00, -1.5367e+00,\n           6.9777e-01, -7.2349e-02,  7.1688e-01,  3.7195e-01,  7.4783e-01,\n           1.6969e-02,  9.8290e-01, -3.8921e-01,  1.8742e+00, -7.2517e-01,\n           4.8942e-01, -5.1985e-01,  8.5410e-01,  8.2816e-01,  1.2706e+00],\n         [-1.0277e+00, -6.0964e-01, -2.0466e-01, -1.1568e+00,  1.0100e+00,\n          -2.1865e-01,  1.0996e+00, -1.9621e-01,  6.4514e-01,  2.0727e-01,\n           2.7683e-01, -2.6392e-02,  1.0104e+00,  2.8703e-01,  1.1893e+00,\n           5.5934e-01,  1.1949e+00,  7.2102e-02,  7.2388e-01,  7.9073e-01,\n           1.2379e+00,  7.2017e-01,  6.4423e-01,  9.5871e-01,  1.1741e+00,\n           7.1135e-01, -3.3062e-01,  5.1678e-01,  7.0726e-02,  1.0743e+00,\n          -2.4028e-01,  1.7778e+00,  2.5352e-01,  8.5907e-01,  3.7533e-01,\n           1.3776e+00, -5.6489e-02,  1.3030e+00,  8.7354e-01,  1.2518e+00,\n          -1.5354e-02,  9.6564e-01,  1.0185e+00,  6.3584e-01,  5.9366e-01,\n           1.2346e+00,  5.8494e-01,  1.2270e+00, -4.7151e-01,  2.0535e-01,\n          -1.6369e-01,  4.9733e-01,  2.2074e-01,  1.9423e+00, -4.2067e-01,\n          -2.6796e+00,  6.7199e-02,  7.3223e-01,  2.2188e+00,  1.2408e+00,\n          -1.7501e-02,  1.6954e+00, -6.3148e-01,  9.9268e-01,  9.0677e-01,\n           1.2001e+00,  4.7409e-01,  1.6193e+00, -9.8978e-02,  1.0869e+00,\n          -4.5596e-01,  1.1826e+00, -1.5322e-01,  1.0208e+00,  1.9812e-01,\n           1.0634e+00, -3.1308e-01,  5.1822e-01, -1.3818e+00,  1.1367e+00,\n           9.7111e-01,  1.0500e+00, -2.7170e-01,  9.6431e-01, -1.0560e+00,\n           7.5533e-01,  9.0511e-01,  8.7558e-01,  8.1984e-02,  1.6599e-01,\n           5.7301e-01,  1.0889e+00, -4.2448e-01,  9.8175e-01, -7.9300e-02,\n           7.1419e-01, -1.0312e-02,  5.0770e-01,  6.3735e-01,  1.2364e+00]],\n\n        [[-3.0457e-01,  7.6355e-01,  1.7576e-01,  2.7146e-01, -2.8343e-01,\n           7.4360e-01,  2.6587e-01,  1.0253e+00, -7.4775e-02,  6.2340e-01,\n          -5.7774e-02,  1.1216e+00,  3.4384e-01,  1.4193e+00, -2.3236e-01,\n           6.8453e-01,  6.0939e-01,  1.2512e+00, -6.8667e-01,  1.7087e+00,\n           1.2162e+00,  8.1760e-01, -4.8442e-01,  6.6555e-01,  3.0343e-01,\n           2.0860e+00,  4.9992e-01,  7.9802e-01,  2.7959e-01,  1.6835e+00,\n          -3.3566e-01,  8.7595e-01,  5.9656e-02,  1.3362e+00,  3.7501e-01,\n           1.5655e+00,  4.4867e-01,  1.1128e+00, -1.6196e-01,  5.6540e-02,\n          -6.7961e-01,  1.1858e+00,  6.0653e-02,  1.4378e+00,  1.3834e-01,\n           5.1793e-01, -5.6141e-01,  7.4578e-01, -5.2445e-01,  1.0970e+00,\n          -4.8925e-01,  1.1908e+00,  2.1481e-01,  2.4969e+00, -8.6665e-01,\n          -2.2846e+00,  5.6854e-01,  1.4197e+00,  1.2294e+00,  1.7852e+00,\n          -2.9369e-01,  1.6380e+00, -1.5926e+00,  7.9563e-01,  1.5306e+00,\n           1.1355e+00,  5.0722e-01,  1.1874e+00,  4.8552e-01,  7.1005e-01,\n           1.9573e-01,  1.0047e+00,  9.2879e-02,  5.7556e-01,  6.4987e-01,\n           1.5284e+00,  7.7908e-02,  1.8263e+00, -1.2208e+00,  6.5045e-01,\n           4.9855e-01,  3.5845e-01, -7.2308e-01,  1.2657e+00, -1.3643e+00,\n           5.3636e-01, -5.2048e-01, -5.2500e-02,  2.2895e-01,  6.5440e-01,\n          -6.5800e-01,  8.3265e-01,  3.5158e-01,  1.7434e+00,  2.6074e-01,\n           1.0611e+00, -3.9079e-01,  1.5443e-01, -3.5432e-02,  1.1704e+00],\n         [ 5.1426e-01,  6.3675e-01,  1.0816e+00,  2.3030e-01,  9.4330e-01,\n           7.2806e-01,  4.7297e-01,  5.2859e-01,  2.3499e-01,  7.0663e-01,\n           9.6794e-02,  3.0638e-01,  6.2262e-01,  9.7617e-01,  6.8699e-01,\n           4.4742e-01,  9.1337e-01,  9.5357e-01,  4.5351e-03,  1.2980e+00,\n           7.5255e-01,  7.7277e-01,  4.2400e-01,  1.4239e+00,  4.6409e-01,\n           6.9740e-01, -1.7979e-01,  5.0631e-01,  2.8394e-02,  1.2423e+00,\n          -9.4776e-02,  1.5954e+00, -3.6418e-01,  1.0563e+00,  1.6694e-01,\n           1.6223e+00, -5.2140e-02,  1.3070e+00, -1.2723e-01,  7.1573e-01,\n          -5.5546e-01,  9.7686e-01,  2.8778e-01,  7.7413e-01,  4.9286e-01,\n           1.1112e+00,  3.8708e-01,  8.5436e-01, -1.5573e-01,  2.0616e-01,\n          -2.9593e-01,  8.9324e-01,  4.5031e-01,  2.5698e+00, -7.2370e-01,\n          -1.7314e+00, -1.8791e-01,  6.7015e-01,  1.2929e+00,  1.6212e+00,\n          -2.5102e-01,  1.8416e+00, -2.3327e-01,  1.4259e+00,  8.6864e-01,\n           6.4096e-01,  7.8391e-01,  1.2040e+00,  8.3089e-01,  1.0016e+00,\n          -2.3850e-01,  2.7265e-01, -5.2353e-02,  7.7736e-01,  3.1144e-01,\n           7.8757e-01, -1.4244e-01,  1.3170e+00, -8.0402e-01,  5.0689e-01,\n           8.8086e-01,  7.5853e-01, -3.9168e-01,  1.1600e+00, -1.5850e+00,\n           7.4176e-01,  5.3197e-02,  8.8017e-01, -1.8572e-02,  2.2644e-01,\n           4.9536e-02,  7.4668e-01, -3.0709e-01,  1.5129e+00, -5.6785e-01,\n           7.8761e-01, -3.9727e-01,  6.1835e-01,  4.4006e-01,  1.2468e+00],\n         [ 1.2864e+00, -7.6086e-01,  1.1298e+00, -1.0431e-01,  7.8827e-01,\n           6.0068e-01,  1.3192e+00,  8.3831e-01,  7.6056e-01,  3.7655e-01,\n           1.1474e+00,  6.8315e-01,  8.3200e-01,  7.8599e-01,  9.1765e-01,\n           4.2405e-01,  4.2482e-01,  3.3028e-01, -7.3646e-02,  7.4297e-01,\n           5.7902e-01,  7.9306e-01,  4.7718e-01,  1.7626e+00,  9.1377e-01,\n           1.1801e+00, -3.1768e-01,  5.2822e-01,  5.3323e-01,  1.1945e+00,\n          -9.0203e-02,  1.0930e+00, -3.9671e-01,  8.8392e-01, -3.4736e-01,\n           7.2834e-01, -1.2979e-01,  1.0012e+00, -4.3072e-01,  8.2176e-01,\n          -8.3949e-01,  7.1974e-01,  9.0564e-01,  9.8177e-01,  3.4685e-01,\n           1.4094e+00,  2.6089e-01,  3.9146e-01,  4.7167e-01,  1.0392e-01,\n          -1.8492e-02,  7.4208e-01,  4.1131e-01,  2.6185e+00, -5.3498e-01,\n          -2.0292e+00, -7.6694e-01,  6.7530e-01,  1.7754e+00,  1.9730e+00,\n          -3.8546e-01,  1.5481e+00,  1.9787e-02,  1.3785e+00,  2.5089e-01,\n           1.0311e+00,  2.4086e-01,  1.2890e+00,  3.0858e-02,  1.2898e+00,\n          -7.4206e-01,  1.0115e+00, -3.9192e-01,  4.2294e-01, -6.3385e-01,\n           1.3102e+00, -3.8135e-01,  9.2234e-01, -1.3524e+00,  1.0180e+00,\n           8.5772e-01,  1.0383e+00, -3.9332e-01,  1.4433e+00, -1.0793e+00,\n           5.6841e-01,  1.4464e-01,  1.1185e+00, -5.6399e-01,  5.2034e-01,\n           2.2910e-01,  7.5631e-01, -4.2781e-01,  2.0366e+00, -8.3036e-01,\n           1.1246e+00,  2.0659e-01,  1.5423e+00,  1.1449e-01,  3.3073e-01],\n         [ 8.0151e-01, -3.5111e-01,  1.4649e+00, -5.2377e-01, -1.6899e-02,\n           1.5817e-01,  7.1489e-01,  1.3543e-01,  9.3340e-01, -1.1041e-01,\n           1.5522e+00,  8.6690e-01,  1.0649e+00, -4.4920e-02,  1.6421e+00,\n          -1.8585e-02,  8.2799e-01,  5.4129e-01,  2.9115e-01,  1.5146e+00,\n           1.1617e+00,  9.4247e-01,  5.5720e-01,  8.6364e-01,  2.9902e-01,\n           8.7224e-01,  4.1618e-01,  7.3053e-01,  5.4354e-02,  9.3074e-01,\n          -2.2251e-01,  1.2435e+00,  5.7759e-01,  8.6454e-01, -6.9717e-01,\n           1.4126e+00,  1.6783e-01,  1.5017e+00,  3.0559e-01,  1.3573e+00,\n          -8.2746e-01,  3.0004e-01,  1.8659e-01,  7.6559e-01, -4.4109e-01,\n           6.0423e-01,  1.9711e-01, -6.7640e-02, -6.4050e-02,  2.1954e-01,\n           6.1789e-01,  8.4815e-01,  2.0745e-01,  2.5859e+00, -9.2713e-01,\n          -1.5913e+00,  2.8908e-01,  4.2928e-01,  1.6001e+00,  1.7202e+00,\n           6.5377e-01,  2.5990e+00, -2.1215e-01,  7.9417e-01,  5.6763e-01,\n           8.8688e-01,  1.0986e+00,  1.7784e+00,  6.0060e-01,  1.3672e+00,\n          -1.6295e-03,  8.6152e-01, -4.8789e-01,  7.7069e-01, -8.7794e-02,\n           1.1416e+00,  5.1965e-01,  1.0946e+00, -1.4850e+00,  1.3587e+00,\n           7.6947e-01,  9.8115e-01, -8.9746e-02,  1.0002e+00, -1.0542e+00,\n           7.8781e-01,  3.8943e-01,  9.9024e-01, -4.3205e-01,  4.9491e-01,\n           2.8964e-01,  1.5023e+00, -3.4567e-01,  3.8186e-01, -6.6203e-01,\n           9.5664e-01, -4.4482e-01,  4.4099e-01,  1.0756e+00,  1.5770e+00],\n         [-1.6826e-01, -8.5614e-01,  5.5039e-01, -1.6662e+00,  1.6884e-01,\n          -1.1110e+00,  3.5276e-01, -3.2587e-01,  3.3590e-01,  3.0116e-01,\n           7.3282e-01,  3.4323e-01,  5.6604e-01,  1.3039e-01,  3.0479e-01,\n           7.3589e-01,  1.5959e+00,  2.5569e-01,  9.9133e-01,  8.0226e-01,\n           1.0365e+00,  3.4663e-01,  1.2961e+00,  1.3678e+00,  7.5271e-01,\n           1.1856e+00, -1.3653e-01,  5.4937e-01,  2.5951e-01,  7.0602e-01,\n           5.0912e-02,  2.1152e+00,  3.4066e-01,  1.8950e+00, -1.9367e-01,\n           1.8790e+00,  6.8732e-01,  1.6069e+00, -5.0155e-01,  6.8139e-01,\n          -4.0902e-01,  1.2283e+00,  1.1661e+00,  9.5187e-01, -5.8214e-02,\n           1.2739e+00,  2.5168e-02,  7.2436e-01,  8.2571e-01,  4.9023e-01,\n           7.8296e-02,  6.6241e-01,  4.5670e-01,  2.2265e+00, -5.1059e-01,\n          -2.2415e+00,  4.4928e-01,  1.0249e+00,  1.4139e+00,  1.6507e+00,\n           4.9174e-02,  1.3713e+00,  4.1764e-01,  1.3555e+00,  9.9367e-01,\n           3.8270e-01,  5.4817e-01,  1.7621e+00,  3.1451e-01,  1.3306e+00,\n           3.1590e-01,  8.4837e-01, -1.0786e-01,  1.8718e-01,  6.1889e-01,\n           5.5658e-01, -1.8798e-01,  9.1044e-01, -1.5897e+00,  1.3740e+00,\n           8.6109e-01,  1.5461e+00, -3.1718e-01,  1.5260e+00, -1.4785e+00,\n           2.0688e-02, -2.9245e-01,  8.5276e-01,  2.5924e-01,  8.1830e-01,\n           1.0159e+00,  1.7765e+00,  1.2682e-01,  1.5478e+00, -1.0309e+00,\n           1.0646e+00, -3.7465e-01,  5.5250e-02,  6.1850e-01,  1.3959e+00]]])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([[[ 8.5703e-02, -2.2201e-01,  1.6569e-01,  1.3373e-01,  3.8239e-01,\n           3.5401e-01,  1.2870e-02,  2.2461e-01, -4.3817e-01,  5.0164e-01,\n          -3.5874e-01, -3.4983e-01,  5.5156e-02,  6.9648e-01, -1.7958e-01,\n           6.7926e-02,  3.9101e-01,  1.6039e-01, -2.6635e-01, -2.1138e-01,\n           5.3698e-01,  4.9379e-01,  9.3660e-01,  6.6902e-01,  2.1793e-01,\n          -4.6642e-01,  2.2383e-01, -3.6204e-01, -1.7656e-01,  1.7480e-01,\n          -2.0367e-01,  1.3931e-01,  1.9832e-02, -1.0413e-01, -2.0244e-01,\n           5.5003e-01, -1.5460e-01,  9.8655e-01, -2.6863e-01, -2.9090e-01,\n          -3.2866e-01, -3.4188e-01, -1.6943e-01, -4.2001e-01, -4.6727e-02,\n          -1.6327e-01,  7.0824e-01, -7.4911e-01, -9.1559e-02, -9.6178e-01,\n          -1.9747e-01,  1.0282e-01,  5.5221e-01,  1.3816e+00, -6.5636e-01,\n          -3.2502e+00, -3.1556e-01, -1.2055e+00,  1.7709e+00,  4.0260e-01,\n          -7.9827e-01,  1.1597e+00, -3.3042e-01,  3.1382e-01,  7.7386e-01,\n           2.2595e-01,  5.2471e-01, -3.4053e-02,  3.2048e-01,  7.9948e-02,\n           1.7752e-01, -4.9426e-01, -7.0045e-01, -4.4569e-01,  1.7244e-01,\n           2.0278e-01,  2.3292e-02, -2.0677e-01, -1.0158e+00,  1.8325e-01,\n           5.6752e-01,  3.1821e-01, -6.5011e-01,  6.8277e-01, -8.6585e-01,\n          -5.9392e-02, -2.9264e-01, -5.5668e-01, -3.4705e-01, -3.2895e-01,\n           4.0215e-01, -1.2746e-01, -2.0228e-01,  8.7368e-01, -5.4500e-01,\n           7.9205e-01, -2.0695e-01, -7.4273e-02,  7.5808e-01, -3.4243e-01],\n         [ 5.8854e-01, -2.0250e-01,  7.3479e-01, -6.8338e-01, -1.9675e-01,\n          -1.8020e-01, -3.9177e-01,  3.4172e-01, -6.0561e-01,  6.3816e-01,\n          -2.6695e-01,  3.6486e-01, -4.0379e-01, -1.1340e-01, -5.8718e-01,\n           2.8380e-01,  8.0250e-01, -3.5303e-01,  3.0083e-01,  7.8935e-02,\n           4.4416e-01, -4.5906e-01,  7.9294e-01,  5.0365e-01,  3.2805e-01,\n           2.8027e-01, -4.9330e-01, -3.8482e-01, -3.9284e-02, -2.4830e-01,\n          -1.9880e-01,  1.1469e+00,  1.3228e-01,  9.1691e-01, -3.6739e-01,\n           8.9425e-01,  5.4260e-01,  6.1738e-01, -6.2205e-01, -3.1132e-01,\n          -5.0933e-01,  2.3335e-01,  1.0826e+00, -4.4637e-02, -1.2767e-01,\n           2.7628e-01, -3.2617e-02, -2.7397e-01,  7.7764e-01, -5.0861e-01,\n           3.8307e-02, -3.3679e-01,  4.2344e-01,  1.2271e+00, -5.3826e-01,\n          -3.2411e+00,  4.2626e-01,  2.5189e-02,  1.3948e+00,  6.5085e-01,\n           3.3250e-02,  3.7141e-01,  4.0440e-01,  3.5558e-01,  9.8265e-01,\n          -6.1724e-01,  5.3901e-01,  7.6219e-01,  3.0689e-01,  3.3065e-01,\n           3.0956e-01, -1.5161e-01, -1.1313e-01, -8.1281e-01,  6.1450e-01,\n          -4.4341e-01, -1.9163e-01, -8.9551e-02, -1.5927e+00,  3.7405e-01,\n           8.5857e-01,  5.4613e-01, -3.1928e-01,  5.2598e-01, -1.4802e+00,\n          -9.7931e-01, -2.9390e-01, -1.4724e-01,  2.5803e-01, -1.8170e-01,\n           1.0149e+00,  7.7649e-01,  1.2598e-01,  5.4779e-01, -1.0316e+00,\n           6.4599e-02, -3.7523e-01, -9.4475e-01,  6.1802e-01,  3.9591e-01],\n         [ 3.3390e-01, -5.2136e-01,  2.6848e-01,  1.7416e-01,  1.5808e-01,\n           9.5567e-01, -3.9404e-01,  7.5332e-01, -1.2433e-01,  6.4539e-01,\n          -1.2848e-01,  6.1024e-01,  1.4794e-01,  5.6136e-01, -1.1478e-01,\n          -2.3334e-01,  2.4459e-01, -3.0026e-02, -1.0939e-01,  4.1958e-01,\n          -1.7990e-01, -4.6465e-01,  4.6621e-01,  8.8600e-01,  8.3695e-01,\n           7.6019e-01, -6.6037e-01, -1.6917e+00,  7.5998e-01, -4.4161e-02,\n           7.8069e-02,  6.8950e-01,  6.5059e-01,  2.5840e-01, -6.3663e-01,\n           3.3152e-01, -2.2198e-01,  1.2196e-01, -1.1606e-01,  2.3557e-01,\n          -5.1854e-01, -1.3450e-01,  4.6457e-01, -3.6864e-01, -1.0744e-01,\n           1.4728e-01, -3.6792e-01, -1.3907e-01,  7.7072e-01, -4.7685e-01,\n          -3.1725e-02,  6.0066e-02,  9.0913e-01,  1.6532e+00,  4.4310e-02,\n          -3.1636e+00,  3.3880e-01, -4.3464e-01,  1.1445e+00,  2.6247e-01,\n           4.5218e-01,  1.5068e+00,  2.1069e-01,  4.8721e-01,  4.9486e-01,\n          -7.0552e-02,  1.0531e+00,  5.1535e-01,  1.9231e-01,  2.0466e-01,\n          -4.1545e-01,  6.3548e-01,  7.7074e-02, -4.2075e-01,  3.6767e-01,\n           9.3886e-01, -5.9122e-01, -7.0948e-01, -8.7497e-01, -3.9823e-01,\n           2.4080e-01,  6.9948e-01,  3.9934e-01,  2.9582e-01, -1.8030e+00,\n          -1.2961e+00,  7.2491e-02,  2.7994e-01, -7.9536e-01, -6.2887e-01,\n           7.8952e-02,  1.3955e-02,  1.1702e+00,  4.5136e-01, -4.1720e-01,\n          -3.4870e-01, -8.0300e-01, -3.1398e-01,  3.1908e-01,  6.2061e-01],\n         [-3.8194e-02, -2.4487e-01,  7.2812e-01, -3.9961e-01,  8.3172e-02,\n           4.3953e-02, -3.9141e-01,  3.3440e-01, -5.7545e-01,  8.7459e-02,\n           2.8787e-01, -6.7310e-02,  3.0906e-01, -2.6384e-01, -1.3231e-01,\n          -2.0757e-01,  3.3395e-01, -3.3848e-01, -3.1743e-01, -4.8336e-01,\n           1.4640e-01, -3.7304e-01,  3.4577e-01,  5.2041e-02,  4.4946e-01,\n          -4.6971e-01,  2.6280e-02, -5.4155e-01, -1.5518e-01, -1.4107e-01,\n          -3.9722e-02,  2.8277e-01,  1.4393e-01,  2.3464e-01, -3.1021e-01,\n           8.6173e-02,  2.0397e-01,  5.2624e-01,  1.7164e-01, -8.2378e-02,\n          -7.1787e-01, -4.1531e-01,  2.0335e-01, -1.2763e-01,  4.1367e-01,\n           5.5187e-01,  5.7908e-01, -3.3477e-01, -3.6559e-01, -5.4857e-01,\n          -6.2892e-02,  2.6584e-01,  3.0205e-01,  9.9775e-01, -8.0481e-01,\n          -3.0243e+00,  1.2540e-02, -3.6942e-01,  2.2167e+00,  7.2201e-01,\n          -2.4978e-01,  9.2136e-01,  3.4514e-02,  4.6745e-01,  1.1079e+00,\n          -1.9358e-01, -7.4575e-02,  2.3353e-01, -5.2062e-02, -2.2044e-01,\n           5.7162e-02, -1.5806e-01, -3.0798e-01, -4.1625e-01,  3.7972e-01,\n           1.5006e-01, -5.3212e-01, -2.0550e-01, -1.2526e+00,  7.1624e-02,\n           7.0565e-01,  4.9744e-01, -4.2063e-01,  2.6148e-01, -1.5380e+00,\n          -3.0223e-01, -7.3438e-02, -2.8312e-01,  3.7104e-01, -2.5217e-01,\n           1.6215e-02, -1.7099e-02, -3.8984e-01,  8.7424e-01, -7.2569e-01,\n          -5.1058e-01, -5.2028e-01, -1.4590e-01,  8.2780e-01,  2.7062e-01],\n         [-2.7086e-01,  4.4006e-02, -2.0260e-02, -1.7395e-01,  6.4440e-01,\n           7.1213e-01,  3.5510e-01,  4.7138e-01, -2.9637e-01,  5.4427e-01,\n          -7.2294e-01, -4.7612e-03,  4.0611e-02,  4.3236e-02,  2.9729e-01,\n           1.0725e-01,  4.0156e-01, -5.3662e-01,  3.3382e-02,  6.7396e-02,\n           6.4556e-01, -8.5523e-02,  1.4103e-01,  9.4539e-02,  7.4947e-01,\n          -1.9400e-01, -6.8739e-01, -4.1741e-01, -2.2807e-01,  1.2000e-01,\n          -4.8999e-01,  8.0945e-01,  4.5138e-02, -1.1898e-01,  2.0161e-01,\n           3.9276e-01, -2.0121e-01,  3.1354e-01,  7.5304e-01,  2.5907e-01,\n          -1.1566e-01, -2.9319e-02,  9.3499e-01, -3.6067e-01,  5.2420e-01,\n           2.3706e-01,  5.2715e-01,  2.2869e-01, -5.1958e-01, -7.9349e-01,\n          -2.0368e-01, -5.0187e-01,  1.8748e-01,  9.4282e-01, -4.4834e-01,\n          -3.6792e+00,  4.4183e-02, -2.6751e-01,  2.1997e+00,  2.4100e-01,\n          -3.3425e-02,  6.9553e-01, -6.4472e-01, -7.2277e-03,  8.9575e-01,\n           2.0015e-01,  4.6493e-01,  6.1933e-01, -1.0660e-01,  8.6910e-02,\n          -4.6230e-01,  1.8262e-01, -1.5849e-01,  2.0791e-02,  1.9373e-01,\n           6.3426e-02, -3.1673e-01, -4.8177e-01, -1.3848e+00,  1.3669e-01,\n           9.6859e-01,  4.9965e-02, -2.7380e-01, -3.5686e-02, -1.0577e+00,\n          -2.4467e-01,  9.0366e-01, -1.2442e-01,  8.0776e-02, -8.3401e-01,\n           5.7201e-01,  8.8945e-02, -4.2532e-01, -1.8253e-02, -7.9995e-02,\n          -2.8581e-01, -1.0890e-02, -4.9230e-01,  6.3687e-01,  2.3642e-01]],\n\n        [[-3.0457e-01, -2.3645e-01,  1.7576e-01, -7.2854e-01, -2.8343e-01,\n          -2.5640e-01,  2.6587e-01,  2.5309e-02, -7.4775e-02, -3.7660e-01,\n          -5.7774e-02,  1.2159e-01,  3.4384e-01,  4.1928e-01, -2.3236e-01,\n          -3.1547e-01,  6.0939e-01,  2.5117e-01, -6.8667e-01,  7.0873e-01,\n           1.2162e+00, -1.8240e-01, -4.8442e-01, -3.3445e-01,  3.0343e-01,\n           1.0860e+00,  4.9992e-01, -2.0198e-01,  2.7959e-01,  6.8352e-01,\n          -3.3566e-01, -1.2405e-01,  5.9656e-02,  3.3617e-01,  3.7501e-01,\n           5.6552e-01,  4.4867e-01,  1.1284e-01, -1.6196e-01, -9.4346e-01,\n          -6.7961e-01,  1.8581e-01,  6.0653e-02,  4.3776e-01,  1.3834e-01,\n          -4.8207e-01, -5.6141e-01, -2.5422e-01, -5.2445e-01,  9.7003e-02,\n          -4.8925e-01,  1.9077e-01,  2.1481e-01,  1.4969e+00, -8.6665e-01,\n          -3.2846e+00,  5.6854e-01,  4.1971e-01,  1.2294e+00,  7.8522e-01,\n          -2.9369e-01,  6.3803e-01, -1.5926e+00, -2.0437e-01,  1.5306e+00,\n           1.3548e-01,  5.0722e-01,  1.8742e-01,  4.8552e-01, -2.8995e-01,\n           1.9573e-01,  4.6515e-03,  9.2879e-02, -4.2444e-01,  6.4987e-01,\n           5.2839e-01,  7.7908e-02,  8.2630e-01, -1.2208e+00, -3.4955e-01,\n           4.9855e-01, -6.4155e-01, -7.2308e-01,  2.6566e-01, -1.3643e+00,\n          -4.6364e-01, -5.2048e-01, -1.0525e+00,  2.2895e-01, -3.4560e-01,\n          -6.5800e-01, -1.6735e-01,  3.5158e-01,  7.4337e-01,  2.6074e-01,\n           6.1104e-02, -3.9079e-01, -8.4557e-01, -3.5432e-02,  1.7036e-01],\n         [-3.2721e-01,  9.6446e-02,  3.4244e-01, -4.4327e-01,  3.0535e-01,\n          -4.2016e-02, -7.1235e-02, -3.1036e-01, -2.2557e-01, -1.8100e-01,\n          -2.9088e-01, -6.1542e-01,  2.9751e-01,  3.0491e-02,  4.1504e-01,\n          -5.1489e-01,  6.8628e-01, -2.0302e-02, -1.8486e-01,  3.1605e-01,\n           5.9472e-01, -2.1470e-01,  2.9256e-01,  4.3262e-01,  3.5466e-01,\n          -2.9659e-01, -2.7086e-01, -4.8953e-01, -4.7391e-02,  2.4521e-01,\n          -1.5783e-01,  5.9742e-01, -4.1664e-01,  5.7632e-02,  1.2330e-01,\n           6.2326e-01, -8.8440e-02,  3.0770e-01, -1.5742e-01, -2.8381e-01,\n          -5.8058e-01, -2.2824e-02,  2.6689e-01, -2.2565e-01,  4.7548e-01,\n           1.1134e-01,  3.7263e-01, -1.4554e-01, -1.6775e-01, -7.9377e-01,\n          -3.0593e-01, -1.0671e-01,  4.4199e-01,  1.5698e+00, -7.3062e-01,\n          -2.7314e+00, -1.9366e-01, -3.2983e-01,  1.2881e+00,  6.2126e-01,\n          -2.5500e-01,  8.4160e-01, -2.3658e-01,  4.2594e-01,  8.6589e-01,\n          -3.5904e-01,  7.8162e-01,  2.0396e-01,  8.2898e-01,  1.6123e-03,\n          -2.4008e-01, -7.2735e-01, -5.3671e-02, -2.2264e-01,  3.1034e-01,\n          -2.1243e-01, -1.4335e-01,  3.1700e-01, -8.0478e-01, -4.9311e-01,\n           8.8023e-01, -2.4147e-01, -3.9220e-01,  1.5997e-01, -1.5854e+00,\n          -2.5824e-01,  5.2834e-02, -1.1983e-01, -1.8874e-02, -7.7356e-01,\n           4.9285e-02, -2.5332e-01, -3.0730e-01,  5.1295e-01, -5.6802e-01,\n          -2.1239e-01, -3.9741e-01, -3.8165e-01,  4.3994e-01,  2.4683e-01],\n         [ 3.7711e-01, -3.4471e-01,  1.3405e-01, -1.1710e-02, -1.9427e-01,\n           4.1464e-01,  4.0608e-01,  4.3063e-01, -5.7060e-02, -1.9921e-01,\n           4.3267e-01, -1.6269e-02,  2.1710e-01, -2.6149e-03,  3.9424e-01,\n          -4.2803e-01, -1.7495e-02, -5.6658e-01, -4.4558e-01, -1.8529e-01,\n           2.6732e-01, -1.5712e-01,  2.1657e-01,  7.9714e-01,  6.9623e-01,\n           2.0405e-01, -4.9907e-01, -4.5519e-01,  3.8210e-01,  2.0603e-01,\n          -2.1606e-01,  1.0093e-01, -5.0148e-01, -1.1058e-01, -4.3455e-01,\n          -2.6785e-01, -2.0234e-01,  3.8320e-03, -4.9108e-01, -1.7642e-01,\n          -8.8971e-01, -2.7900e-01,  8.6387e-01, -1.7356e-02,  3.1210e-01,\n           4.1004e-01,  2.3199e-01, -6.0812e-01,  4.4763e-01, -8.9579e-01,\n          -3.8491e-02, -2.5772e-01,  3.9468e-01,  1.6186e+00, -5.4882e-01,\n          -3.0291e+00, -7.7845e-01, -3.2463e-01,  1.7658e+00,  9.7303e-01,\n          -3.9342e-01,  5.4811e-01,  1.3164e-02,  3.7850e-01,  2.4538e-01,\n           3.1079e-02,  2.3628e-01,  2.8901e-01,  2.7047e-02,  2.8985e-01,\n          -7.4523e-01,  1.1517e-02, -3.9456e-01, -5.7706e-01, -6.3604e-01,\n           3.1022e-01, -3.8317e-01, -7.7663e-02, -1.3539e+00,  1.8009e-02,\n           8.5646e-01,  3.8259e-02, -3.9437e-01,  4.4331e-01, -1.0802e+00,\n          -4.3159e-01,  1.4391e-01,  1.1854e-01, -5.6459e-01, -4.7966e-01,\n           2.2860e-01, -2.4369e-01, -4.2823e-01,  1.0366e+00, -8.3071e-01,\n           1.2460e-01,  2.0630e-01,  5.4232e-01,  1.1425e-01, -6.6927e-01],\n         [ 6.6039e-01,  6.3888e-01,  8.6264e-01,  2.7455e-01, -8.9222e-01,\n           6.4171e-01, -2.7304e-01,  2.9033e-01, -5.7512e-02, -2.4491e-01,\n           6.2227e-01,  4.9926e-01,  2.2700e-01, -5.9077e-01,  9.0674e-01,\n          -6.9621e-01,  1.9357e-01, -2.3170e-01, -2.4986e-01,  6.7362e-01,\n           7.0398e-01,  5.3391e-02,  1.7195e-01, -5.9168e-02, -2.4024e-02,\n          -7.4141e-02,  1.4598e-01, -2.3227e-01, -1.7126e-01, -4.3476e-02,\n          -4.1067e-01,  2.6134e-01,  4.2080e-01, -1.2309e-01, -8.2775e-01,\n           4.2115e-01,  5.9120e-02,  5.0765e-01,  2.1512e-01,  3.6136e-01,\n          -9.0275e-01, -6.9712e-01,  1.2395e-01, -2.3245e-01, -4.9320e-01,\n          -3.9441e-01,  1.5376e-01, -1.0667e+00, -1.0011e-01, -7.7981e-01,\n           5.8789e-01, -1.5140e-01,  1.8250e-01,  1.5862e+00, -9.4788e-01,\n          -2.5911e+00,  2.7182e-01, -5.7057e-01,  1.5857e+00,  7.2034e-01,\n           6.4183e-01,  1.5991e+00, -2.2208e-01, -2.0578e-01,  5.5937e-01,\n          -1.1309e-01,  1.0917e+00,  7.7839e-01,  5.9488e-01,  3.6718e-01,\n          -6.3842e-03, -1.3847e-01, -4.9184e-01, -2.2930e-01, -9.1083e-02,\n           1.4165e-01,  5.1691e-01,  9.4609e-02, -1.4873e+00,  3.5870e-01,\n           7.6758e-01, -1.8850e-02, -9.1320e-02,  2.4154e-04, -1.0555e+00,\n          -2.1219e-01,  3.8834e-01, -9.7613e-03, -4.3296e-01, -5.0509e-01,\n           2.8889e-01,  5.0233e-01, -3.4630e-01, -6.1814e-01, -6.6255e-01,\n          -4.3362e-02, -4.4525e-01, -5.5901e-01,  1.0752e+00,  5.7700e-01],\n         [ 5.8854e-01, -2.0250e-01,  7.3479e-01, -6.8338e-01, -1.9675e-01,\n          -1.8020e-01, -3.9177e-01,  3.4172e-01, -6.0561e-01,  6.3816e-01,\n          -2.6695e-01,  3.6486e-01, -4.0379e-01, -1.1340e-01, -5.8718e-01,\n           2.8380e-01,  8.0250e-01, -3.5303e-01,  3.0083e-01,  7.8935e-02,\n           4.4416e-01, -4.5906e-01,  7.9294e-01,  5.0365e-01,  3.2805e-01,\n           2.8027e-01, -4.9330e-01, -3.8482e-01, -3.9284e-02, -2.4830e-01,\n          -1.9880e-01,  1.1469e+00,  1.3228e-01,  9.1691e-01, -3.6739e-01,\n           8.9425e-01,  5.4260e-01,  6.1738e-01, -6.2205e-01, -3.1132e-01,\n          -5.0933e-01,  2.3335e-01,  1.0826e+00, -4.4637e-02, -1.2767e-01,\n           2.7628e-01, -3.2617e-02, -2.7397e-01,  7.7764e-01, -5.0861e-01,\n           3.8307e-02, -3.3679e-01,  4.2344e-01,  1.2271e+00, -5.3826e-01,\n          -3.2411e+00,  4.2626e-01,  2.5189e-02,  1.3948e+00,  6.5085e-01,\n           3.3250e-02,  3.7141e-01,  4.0440e-01,  3.5558e-01,  9.8265e-01,\n          -6.1724e-01,  5.3901e-01,  7.6219e-01,  3.0689e-01,  3.3065e-01,\n           3.0956e-01, -1.5161e-01, -1.1313e-01, -8.1281e-01,  6.1450e-01,\n          -4.4341e-01, -1.9163e-01, -8.9551e-02, -1.5927e+00,  3.7405e-01,\n           8.5857e-01,  5.4613e-01, -3.1928e-01,  5.2598e-01, -1.4802e+00,\n          -9.7931e-01, -2.9390e-01, -1.4724e-01,  2.5803e-01, -1.8170e-01,\n           1.0149e+00,  7.7649e-01,  1.2598e-01,  5.4779e-01, -1.0316e+00,\n           6.4599e-02, -3.7523e-01, -9.4475e-01,  6.1802e-01,  3.9591e-01]]])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_dim = 100\n",
    "embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze = True, sparse=True)\n",
    "embedding.eval()\n",
    "pos_encoding = PositionalEncoding(encoding_dim)\n",
    "pos_encoding.eval()\n",
    "\n",
    "X = embedding(torch.tensor([[7, 10, 72, 1, 8], [9, 20, 50, 60, 10]]))\n",
    "X_pos = pos_encoding(X)\n",
    "display(X_pos)\n",
    "display(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1000, 100])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding.P.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 60, 32])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n           0.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  ...,  1.0000e+00,\n           1.7783e-04,  1.0000e+00],\n         [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  ...,  1.0000e+00,\n           3.5566e-04,  1.0000e+00],\n         ...,\n         [ 4.3616e-01,  8.9987e-01,  5.9521e-01,  ...,  9.9984e-01,\n           1.0136e-02,  9.9995e-01],\n         [ 9.9287e-01,  1.1918e-01,  9.3199e-01,  ...,  9.9983e-01,\n           1.0314e-02,  9.9995e-01],\n         [ 6.3674e-01, -7.7108e-01,  9.8174e-01,  ...,  9.9983e-01,\n           1.0492e-02,  9.9994e-01]]])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_dim, num_steps = 32, 60\n",
    "pos_encoding = PositionalEncoding(encoding_dim)\n",
    "pos_encoding.eval()\n",
    "X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))\n",
    "P = pos_encoding.P[:, :X.shape[1], :]\n",
    "display(P.shape)\n",
    "display(P)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[        nan,  4.0096e+37,  6.2351e+37, -4.2478e+36, -2.0465e+37,\n",
      "          -4.1833e+36,         nan,         nan, -7.8783e+36,  1.0117e+37,\n",
      "          -9.4423e+35,  2.0174e+36, -5.8751e+36, -3.1546e+36, -3.0629e+36,\n",
      "          -1.6364e+37, -1.8854e+36, -5.5364e+35,  3.2821e+36, -3.7999e+36,\n",
      "          -8.9872e+35, -2.5219e+36,  3.8568e+36, -5.8303e+36,  1.0008e+37,\n",
      "           4.1014e+35, -5.0816e+36, -1.4643e+35, -4.0584e+37, -1.0357e+38,\n",
      "                  nan, -3.9588e+36,  1.1894e+36,  9.2627e+36,         nan,\n",
      "           3.2849e+36, -2.2424e+36,  6.2773e+36,  6.5724e+36, -2.8173e+36,\n",
      "          -1.7648e+37, -4.5027e+37,  9.5633e+35, -3.9691e+37,  2.6361e+36,\n",
      "           6.8147e+36,  2.2129e+37,  4.7346e+36,  3.1635e+36,  1.1845e+36,\n",
      "          -9.1324e+36,         nan,  3.2725e+37, -2.1265e+37, -2.5488e+37,\n",
      "                  nan, -1.6300e+37, -9.5616e+36,         nan, -1.2026e+37,\n",
      "          -3.9547e+36, -6.9687e+36,         nan, -1.0574e+37],\n",
      "         [        nan,  4.5371e+37,  4.7310e+37,  5.4921e+35, -1.1456e+37,\n",
      "          -5.6767e+36,         nan,         nan, -1.7323e+37,  1.1040e+37,\n",
      "          -7.2874e+35,  2.3083e+35, -3.2761e+36, -1.7506e+36, -2.2463e+36,\n",
      "          -1.1745e+37, -2.2174e+36, -1.2140e+36,  3.5099e+36, -4.3533e+36,\n",
      "          -1.3467e+36, -2.7611e+36,  6.2095e+36, -5.3817e+36,  8.0789e+36,\n",
      "           1.7980e+36, -1.5847e+37, -7.4729e+35, -2.5404e+37, -7.9355e+37,\n",
      "                  nan, -8.5253e+35,  1.1945e+37,  6.3805e+36,         nan,\n",
      "          -4.4083e+35, -2.8747e+36,  6.0402e+36, -2.0758e+37, -2.3559e+36,\n",
      "          -1.8084e+37, -5.2724e+37,  1.5476e+36, -2.5422e+37,  1.9936e+36,\n",
      "           4.9472e+36,  2.3681e+37,  3.7612e+36,  2.3868e+36,  1.3099e+36,\n",
      "          -1.7271e+37,         nan,  3.9787e+37, -2.3462e+37, -3.1127e+37,\n",
      "                  nan, -7.4143e+36, -5.7982e+36,         nan, -9.0302e+36,\n",
      "          -2.3564e+36, -2.4159e+37,         nan, -1.0047e+37],\n",
      "         [        nan,  4.9118e+37,  2.8799e+37, -2.3690e+36, -1.3319e+37,\n",
      "          -6.3740e+36,         nan,         nan, -1.2797e+37,  1.8175e+37,\n",
      "           6.8685e+35,  1.9020e+36, -4.2108e+36, -4.3006e+35, -2.8865e+36,\n",
      "          -1.1824e+37, -1.8071e+36, -5.8824e+35,  7.4211e+36, -2.5073e+36,\n",
      "          -2.7051e+35, -3.2702e+36,  3.5792e+36, -5.2558e+36,  9.0414e+36,\n",
      "           3.1522e+36, -1.2399e+37, -1.6784e+35, -3.4883e+37, -4.8289e+37,\n",
      "                  nan, -3.7508e+35, -4.6989e+36,  5.9225e+36,         nan,\n",
      "           4.4778e+36, -3.5246e+36,  5.1440e+36, -3.7284e+37, -1.4928e+36,\n",
      "          -1.9490e+37, -2.9684e+37,  2.5439e+36, -3.7528e+37,  2.1151e+36,\n",
      "           4.8459e+36,  3.1743e+37,  5.6058e+36,  2.5647e+36,  1.6227e+36,\n",
      "          -2.3841e+37,         nan,  7.2269e+36, -2.1338e+37, -4.1046e+37,\n",
      "                  nan, -7.5939e+36, -7.1677e+36,         nan, -9.5367e+36,\n",
      "          -4.7869e+35, -2.0132e+37,         nan, -1.3228e+37],\n",
      "         [        nan,  4.2701e+37,  3.1278e+37, -3.4594e+36, -1.3173e+37,\n",
      "          -4.4869e+36,         nan,         nan, -2.0866e+37,  1.4397e+37,\n",
      "           9.2448e+35,  1.7016e+36, -3.9572e+36, -2.2918e+36, -3.2081e+36,\n",
      "          -1.2101e+37, -3.4974e+36, -1.4807e+35,  6.6339e+36, -3.5434e+36,\n",
      "          -5.6288e+35, -2.7698e+36,  3.6293e+36, -5.5168e+36,  9.3535e+36,\n",
      "           9.5465e+35, -2.0705e+37, -1.5194e+36, -2.7286e+37, -2.0721e+37,\n",
      "                  nan, -1.8357e+36,  3.2049e+37,  7.1670e+36,         nan,\n",
      "           4.6845e+36, -4.2058e+36,  6.2472e+36,  2.4124e+36, -2.6578e+36,\n",
      "          -2.2141e+37, -2.8636e+37, -4.1369e+35, -3.0506e+37,  1.9976e+36,\n",
      "           5.8713e+36,  3.0222e+37,  5.1576e+36,  3.6138e+36,  1.6406e+36,\n",
      "          -2.7575e+37,         nan, -4.5744e+36, -2.1539e+37, -2.7313e+37,\n",
      "                  nan, -8.7414e+36, -7.4574e+36,         nan, -8.2005e+36,\n",
      "           3.8115e+35, -2.6973e+37,         nan, -1.3051e+37],\n",
      "         [        nan,  2.8682e+37,  6.1103e+37, -2.6967e+36, -1.5549e+37,\n",
      "          -5.8104e+36,         nan,         nan, -1.0133e+37,  6.5978e+36,\n",
      "          -8.2512e+35,  1.2320e+36, -4.6589e+36, -3.4753e+36, -2.1134e+36,\n",
      "          -1.4487e+37, -3.2187e+36,  7.7062e+35,  3.4364e+36, -4.9016e+36,\n",
      "          -8.6521e+35, -4.1841e+36,  2.6152e+36, -7.9143e+36,  8.8688e+36,\n",
      "           2.0476e+36, -4.5180e+36,  1.5368e+35, -2.7410e+37, -9.4083e+37,\n",
      "                  nan, -2.1257e+36,  9.6171e+36,  8.8870e+36,         nan,\n",
      "           3.1058e+36, -5.2376e+36,  4.9070e+36,  3.7013e+37, -3.1266e+36,\n",
      "          -2.1147e+37, -3.5390e+37, -2.3136e+35, -3.0475e+37,  1.3876e+36,\n",
      "           5.2787e+36,  2.8096e+37,  6.7754e+36,  3.1925e+36,  1.3650e+36,\n",
      "          -2.2867e+37,         nan, -2.0622e+36, -2.0660e+37, -3.1112e+37,\n",
      "                  nan, -1.1481e+37, -8.1132e+36,         nan, -1.1277e+37,\n",
      "          -4.6646e+35, -9.7187e+36,         nan, -8.9855e+36]],\n",
      "\n",
      "        [[        nan, -6.7382e+37, -4.2625e+37, -2.0470e+36, -1.2120e+37,\n",
      "           6.9545e+36,         nan,         nan,  4.5927e+36, -4.6779e+36,\n",
      "           3.6045e+36,  3.8402e+36, -3.9906e+35,  4.0838e+36, -4.7419e+35,\n",
      "          -5.7820e+36, -3.2959e+36,  2.5794e+36,  1.2670e+37,  2.2927e+36,\n",
      "          -6.5024e+36,  3.9236e+36, -4.9213e+36,  4.4147e+36, -2.3720e+36,\n",
      "           8.0673e+34,  3.2192e+36,  8.3080e+36, -2.1957e+36,  1.1026e+38,\n",
      "                  nan, -4.4608e+36,  2.4272e+37,  1.6779e+36,         nan,\n",
      "           1.2393e+37, -3.2946e+36, -1.3854e+36,  1.0151e+38, -2.3405e+36,\n",
      "          -8.0749e+34,  6.3272e+37, -5.7680e+35, -2.0210e+37,  1.2589e+36,\n",
      "           3.9819e+36,  1.5783e+37, -1.7765e+37,  2.7154e+36,  5.9493e+34,\n",
      "          -7.5366e+37,         nan,  5.1530e+37, -1.2407e+37,  9.6505e+37,\n",
      "                  nan, -1.2620e+37, -4.3970e+36,         nan, -6.5372e+36,\n",
      "          -1.5062e+36,  5.9815e+37,         nan, -1.8907e+36],\n",
      "         [        nan, -4.7847e+37, -7.2829e+37, -4.0247e+36, -9.4325e+36,\n",
      "           3.2184e+36,         nan,         nan,  2.0933e+36, -4.4742e+36,\n",
      "           3.3286e+36,  3.7517e+36, -7.4822e+35,  3.7271e+36,  5.6983e+34,\n",
      "          -5.2446e+36, -5.6281e+35,  3.2002e+36,  1.6434e+37,  4.0984e+36,\n",
      "          -6.8866e+36,  9.5101e+35, -8.3455e+36,  3.3271e+36, -3.5104e+36,\n",
      "          -9.1216e+35, -1.4527e+36,  1.0200e+37, -9.2892e+36,  1.0743e+38,\n",
      "                  nan, -4.9266e+36,  2.4732e+37,  2.8699e+35,         nan,\n",
      "           1.8105e+37, -3.9720e+36, -4.2993e+36,  1.2451e+38, -9.3465e+35,\n",
      "           5.9127e+35,  9.2176e+37,  1.0130e+36, -1.8885e+37,  8.3359e+35,\n",
      "           2.3037e+36,  5.7416e+37, -2.1029e+37,  2.3048e+36, -8.0261e+35,\n",
      "          -6.6816e+37,         nan,  3.6136e+37, -6.5235e+36,  8.6220e+37,\n",
      "                  nan, -1.1122e+37, -4.8830e+36,         nan, -8.2444e+36,\n",
      "          -1.5866e+36,  5.5652e+37,         nan, -4.3248e+35],\n",
      "         [        nan, -7.7807e+37, -8.4701e+37, -1.5271e+35, -1.0586e+37,\n",
      "           4.2972e+36,         nan,         nan,  8.2462e+36, -7.3330e+36,\n",
      "           3.9321e+36,  3.5485e+36, -1.1267e+36,  4.2229e+36, -5.0422e+34,\n",
      "          -5.1021e+36, -1.5769e+36,  2.3742e+36,  2.1336e+37,  3.6011e+36,\n",
      "          -7.0191e+36,  5.1292e+36, -6.4630e+36,  4.9931e+36, -1.5905e+36,\n",
      "          -2.1721e+35,  5.5742e+36,  8.1162e+36, -1.1983e+37,  1.0641e+38,\n",
      "                  nan, -5.1742e+36,  2.3310e+37,  2.7715e+36,         nan,\n",
      "           1.9239e+37, -2.8520e+36,  4.8822e+35,  1.4534e+38, -2.0154e+36,\n",
      "          -4.1000e+35,  8.3717e+37,  2.9008e+36, -1.7059e+37,  3.5598e+36,\n",
      "           5.8412e+36,  5.2233e+35, -1.8547e+37,  3.5854e+36,  5.3653e+35,\n",
      "          -8.1522e+37,         nan,  6.3159e+37, -1.2448e+37,  8.7578e+37,\n",
      "                  nan, -1.1996e+37, -2.9783e+36,         nan, -5.9547e+36,\n",
      "          -3.2199e+35,  6.0936e+37,         nan,  1.4890e+35],\n",
      "         [        nan, -4.0866e+37, -8.1404e+35, -3.8771e+36, -1.2921e+37,\n",
      "           9.6882e+36,         nan,         nan,  7.4035e+35, -1.1025e+36,\n",
      "           9.3987e+35,  3.4053e+36, -1.0225e+36,  2.9446e+36,  4.6211e+35,\n",
      "          -6.9564e+36,  8.7896e+35,  1.3443e+36,  2.3434e+36,  2.0795e+36,\n",
      "          -7.1560e+36,  6.1440e+35, -6.0527e+36,  1.5701e+36, -5.3739e+36,\n",
      "           7.3904e+35,  8.8220e+35,  7.2547e+36,  3.1726e+36,  7.9941e+37,\n",
      "                  nan, -1.9231e+36,  2.5558e+37,  1.0059e+36,         nan,\n",
      "           6.1404e+36, -3.1408e+36, -2.3816e+36,  6.9959e+37, -1.8776e+36,\n",
      "          -3.5930e+35,  4.1430e+37,  4.6489e+36, -1.5787e+37, -1.7887e+36,\n",
      "           4.5677e+36,  5.2622e+37, -1.7067e+37,  1.2779e+36,  1.3825e+35,\n",
      "          -6.5667e+37,         nan,  4.8443e+37, -4.4556e+36,  7.9856e+37,\n",
      "                  nan, -1.3251e+37, -3.3514e+36,         nan, -8.7271e+36,\n",
      "          -1.4011e+36,  6.8786e+37,         nan,  3.4693e+36],\n",
      "         [        nan, -5.0943e+37, -2.2526e+37, -3.2216e+36, -1.1693e+37,\n",
      "           6.9668e+36,         nan,         nan,  1.3614e+37, -4.3622e+36,\n",
      "           3.3956e+36,  3.7700e+36, -5.9977e+35,  4.0461e+36,  3.0312e+35,\n",
      "          -6.0069e+36,  1.1386e+36,  2.6835e+36,  1.1526e+37,  3.6401e+36,\n",
      "          -6.6442e+36,  1.1696e+36, -6.7622e+36,  3.2996e+36, -3.9768e+36,\n",
      "          -1.0856e+36,  1.2781e+37,  8.2962e+36, -1.5192e+37,  1.4187e+38,\n",
      "                  nan, -5.2713e+36,  1.8595e+37,  1.2509e+36,         nan,\n",
      "           1.1996e+37, -2.0380e+36, -9.9744e+35,  1.1079e+38, -1.6160e+36,\n",
      "           1.6075e+36,  6.6058e+37,  1.6729e+36, -2.0665e+37,  2.1890e+36,\n",
      "           3.2580e+36,  4.0523e+37, -1.9129e+37,  3.1845e+36, -2.2442e+35,\n",
      "          -6.7577e+37,         nan,  4.7854e+37, -7.3919e+36,  9.2492e+37,\n",
      "                  nan, -1.2633e+37, -3.5836e+36,         nan, -7.0847e+36,\n",
      "          -2.8390e+36,  6.1240e+37,         nan,  9.7956e+35]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)\n",
    "Y = torch.randn(batch_size, seq_length, d_k)\n",
    "\n",
    "attention_is_all_you_need = AttentionIsAllYouNeed(model_dim, d_k, model_dim, h, 1, 1)\n",
    "output = attention_is_all_you_need(X, Y)\n",
    "print(output.shape)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}