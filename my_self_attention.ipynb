{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch.matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention1.set_mask(mask)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[-4.3076e-01,  6.0484e-01,  5.4121e-02, -5.1875e-01,  1.1079e+00,\n",
      "           1.4728e+00,  1.5404e+00, -2.3107e+00, -1.7760e+00, -3.9833e-01,\n",
      "           1.6302e+00,  1.7585e+00, -5.2821e-01, -1.5424e+00,  5.9872e-01,\n",
      "           1.3088e+00, -1.8706e-01, -1.9072e-01, -7.2235e-01, -3.9332e-01,\n",
      "          -6.6101e-01,  8.5264e-01,  1.2726e+00, -6.0114e-01, -2.6549e-01,\n",
      "           1.1766e+00,  3.0314e-01, -1.4057e+00, -1.1424e+00,  8.1405e-01,\n",
      "           6.5785e-01,  2.3066e-01,  3.4055e-01, -1.7748e-01,  3.4725e-01,\n",
      "          -1.2393e+00, -1.9766e+00,  5.8639e-01, -5.5633e-01, -6.5792e-01,\n",
      "          -3.2908e-01,  3.4763e-02,  7.1937e-01,  4.5273e-02,  1.9821e-01,\n",
      "          -1.1837e-01,  6.7208e-01,  7.0503e-01, -2.4000e+00, -7.1432e-02,\n",
      "           1.1389e+00,  8.2356e-01, -2.8059e-02,  1.8813e+00, -7.5849e-01,\n",
      "           7.8319e-01, -3.1497e-01, -8.8514e-01,  1.5571e+00, -5.3806e-01,\n",
      "          -4.2848e-01,  6.4868e-01, -5.9181e-01, -1.7197e+00],\n",
      "         [ 1.6190e+00,  4.1014e-01, -2.8741e-01,  2.2244e+00,  8.7977e-02,\n",
      "           2.3022e-01,  1.1361e+00, -7.0249e-01, -2.3417e-01,  1.0444e+00,\n",
      "          -8.7244e-01, -4.0606e-01,  1.6672e+00,  1.5357e-01,  3.7801e-01,\n",
      "          -1.0315e-01,  8.2897e-01,  6.9658e-01, -1.0817e+00, -2.2604e+00,\n",
      "           3.9238e-01,  8.6629e-01, -1.4856e+00,  7.9962e-01,  8.2867e-01,\n",
      "           3.5259e-01,  6.0858e-01,  1.3645e+00,  8.8756e-01,  1.2111e-01,\n",
      "          -1.3799e+00,  9.2287e-01,  2.2982e+00, -3.8689e-01, -1.0782e+00,\n",
      "          -9.5784e-01,  6.2155e-01, -1.1333e+00, -6.9688e-01, -3.4111e-01,\n",
      "           1.3976e-01, -3.1496e-01, -1.0686e+00, -1.2182e+00,  5.3041e-01,\n",
      "          -1.3508e+00, -2.0230e+00, -1.0034e+00, -1.0912e+00, -4.7019e-01,\n",
      "           6.1549e-01, -3.1130e-01, -1.0123e+00,  1.1791e+00, -1.3136e-01,\n",
      "          -7.4803e-01,  5.9378e-01,  1.2187e+00, -1.3276e+00,  7.6189e-01,\n",
      "          -4.8342e-01,  1.3722e+00, -7.1976e-01, -2.7037e-01],\n",
      "         [-2.0678e+00, -2.3036e+00,  6.1419e-02, -4.6132e-01,  9.7182e-01,\n",
      "          -7.2167e-01,  9.8101e-01,  1.5921e+00,  3.2862e-01, -1.3289e+00,\n",
      "          -7.2029e-01, -1.5245e+00, -1.4337e+00, -6.5801e-01,  4.2963e-01,\n",
      "           2.3815e-01, -3.7678e-01, -6.5637e-01,  5.2774e-01,  1.9270e+00,\n",
      "          -2.0061e+00,  6.3311e-01, -3.9784e-01,  1.3680e+00,  1.6615e-01,\n",
      "           7.4330e-01, -1.2709e+00,  9.3194e-01,  2.0593e-01,  9.7478e-01,\n",
      "          -1.0093e+00,  3.9403e-01,  2.5851e+00, -3.4453e-02,  1.1153e-01,\n",
      "          -1.5324e-01, -1.4079e-01, -1.2048e+00,  9.4971e-01, -3.0437e-01,\n",
      "          -2.4108e-01, -8.9926e-01,  7.4480e-01,  5.1941e-01, -7.1455e-01,\n",
      "           8.9225e-01,  1.5254e+00, -9.4544e-01,  1.1325e+00,  1.4581e-01,\n",
      "          -1.0847e-01, -8.9421e-01, -4.1455e-01,  1.7900e-01,  1.0904e+00,\n",
      "           1.4006e+00, -1.3622e+00, -3.9353e-01,  9.9419e-01, -4.6576e-03,\n",
      "           8.5952e-01,  3.7669e-01, -7.4232e-01, -4.8676e-01],\n",
      "         [-5.8601e-02, -6.6615e-01, -1.0267e+00,  1.6153e+00, -1.8504e-01,\n",
      "           3.7274e-02, -6.2560e-02, -8.3356e-01, -2.3986e+00, -4.6076e-01,\n",
      "           1.5056e+00,  2.1542e+00,  7.4144e-02, -1.1256e+00, -9.0561e-01,\n",
      "           5.5290e-01,  7.7238e-02,  6.8130e-02, -9.9245e-01, -1.9708e+00,\n",
      "           8.3603e-01, -1.5456e-01,  1.1599e+00,  6.2575e-01, -3.3653e-01,\n",
      "           1.2247e-01, -6.9383e-01, -9.2919e-01, -1.4385e+00,  2.1010e+00,\n",
      "           3.0330e-01, -1.1027e+00,  1.7495e+00, -8.2139e-01, -8.6674e-02,\n",
      "          -8.6105e-01,  1.0189e+00,  3.4113e-01,  2.3383e-01,  1.5443e+00,\n",
      "          -1.3987e+00, -2.7999e-01,  3.3696e-01, -1.0135e+00,  2.1606e+00,\n",
      "           5.4669e-01,  1.2080e+00, -1.7093e-02,  1.8504e-01, -1.2445e+00,\n",
      "           5.4508e-01,  1.7136e-01, -7.8537e-01,  6.5283e-01,  4.3869e-01,\n",
      "          -2.0779e+00,  3.7334e-02,  3.4563e-01, -2.4252e-01, -2.9405e-01,\n",
      "           9.8440e-01,  7.4860e-01,  4.0405e-01, -4.2174e-01],\n",
      "         [-1.1632e+00,  9.8078e-01,  9.9792e-01, -1.4823e+00, -4.3960e-01,\n",
      "           2.8913e-01,  6.5607e-02, -1.5229e+00, -3.7602e-01, -2.2927e-01,\n",
      "           5.9785e-01,  1.0751e+00,  1.3795e+00, -1.4145e+00, -2.4485e-01,\n",
      "          -1.4369e+00,  1.7453e+00,  6.3438e-01,  4.3753e-02,  9.9188e-01,\n",
      "           2.2645e+00, -1.4650e-01,  6.9082e-01,  4.2675e-01, -7.1012e-01,\n",
      "           6.0684e-02,  4.5850e-01, -1.0972e+00, -5.7356e-02,  1.4510e+00,\n",
      "          -6.2833e-01, -2.4500e-01,  1.8475e-01, -1.4221e+00,  4.6640e-01,\n",
      "           6.3261e-01, -4.2210e-01, -2.4753e-01, -7.5412e-01, -3.3609e-01,\n",
      "           1.1434e+00, -5.1153e-01,  4.1403e-01,  1.4139e+00,  2.0022e+00,\n",
      "          -1.7982e+00, -5.9578e-01, -6.0272e-01, -2.0706e+00, -3.1056e-01,\n",
      "          -2.1944e-01, -1.3003e-01,  2.7116e+00,  9.2131e-01,  2.1866e-01,\n",
      "          -1.6535e+00, -2.4903e-01, -5.6592e-01, -6.4819e-01,  1.5058e-01,\n",
      "          -7.4372e-01, -7.6495e-01, -2.9974e-02,  8.5716e-01]],\n",
      "\n",
      "        [[ 1.8413e+00, -1.6494e+00,  4.8909e-01,  5.6322e-01,  1.0174e+00,\n",
      "           1.0641e-01,  4.1242e-01,  7.8465e-02, -2.2323e+00, -9.6314e-01,\n",
      "           1.2397e-01,  1.5800e-01, -4.7526e-01, -4.3630e-01, -5.0006e-01,\n",
      "          -1.4158e+00, -1.0136e+00, -4.5296e-01, -3.8520e-01,  1.4248e+00,\n",
      "          -6.8752e-01,  9.1354e-01,  4.1920e-01,  2.4667e-01,  7.3165e-01,\n",
      "          -1.7984e-01,  5.5094e-01, -1.1268e+00, -1.4562e+00,  1.8645e+00,\n",
      "           4.3732e-01,  2.1952e-01,  1.0347e+00,  4.6492e-01, -2.4022e-01,\n",
      "           1.2845e+00, -1.5353e-02, -8.6020e-01, -8.3622e-02, -4.0282e-01,\n",
      "           1.0113e+00, -1.7301e+00,  3.5156e-01, -7.5322e-02,  1.7452e-01,\n",
      "           1.2172e+00, -2.0441e-01, -1.1140e+00,  7.4113e-01,  1.5383e+00,\n",
      "          -5.2729e-01, -2.1293e+00, -4.9154e-01,  8.0696e-01,  9.7568e-01,\n",
      "          -4.1801e-01, -2.0116e+00, -1.9457e+00,  4.6570e-01,  1.6255e+00,\n",
      "           1.4759e+00, -1.8467e-01, -3.3259e-01,  9.7476e-01],\n",
      "         [-1.2619e+00,  4.3266e-01,  1.2031e+00,  6.8519e-01, -5.6073e-01,\n",
      "           7.5376e-01,  1.1113e+00, -3.5746e-01, -6.6920e-01,  1.2450e+00,\n",
      "           7.3280e-01,  2.3758e-01, -8.1368e-01, -1.0578e+00,  5.0456e-01,\n",
      "          -5.6945e-01, -6.6215e-01,  1.9446e+00,  4.4868e-01,  1.5364e-01,\n",
      "           3.3966e-01,  6.6510e-01,  1.0879e+00,  1.2954e-01,  9.8809e-02,\n",
      "          -1.3835e+00,  1.5130e+00,  2.5592e-01,  7.2702e-01, -1.0684e+00,\n",
      "           1.2750e+00, -1.1914e+00, -2.5403e-01,  3.7913e-01, -7.7645e-01,\n",
      "          -1.0038e-01,  8.7188e-02, -3.1017e-01,  9.5917e-01,  3.0682e-01,\n",
      "          -2.0515e-01,  2.0944e+00,  1.0293e+00, -1.8695e+00, -1.2366e+00,\n",
      "          -9.7650e-01, -2.0300e+00, -9.6133e-02,  5.6717e-01, -8.9976e-02,\n",
      "           3.3588e-01,  4.8341e-01,  1.1166e+00,  4.3598e-01,  2.0283e+00,\n",
      "          -2.4863e+00, -1.5258e-01, -3.6032e-01, -2.0594e+00, -7.5905e-01,\n",
      "           1.5929e-01, -1.6458e+00, -5.1147e-01, -1.2004e-02],\n",
      "         [ 2.9406e-03,  1.9013e-01, -4.6619e-01, -1.8575e+00,  2.3828e-02,\n",
      "          -2.5428e-01,  1.5886e+00, -3.6094e-01,  3.2908e-01, -1.7511e-01,\n",
      "          -9.1688e-01,  9.7589e-01,  5.9088e-01, -1.2384e+00,  1.7034e+00,\n",
      "           1.1964e+00, -7.6699e-02, -5.1425e-01, -1.4158e-01,  1.1262e+00,\n",
      "           2.3267e-01,  8.2898e-01,  2.1426e+00, -1.1326e+00,  3.3301e-02,\n",
      "          -1.8875e+00,  2.9550e-01, -5.9403e-01, -5.8192e-01, -1.5603e-01,\n",
      "           2.3123e-01, -2.0995e-02,  4.6574e-01,  5.3416e-03, -4.5614e-01,\n",
      "           1.1826e+00,  6.4981e-01, -2.0834e+00,  3.3818e-01,  1.1925e+00,\n",
      "           6.0767e-01,  8.4736e-01, -1.7220e-01, -8.2040e-01, -7.4184e-01,\n",
      "           1.4235e-01,  1.4051e+00,  2.6685e-01,  2.2075e+00, -8.5040e-01,\n",
      "          -4.0782e-01,  3.5879e-01, -2.3297e-01,  1.0601e+00, -1.4347e+00,\n",
      "          -8.5692e-01, -2.8903e+00, -6.7480e-01, -1.1491e+00,  1.0474e+00,\n",
      "           1.1902e+00,  3.7635e-01, -1.1848e+00, -5.0489e-01],\n",
      "         [ 1.0229e+00, -1.6335e-01,  6.6112e-01, -8.1879e-01,  3.4877e-01,\n",
      "           1.3503e+00,  6.0492e-02, -1.1478e-01,  5.5420e-01, -1.2719e+00,\n",
      "           1.2743e+00, -1.2525e-01, -6.2547e-03, -1.3604e+00, -9.1278e-01,\n",
      "           9.6569e-01, -6.2302e-01,  3.2762e-01, -7.2811e-01,  7.8813e-01,\n",
      "           1.7477e-01,  6.9360e-02,  9.6253e-02,  1.8638e-01,  1.7001e+00,\n",
      "           1.5435e+00,  9.5076e-01, -1.8316e+00, -1.3595e+00,  1.8009e-01,\n",
      "          -1.1935e+00,  1.7408e+00,  2.6372e+00,  5.5977e-01, -1.1422e+00,\n",
      "           1.5695e-01, -2.1301e+00, -1.0766e+00, -1.6704e+00, -1.1565e+00,\n",
      "          -1.9061e+00, -1.2323e+00, -4.3395e-04,  7.9986e-02, -7.2745e-01,\n",
      "           6.0000e-01, -1.1506e-01, -9.5284e-01, -3.0855e-01, -1.2404e-01,\n",
      "          -3.6521e-01,  8.4383e-01,  7.6250e-01,  3.6522e-01, -1.1138e+00,\n",
      "           9.2302e-01, -6.1335e-01,  2.9120e-01, -1.3626e-01,  5.4579e-01,\n",
      "           1.0481e+00, -3.3615e-01,  1.3997e+00,  1.4078e+00],\n",
      "         [-1.2530e+00, -3.5441e-01, -1.2149e+00, -1.3924e+00,  1.1856e+00,\n",
      "          -9.5247e-01, -1.8709e-01, -1.0434e+00, -1.9310e-01,  1.1247e+00,\n",
      "          -2.3371e-02, -2.5110e-01,  5.7534e-01,  1.6753e+00,  1.1756e+00,\n",
      "          -1.4249e+00,  4.9956e-01, -1.5091e+00,  2.8150e-01, -2.0312e+00,\n",
      "          -2.3784e+00, -2.9521e-01,  1.8715e-03, -1.3317e+00,  1.0516e+00,\n",
      "          -1.1230e+00,  1.5108e+00, -6.3368e-01,  2.4147e+00,  1.8988e+00,\n",
      "           8.0913e-02,  7.2537e-01,  1.1353e+00,  2.9911e-01,  3.7217e-01,\n",
      "          -1.2251e+00, -1.7304e+00,  1.2629e+00,  2.5659e-01, -4.0324e-01,\n",
      "           1.9171e-01, -2.3617e-01, -4.8738e-01, -1.7720e-01,  1.8236e-01,\n",
      "           6.4460e-01, -2.1598e-01, -5.8098e-01, -5.1868e-01,  9.2177e-01,\n",
      "           1.8800e-01, -8.8438e-01,  2.0535e+00,  8.7288e-02,  4.9677e-01,\n",
      "           7.0095e-01, -2.2685e-01,  5.1597e-01,  5.5009e-01,  2.0173e-01,\n",
      "          -1.0413e+00,  1.6181e-01,  7.3594e-01,  1.6015e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[ 4.4167e-01,  1.3096e+00,  1.1925e+00,  3.5202e-01,  9.8557e-01,\n",
      "          -4.5629e-01,  4.8104e-02, -3.4329e+00, -2.8309e+00, -1.7597e+00,\n",
      "           1.4486e+00,  1.6226e+00, -5.8385e-01, -7.4539e-01,  9.2908e-01,\n",
      "           1.1153e+00, -4.0416e-01,  8.0218e-02, -5.0402e-01, -2.9100e-01,\n",
      "          -1.8066e-01,  9.4763e-01,  1.1753e+00,  9.9155e-02, -4.3011e-01,\n",
      "           1.2382e+00,  1.9882e-01, -1.2579e+00, -7.1012e-01,  6.8816e-01,\n",
      "           5.6130e-01, -5.1725e-01,  5.9561e-01,  4.6011e-01,  2.9936e-01,\n",
      "          -1.0416e+00, -1.6320e+00,  7.6652e-01,  5.2690e-01, -7.2720e-01,\n",
      "          -5.9806e-01,  6.8260e-01,  5.0967e-01,  2.7464e-01,  5.3049e-01,\n",
      "           4.7798e-02,  2.2059e-01,  7.2548e-01, -2.1976e+00, -2.0431e-01,\n",
      "           9.1432e-01,  3.4441e-02,  1.3584e-01,  1.4703e+00, -9.4595e-01,\n",
      "           3.7799e-01, -2.7673e-01, -8.6023e-01,  1.0490e+00,  5.0996e-02,\n",
      "          -5.7086e-02,  2.8351e-01, -2.6733e-01, -1.4776e+00],\n",
      "         [ 2.3598e+00,  1.0325e+00,  5.8112e-01,  2.8516e+00,  4.2781e-01,\n",
      "          -1.0305e+00, -5.6673e-01, -2.5123e+00, -1.2407e+00, -5.0582e-01,\n",
      "          -4.0555e-01, -2.5853e-01,  1.1945e+00,  6.2181e-02,  8.5149e-01,\n",
      "          -2.1478e-01,  6.5877e-02,  5.3742e-01, -5.8543e-01, -2.0108e+00,\n",
      "           9.8597e-02,  1.7960e-01, -1.0003e+00,  8.4642e-01,  4.1700e-01,\n",
      "           4.5746e-01,  6.0672e-01,  1.3342e+00,  8.0020e-01,  4.5530e-01,\n",
      "          -1.7776e+00,  1.6452e-01,  2.5367e+00,  2.0421e-01, -5.6772e-01,\n",
      "           1.5836e-03,  1.3715e+00, -7.6302e-01, -2.3308e-01, -5.8860e-01,\n",
      "          -3.2779e-01,  4.9781e-02, -8.9559e-01, -8.5248e-01,  1.8766e-01,\n",
      "          -8.7537e-01, -1.4343e+00, -5.2967e-01, -6.7793e-01, -5.4157e-01,\n",
      "           8.3424e-01, -7.6699e-01, -7.1847e-01,  1.5870e+00,  2.0527e-01,\n",
      "          -7.5179e-01,  1.2508e-01,  1.0023e+00, -1.1462e+00,  6.4716e-01,\n",
      "           5.7322e-02,  6.3800e-01, -5.6737e-01, -4.2519e-01],\n",
      "         [-1.7432e+00, -1.6856e+00,  1.1587e+00, -3.0189e-01,  4.1326e-01,\n",
      "          -2.3584e+00, -2.5891e-02,  5.3843e-02, -7.9438e-01, -2.4736e+00,\n",
      "          -6.2987e-01, -1.3113e+00, -1.3450e+00,  3.1029e-02,  5.5063e-01,\n",
      "           1.8990e-01, -2.9077e-01, -5.2732e-01,  9.9498e-01,  1.9475e+00,\n",
      "          -1.4176e+00,  8.0044e-01, -3.0692e-01,  1.0766e+00, -1.3605e-01,\n",
      "           1.0110e+00, -1.1320e+00,  9.1087e-01,  2.8361e-01,  6.5021e-01,\n",
      "          -9.8143e-01,  2.8306e-01,  2.6485e+00,  4.5006e-01,  1.5671e-01,\n",
      "           5.8490e-01,  4.8657e-01, -1.0976e+00,  2.0391e+00, -6.1458e-01,\n",
      "           2.9101e-02, -3.9785e-02,  1.0047e+00,  3.3370e-01, -9.6673e-01,\n",
      "           7.6840e-01,  8.2380e-01, -8.8915e-01,  1.0631e+00,  4.6502e-01,\n",
      "           2.5035e-01, -1.1013e+00, -1.3872e-01, -9.5267e-02,  1.1899e+00,\n",
      "           7.1118e-01, -1.1586e+00, -2.8395e-01,  6.1732e-01, -1.7321e-01,\n",
      "           1.1553e+00,  8.9481e-03, -8.2395e-01, -2.9851e-01],\n",
      "         [ 7.8578e-01, -1.7170e-01,  9.2441e-02,  2.4228e+00,  5.1683e-02,\n",
      "          -1.4800e+00, -8.8443e-01, -2.5487e+00, -2.8615e+00, -1.7588e+00,\n",
      "           1.3163e+00,  1.6400e+00,  1.1761e-01, -2.0743e-01, -1.2007e-01,\n",
      "           4.1063e-01, -6.0216e-01,  1.0483e-01, -6.1948e-01, -1.6258e+00,\n",
      "           7.8130e-01, -8.4091e-02,  1.2305e+00,  5.1935e-01, -5.9286e-01,\n",
      "           5.3807e-01, -6.4148e-01, -7.6553e-01, -1.1313e+00,  1.5813e+00,\n",
      "           6.1782e-02, -1.5110e+00,  1.1016e+00, -3.3599e-01,  1.1876e-01,\n",
      "          -1.7660e-01,  1.1508e+00,  6.9973e-01,  7.2974e-01,  1.3137e+00,\n",
      "          -8.5375e-01,  7.3457e-01,  2.0173e-01, -1.3206e-01,  1.7051e+00,\n",
      "           4.6761e-01,  8.3140e-01, -8.3497e-02, -1.1938e-01, -1.1055e+00,\n",
      "           5.8458e-01, -4.5424e-01, -2.6747e-01,  7.4332e-01,  1.9736e-01,\n",
      "          -1.6691e+00, -4.3126e-03, -1.6896e-02, -4.2758e-01, -2.6929e-01,\n",
      "           1.3551e+00,  3.8507e-01,  2.0972e-01, -6.6238e-01],\n",
      "         [-7.2257e-01,  1.1354e+00,  1.3376e+00, -9.6850e-01, -5.1129e-01,\n",
      "          -1.5147e+00, -1.2553e+00, -2.8051e+00, -1.1786e+00, -1.3283e+00,\n",
      "           7.7985e-01,  1.1059e+00,  1.0166e+00, -1.2102e+00,  7.1517e-01,\n",
      "          -1.2061e+00,  1.2519e+00,  6.4476e-01,  3.3290e-01,  1.2366e+00,\n",
      "           2.2565e+00, -2.2217e-01,  2.8339e-01,  4.2997e-01, -9.1978e-01,\n",
      "           1.6466e-01,  4.0291e-01, -8.8281e-01, -2.9780e-01,  1.1464e+00,\n",
      "          -8.0190e-01, -1.0519e-01,  8.6207e-01, -5.7393e-01,  1.4971e-01,\n",
      "           1.1297e+00, -6.1502e-03, -2.8096e-01,  1.6821e-02, -6.2564e-01,\n",
      "           1.1857e+00,  6.1495e-01,  3.8592e-01,  1.4975e+00,  1.7014e+00,\n",
      "          -1.4522e+00, -8.2624e-02, -2.9707e-01, -9.0242e-01, -7.2610e-01,\n",
      "          -1.0753e-01, -3.8556e-01,  2.3766e+00,  9.4210e-01,  1.2151e-01,\n",
      "          -1.5336e+00, -1.2228e-01, -9.2437e-01, -6.4414e-01,  1.9837e-01,\n",
      "           6.2065e-03, -9.3940e-01, -3.8264e-01,  4.8767e-01]],\n",
      "\n",
      "        [[ 4.0975e-02, -1.9489e+00, -7.5547e-01, -1.0941e+00, -4.6020e-01,\n",
      "           1.0320e+00,  1.8573e+00,  1.8207e+00, -5.2385e-01,  4.5473e-01,\n",
      "           1.3109e-01,  4.4735e-01, -9.6392e-01, -2.5300e-01, -4.0221e-01,\n",
      "          -1.3244e+00, -1.7609e+00, -4.7958e-01,  2.3760e-01,  8.0027e-01,\n",
      "          -2.4938e-01,  7.6245e-01,  4.1431e-01,  4.7698e-01,  3.6417e-01,\n",
      "          -8.2950e-03,  2.0946e-01, -4.5858e-01, -1.7020e+00,  1.3372e+00,\n",
      "          -1.6034e-01, -1.7961e-01,  6.2275e-01,  2.0567e+00, -7.3339e-02,\n",
      "           1.9914e+00,  5.7746e-01, -2.7214e-01,  4.0611e-01, -3.9395e-01,\n",
      "           7.1626e-01, -1.3306e+00,  1.9115e-01,  1.6609e-01,  9.6637e-02,\n",
      "           1.0607e+00, -3.2289e-01, -1.1599e+00,  6.6512e-02,  1.3769e+00,\n",
      "          -6.1316e-01, -2.5137e+00, -2.4288e-01,  7.0318e-01,  5.9685e-01,\n",
      "          -7.7405e-01, -1.7865e+00, -1.5437e+00,  6.3371e-02,  1.6010e+00,\n",
      "           1.6276e+00, -4.8002e-01, -8.1477e-01,  7.3890e-01],\n",
      "         [-2.2365e+00, -3.4395e-01, -1.3221e-01, -7.0827e-01, -1.9203e+00,\n",
      "           1.2272e+00,  1.9429e+00,  8.6882e-01,  6.8539e-01,  2.1507e+00,\n",
      "           7.6001e-01,  1.8211e-02, -1.3332e+00, -6.7778e-01,  1.0069e+00,\n",
      "          -6.1983e-01, -8.3742e-01,  1.6614e+00,  3.7926e-01, -1.4344e-01,\n",
      "           3.8772e-01,  8.9972e-01,  8.8539e-01,  3.2626e-01, -2.6550e-01,\n",
      "          -1.0786e+00,  8.9662e-01,  2.7047e-01,  6.0653e-02, -1.1491e+00,\n",
      "           7.4686e-01, -7.5259e-01,  4.7440e-01,  9.4813e-01,  2.4412e-01,\n",
      "           3.9122e-01,  5.2904e-01, -1.8624e-01,  1.3294e+00, -6.3364e-02,\n",
      "           3.6510e-01,  1.9069e+00,  5.8226e-01, -1.2383e+00, -9.3440e-01,\n",
      "          -1.2705e+00, -1.3680e+00, -4.4130e-01,  1.9375e-01, -5.1516e-01,\n",
      "           3.2328e-01,  1.0997e-01,  8.7814e-01,  2.2736e-01,  1.8787e+00,\n",
      "          -1.9535e+00,  1.3059e-01, -5.3902e-01, -1.7500e+00, -7.9221e-01,\n",
      "           8.4740e-02, -1.5765e+00, -7.4696e-01, -1.9744e-01],\n",
      "         [-1.1275e+00,  3.4904e-01, -1.1240e-01, -2.0358e+00, -1.0192e+00,\n",
      "           9.1812e-01,  2.7152e+00,  8.3351e-01,  1.4769e+00,  1.1989e+00,\n",
      "          -7.7120e-01,  4.6893e-01,  3.1117e-02, -7.4329e-01,  1.8960e+00,\n",
      "           7.9976e-01, -8.7773e-01, -1.8396e-01, -2.3305e-01,  8.0280e-01,\n",
      "           2.5370e-01,  4.8057e-01,  1.8109e+00, -6.0967e-01, -4.0725e-01,\n",
      "          -1.5345e+00,  1.4906e-01, -5.4915e-01, -8.9316e-01, -8.4687e-01,\n",
      "          -1.7449e-01, -2.0906e-01,  6.8456e-01,  1.0105e+00, -1.4302e-01,\n",
      "           1.8057e+00,  1.1461e+00, -1.1435e+00,  1.1612e+00,  1.0214e+00,\n",
      "           8.2187e-01,  8.7918e-01, -5.6792e-01, -8.6911e-01, -8.2874e-01,\n",
      "          -2.6679e-01,  4.7971e-01,  9.3013e-02,  7.8534e-01, -9.4245e-01,\n",
      "          -8.2213e-02,  4.7764e-03, -3.8443e-01,  5.4046e-01, -1.2587e+00,\n",
      "          -7.4117e-01, -2.5035e+00, -7.6546e-01, -1.3725e+00,  6.1208e-01,\n",
      "           8.4657e-01,  1.4144e-01, -1.2492e+00, -7.7138e-01],\n",
      "         [-6.4998e-01, -1.6679e-01,  1.0567e+00, -1.8018e+00, -6.1655e-01,\n",
      "           2.2011e+00,  1.8488e+00,  1.3824e+00,  1.7669e+00,  5.4872e-01,\n",
      "           9.5257e-01, -7.9238e-01, -1.6848e-01, -1.0508e+00, -6.8011e-01,\n",
      "           8.6996e-01, -8.2715e-01,  3.0620e-01, -6.4211e-01,  5.7866e-01,\n",
      "           2.7277e-01, -3.0384e-02, -4.4225e-02,  3.7866e-01,  4.9460e-01,\n",
      "           1.2706e+00,  8.5725e-01, -1.7305e+00, -1.3555e+00, -2.3338e-01,\n",
      "          -1.8415e+00,  1.2471e+00,  2.3603e+00,  2.0777e+00, -8.7627e-01,\n",
      "           8.1896e-01, -1.0381e+00, -5.7566e-01, -7.6062e-01, -9.3777e-01,\n",
      "          -1.5078e+00, -9.4655e-01, -6.8857e-02, -2.1167e-02, -9.5841e-01,\n",
      "           7.7312e-01, -1.5945e-01, -1.1847e+00, -9.0240e-01, -4.3696e-02,\n",
      "          -5.3928e-01,  2.8915e-01,  5.2514e-01,  6.1168e-03, -1.2778e+00,\n",
      "           3.4964e-01, -6.7041e-01,  7.9125e-02, -5.3369e-01,  4.3112e-01,\n",
      "           1.0700e+00, -6.3368e-01,  9.3179e-01,  5.2301e-01],\n",
      "         [-2.7768e+00, -7.3997e-01, -2.0299e+00, -2.5588e+00, -3.2744e-01,\n",
      "          -3.5091e-01,  7.6732e-01, -2.9021e-01,  6.7389e-01,  1.8555e+00,\n",
      "          -5.6094e-02, -3.1465e-01,  3.2983e-01,  1.4151e+00,  1.4142e+00,\n",
      "          -7.7149e-01, -3.2379e-01, -1.4114e+00,  7.6415e-01, -1.5312e+00,\n",
      "          -1.8443e+00,  8.6766e-02,  1.0125e-01, -1.1073e+00,  4.0426e-01,\n",
      "          -8.6780e-01,  9.9740e-01, -5.2822e-01,  1.6489e+00,  1.2769e+00,\n",
      "          -3.7344e-01,  4.2204e-01,  1.5512e+00,  1.5498e+00,  1.4359e+00,\n",
      "           3.4187e-02, -5.3877e-02,  1.2426e+00,  1.0309e+00, -1.3425e-01,\n",
      "           6.9216e-01,  3.0700e-01, -3.1616e-01, -1.3012e-01,  3.6988e-01,\n",
      "           4.5838e-01,  5.1040e-04, -9.1433e-01, -2.6001e-01,  5.0388e-01,\n",
      "          -5.1974e-01, -1.0424e+00,  1.8537e+00, -1.6543e-02,  4.4072e-01,\n",
      "           1.7429e-01, -1.1159e+00,  2.4625e-01,  1.4693e-01, -2.1527e-01,\n",
      "          -4.6643e-01, -3.5361e-01,  2.2417e-01, -6.7731e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}