{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch. matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[-7.1855e-02,  5.7488e-01,  1.6991e+00, -3.3355e-01, -1.0699e+00,\n",
      "          -1.0368e+00,  8.2217e-01, -1.9106e+00, -2.1634e-01,  7.3367e-01,\n",
      "           1.2038e-01,  8.3790e-01, -2.4612e-01, -1.1015e+00,  3.1288e-01,\n",
      "          -9.1988e-01,  3.0994e-01, -1.2924e+00,  8.9709e-01,  1.9070e-01,\n",
      "          -5.1320e-01,  1.6952e+00, -9.3109e-01,  1.7441e-01, -9.1604e-01,\n",
      "          -5.2297e-01,  4.4118e-01,  1.3991e+00, -1.4732e+00, -1.0045e+00,\n",
      "          -9.2017e-01, -3.7333e-01,  9.5000e-01,  1.5328e+00,  1.1153e+00,\n",
      "           2.3295e+00,  2.1465e+00, -8.1081e-01, -2.6735e-01, -7.2090e-01,\n",
      "           1.1741e+00, -7.3820e-02,  3.4982e-01, -3.7034e-01,  1.3529e+00,\n",
      "          -7.2184e-01, -9.6230e-01,  1.2636e+00, -1.8280e+00, -1.4608e+00,\n",
      "          -1.0353e+00,  2.8943e-01, -2.6714e-01,  5.4362e-01, -1.5717e+00,\n",
      "          -2.5229e-01,  7.5586e-01,  1.5564e+00,  5.6529e-01, -8.7932e-01,\n",
      "           2.4865e-01,  3.4835e-01,  4.8082e-05, -6.5537e-01],\n",
      "         [-4.0062e-01,  4.9628e-01,  1.5468e+00, -1.5934e-01, -4.3516e-01,\n",
      "          -5.5152e-01, -1.5908e+00, -1.1938e+00,  8.8194e-03,  5.9335e-01,\n",
      "           3.2376e-01, -4.9223e-01, -3.5326e-01, -7.3689e-01, -3.6528e-01,\n",
      "          -1.3067e+00, -4.1190e-01,  1.7728e+00,  5.4186e-01,  4.7492e-02,\n",
      "          -8.8111e-01,  1.9945e-01, -7.0689e-01, -1.8882e+00, -2.6439e-01,\n",
      "           2.4490e-01,  1.0679e+00,  3.9394e-01,  1.2236e+00,  1.7063e+00,\n",
      "          -4.2091e-01,  5.4966e-01,  1.6012e+00,  2.2736e+00,  1.2846e+00,\n",
      "           1.1188e+00,  2.7378e+00, -1.0310e+00,  1.0078e-01, -1.7407e+00,\n",
      "           1.8553e-01, -1.1989e+00, -1.0797e+00,  1.1851e+00,  5.5186e-02,\n",
      "          -5.7303e-01, -1.6531e-01,  1.8185e-02, -3.9624e-01, -8.7805e-01,\n",
      "          -1.5119e+00, -6.3859e-01,  1.6425e-01,  1.7284e+00, -4.4434e-01,\n",
      "          -7.5137e-01, -1.3324e+00,  3.4053e-01,  3.2823e-01,  1.2788e+00,\n",
      "          -1.6194e-01, -4.5721e-01, -8.6181e-01,  2.6385e-01],\n",
      "         [ 3.8175e-01, -2.2586e-01,  1.1987e+00,  2.3171e-01, -1.0428e+00,\n",
      "          -2.4557e+00, -2.2697e+00,  3.4215e-01, -1.5625e-01,  4.4865e-02,\n",
      "           2.6415e-01,  8.6049e-02, -2.6615e-01,  9.1320e-01,  1.0909e+00,\n",
      "           3.0983e-01, -2.8251e-01, -1.7221e-01, -5.7459e-01, -3.9782e-01,\n",
      "          -1.0915e+00, -5.0821e-02,  1.9109e-01, -2.4875e-01, -5.4611e-01,\n",
      "           1.2837e+00, -5.8829e-01,  7.5227e-01, -2.1862e-01,  4.7648e-01,\n",
      "          -3.6570e-01,  2.8430e-01,  2.2311e+00,  2.0810e+00,  3.4633e+00,\n",
      "           1.0405e+00,  2.1404e+00, -2.5295e-01, -5.5939e-01, -1.4370e+00,\n",
      "          -2.3705e+00, -1.6284e-01, -5.8509e-01, -1.1070e+00,  9.1692e-02,\n",
      "          -1.3198e+00,  6.2203e-01, -3.4077e-01, -2.8986e-01, -1.0822e+00,\n",
      "           7.4701e-02, -3.7393e-01, -1.8701e-01,  7.8496e-01, -1.5760e-02,\n",
      "          -4.4603e-01,  4.3035e-01,  2.5131e-01,  6.9454e-02,  6.1323e-01,\n",
      "          -5.5336e-01,  5.4585e-01, -1.5188e-01, -1.0224e-01],\n",
      "         [-2.0157e+00, -2.2610e-01, -9.4854e-01,  1.3627e-01, -1.5557e+00,\n",
      "           5.9944e-01, -9.7475e-01, -5.6231e-01,  1.4130e-01,  8.7621e-01,\n",
      "           1.3953e+00,  6.1528e-01, -2.4212e-01, -6.2964e-02, -3.1001e-01,\n",
      "          -5.7630e-01,  1.3772e+00,  1.0586e+00, -1.0756e+00, -1.6503e+00,\n",
      "          -1.3869e-01, -1.3733e+00,  1.3339e+00, -4.3167e-01,  1.2303e-01,\n",
      "           1.7235e+00, -5.8520e-01,  2.6015e-02, -1.2985e+00,  1.1347e+00,\n",
      "          -6.2693e-01, -2.6943e-01,  6.6166e-01,  1.4495e+00,  1.1467e-01,\n",
      "           2.2041e+00,  2.5519e+00, -9.3736e-02,  1.1177e+00,  1.0067e+00,\n",
      "          -1.4146e+00, -6.4042e-01,  9.9352e-01, -1.8152e+00,  5.2539e-01,\n",
      "          -5.9772e-01,  3.6132e-01, -2.2636e-01, -4.5458e-01,  8.7156e-01,\n",
      "          -4.9853e-02,  1.1311e-01,  1.6678e-01,  1.1546e+00,  1.0728e+00,\n",
      "           4.2056e-01, -1.4789e+00, -1.7538e-01,  3.1496e-01, -7.5329e-01,\n",
      "          -2.3676e-01, -1.0662e+00, -1.5133e+00, -2.0115e-01],\n",
      "         [-1.7956e+00,  3.8927e-01,  5.1683e-01, -1.1204e+00, -1.0887e+00,\n",
      "           5.7963e-01, -4.8053e-04, -1.7083e+00, -5.1412e-01, -1.0060e+00,\n",
      "           7.2524e-01,  3.0115e-01,  1.0382e+00, -1.7078e+00, -3.5092e-01,\n",
      "           6.6951e-01,  2.8275e-01,  1.3948e-02,  7.6579e-01,  1.0529e+00,\n",
      "           1.7090e-01, -9.2727e-01, -1.1425e+00, -1.1650e+00,  1.0039e+00,\n",
      "          -9.1499e-01,  1.2479e+00,  1.2798e+00, -5.2828e-01,  3.7706e-02,\n",
      "          -1.9155e-02, -3.7947e-01,  1.6491e+00,  1.8840e+00,  1.5411e+00,\n",
      "           9.7448e-01,  1.7808e+00, -9.6134e-01,  9.8296e-01, -1.5013e+00,\n",
      "           1.1284e+00, -1.9140e+00, -7.5395e-01, -1.3804e-01, -1.4333e+00,\n",
      "           5.9621e-01,  1.0110e+00,  1.0940e+00, -2.8287e-01, -9.5908e-02,\n",
      "          -5.7297e-01, -3.2968e-01,  7.0827e-01, -3.5168e-01,  8.5588e-01,\n",
      "           8.6434e-02,  4.3344e-01, -8.8492e-01, -1.4069e+00,  1.7086e+00,\n",
      "          -6.3626e-01, -1.1864e+00, -4.4922e-01,  7.5768e-01]],\n",
      "\n",
      "        [[-5.9359e-01, -1.7996e+00, -1.5322e+00, -1.3487e+00, -4.6723e-01,\n",
      "          -6.6666e-02,  9.2130e-02, -1.3825e-02, -9.1855e-01,  6.8427e-01,\n",
      "           8.1751e-01,  1.0211e+00,  1.6194e+00, -2.6419e-01,  7.2606e-01,\n",
      "          -5.6970e-01, -2.2087e+00, -9.3273e-01,  6.0562e-01, -3.5701e-01,\n",
      "           1.1106e+00, -4.4623e-01, -1.0187e+00,  6.2350e-01, -9.3862e-01,\n",
      "           1.1646e+00, -3.4646e-01,  1.9092e-02,  6.1393e-01,  2.3825e-01,\n",
      "           6.5178e-01,  2.6143e+00, -1.6503e+00, -5.9788e-01, -1.4677e+00,\n",
      "          -2.1264e+00, -4.2327e-01, -6.1094e-01,  1.2288e+00,  2.3932e+00,\n",
      "           5.6246e-01,  3.4090e-01,  5.2530e-01, -5.5496e-01,  5.4591e-01,\n",
      "           7.6961e-01,  3.7914e-01, -1.4899e-01,  1.7082e+00, -5.3097e-03,\n",
      "          -5.7773e-01,  1.4211e+00,  1.4348e-01,  3.1099e-01,  1.3493e-01,\n",
      "          -5.8821e-01, -1.2974e+00,  1.0384e+00, -1.3592e-01,  1.0486e+00,\n",
      "          -5.2795e-01,  1.4634e-03, -8.4339e-01,  2.2436e-01],\n",
      "         [-1.0892e+00, -1.2981e+00,  1.5706e-01, -7.4337e-01,  1.7588e+00,\n",
      "          -7.5764e-01, -2.0387e-01, -3.9070e-01, -7.6642e-01, -1.3216e-01,\n",
      "           3.1109e-01,  7.2455e-01,  8.7786e-02, -7.4734e-01,  4.5688e-01,\n",
      "          -4.1605e-01,  1.3229e+00, -3.3997e-02, -5.4378e-01,  1.0239e-02,\n",
      "          -3.4606e-01, -1.7850e-01, -2.0952e-01,  1.5093e+00,  1.0793e-01,\n",
      "          -4.6993e-01,  3.2484e-02,  1.0752e+00, -6.4055e-01, -1.4910e+00,\n",
      "           1.0253e+00,  1.0245e+00, -1.5655e+00, -1.7974e+00, -1.2403e+00,\n",
      "          -1.9316e+00,  1.6071e-01, -9.4672e-01,  1.0918e+00,  2.6915e+00,\n",
      "           2.1124e+00,  1.2681e+00, -8.9967e-02, -2.4895e+00, -1.2048e+00,\n",
      "          -2.4814e-01,  1.1411e+00, -7.2161e-01, -5.3414e-01,  2.4510e-01,\n",
      "          -7.9705e-02,  7.9544e-01, -1.9611e-01,  1.2702e+00,  1.1923e+00,\n",
      "          -1.6828e-01,  3.2185e-02, -7.9033e-01,  2.4381e-01,  5.7710e-01,\n",
      "           1.5670e-01,  1.5019e+00, -3.0742e-01,  6.8521e-01],\n",
      "         [-1.6082e+00, -2.6321e-01,  1.2352e+00, -6.9852e-01, -5.9800e-01,\n",
      "          -1.1462e+00,  8.8956e-01, -4.4833e-02,  6.7427e-02,  1.3802e+00,\n",
      "           7.6949e-01,  1.1003e+00,  1.2064e+00,  1.3872e-01,  1.3277e+00,\n",
      "          -1.6448e+00,  1.1625e+00, -1.1277e-01, -2.1914e-01, -1.8309e-01,\n",
      "          -2.8796e-01, -6.9839e-01,  6.4675e-01, -7.4404e-02,  1.4593e+00,\n",
      "          -5.6918e-01, -1.4090e-01,  7.1932e-01, -4.2913e-01,  3.0794e-01,\n",
      "           1.0328e-01,  3.5851e-01, -1.6433e+00, -2.1543e+00, -1.5429e+00,\n",
      "          -2.1164e+00,  7.1344e-01, -1.1966e+00, -1.3773e-02,  1.6213e+00,\n",
      "           1.4131e+00,  9.0248e-01, -3.8858e-02, -9.6582e-01,  3.1245e-01,\n",
      "          -2.5411e-01,  1.7471e+00,  1.3194e+00,  5.9298e-01,  7.5624e-01,\n",
      "          -2.4537e-01, -9.4809e-02, -9.4829e-02,  1.1569e+00,  1.4775e+00,\n",
      "          -1.7318e-01, -1.2548e+00, -1.1838e+00, -2.4804e-01, -2.2269e+00,\n",
      "           2.5949e-01,  4.5399e-01, -8.5848e-01, -5.7382e-01],\n",
      "         [-2.3577e+00, -8.2313e-01,  9.7653e-01, -8.0013e-02, -1.9676e+00,\n",
      "          -7.0484e-01,  3.6890e-01, -4.0312e-01, -8.0221e-01, -2.2842e+00,\n",
      "          -8.0052e-02, -5.7735e-02,  1.9749e-01,  1.3951e+00, -1.3418e-01,\n",
      "           3.9229e-01,  5.6876e-01,  7.0715e-01, -1.8538e-01, -6.8352e-01,\n",
      "          -1.8107e-01, -2.4264e+00,  5.1526e-01, -3.8165e-01,  7.8443e-01,\n",
      "           7.7241e-01,  5.7274e-01, -6.0333e-01, -2.4689e-01,  2.8769e-01,\n",
      "           9.1425e-01, -4.2222e-02, -3.5558e-01, -6.9158e-01, -1.5260e+00,\n",
      "          -1.1340e+00, -1.4180e+00,  1.5629e+00,  5.0976e-01,  2.6744e+00,\n",
      "           2.1453e+00,  2.4194e-01,  1.2424e+00,  8.9472e-03, -1.0467e+00,\n",
      "          -6.2776e-01,  8.2673e-01,  7.8887e-02, -4.8497e-01,  1.1495e+00,\n",
      "           5.2944e-01, -1.3940e-01,  1.2051e+00,  1.1014e+00,  9.1370e-01,\n",
      "           1.1661e+00, -3.7445e-01,  4.0223e-01,  1.2921e-01, -4.9655e-01,\n",
      "          -9.9392e-02,  2.1184e-02, -1.3299e+00, -1.9269e-01],\n",
      "         [-1.3337e+00, -1.2194e+00, -8.7308e-01, -1.6873e+00, -8.4882e-01,\n",
      "          -2.8931e-01, -3.9370e-01, -1.4746e-01,  3.0373e-01, -9.8814e-01,\n",
      "           1.3409e+00,  1.9581e-01,  8.1680e-01, -2.3613e-01,  1.1205e+00,\n",
      "          -4.1170e-01, -1.2212e+00,  6.4595e-01,  1.3525e+00,  7.2610e-01,\n",
      "          -6.4463e-01, -5.5752e-01, -9.8880e-01,  1.7850e+00,  2.4808e-01,\n",
      "          -7.0189e-01,  1.6802e+00,  4.5299e-01,  3.8532e-01, -1.0474e+00,\n",
      "          -5.5990e-01, -2.0002e-02,  8.0962e-01, -1.6434e+00, -7.6222e-01,\n",
      "          -2.4919e+00, -5.1103e-01, -5.0367e-01,  1.0049e+00,  2.3585e+00,\n",
      "           2.6495e+00,  1.5050e+00, -3.7509e-01, -3.0552e-01,  3.0218e-01,\n",
      "           1.0554e+00,  1.8671e-01,  5.9722e-01,  7.8987e-01, -3.0073e-01,\n",
      "           3.1162e-01, -2.9587e-01, -8.2729e-01,  3.6969e-01, -1.4011e-01,\n",
      "          -1.2056e+00, -2.8174e-01,  2.7714e-01, -6.3921e-01,  6.6286e-01,\n",
      "           1.8372e+00, -2.9953e-02, -5.0148e-01, -7.8644e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[ 5.2592e-01,  5.9251e-01,  1.2091e+00, -6.2124e-01, -1.1024e+00,\n",
      "          -2.2659e-01,  1.1451e+00, -1.4376e+00,  4.2081e-01,  9.5455e-01,\n",
      "           4.4154e-01,  1.0591e+00, -4.2336e-01, -6.8584e-01, -5.5280e-02,\n",
      "          -9.7339e-01, -1.4228e-01, -1.4229e+00,  1.2895e+00,  9.8546e-01,\n",
      "           3.4625e-01,  2.0627e+00, -3.1307e-01,  5.2694e-01, -6.4048e-01,\n",
      "          -6.7646e-01,  1.0326e+00,  1.1615e+00, -1.3496e+00, -8.8195e-01,\n",
      "          -7.8941e-01, -2.9552e-01, -3.1684e-01, -1.3807e-01, -3.1796e-01,\n",
      "           1.5339e+00,  2.7400e-01, -1.0763e+00, -7.8740e-01, -5.7490e-01,\n",
      "           1.3726e+00, -1.7887e-01, -3.1767e-02, -7.0596e-01,  2.2123e+00,\n",
      "          -2.1191e-01, -1.4895e+00,  1.4900e+00, -2.5139e+00, -1.3836e+00,\n",
      "          -3.4774e-01,  6.5075e-01,  1.8678e-01,  2.0656e-01, -1.8301e+00,\n",
      "          -4.7346e-02,  9.3532e-01,  2.1014e+00,  4.3885e-01, -1.1251e+00,\n",
      "          -3.9500e-01,  1.6649e-01,  8.4980e-01, -6.6293e-01],\n",
      "         [-1.8751e-02,  5.9893e-01,  1.1006e+00, -9.2747e-01, -3.5606e-01,\n",
      "           4.8661e-01, -1.1296e+00, -2.5026e-01,  8.4791e-01,  7.3100e-01,\n",
      "           1.2110e+00, -6.8302e-01, -4.0621e-01, -5.0046e-01, -6.4066e-01,\n",
      "          -1.1341e+00, -5.8716e-02,  2.0360e+00,  7.8888e-01,  1.0405e+00,\n",
      "          -4.7720e-01,  3.1589e-01, -7.0134e-01, -2.1026e+00,  1.9329e-01,\n",
      "           1.3943e-03,  1.7621e+00,  2.3962e-02,  1.9606e+00,  2.6333e+00,\n",
      "          -6.4444e-01,  9.7250e-01, -2.6716e-01,  7.6125e-01, -6.6386e-01,\n",
      "           4.0846e-02,  8.9039e-01, -1.4641e+00,  2.0090e-01, -2.0594e+00,\n",
      "           4.9786e-01, -8.0140e-01, -1.6813e+00,  1.7747e+00,  6.8328e-01,\n",
      "          -8.9962e-01, -2.4597e-01, -1.5581e-01, -9.4819e-01, -8.6658e-01,\n",
      "          -1.0282e+00, -7.1025e-01,  3.7977e-01,  1.4057e+00, -2.3076e-01,\n",
      "          -8.4825e-01, -1.1661e+00,  5.1212e-02,  4.6919e-01,  1.4910e+00,\n",
      "          -5.8687e-01, -1.1271e+00,  8.1629e-02,  3.4988e-01],\n",
      "         [ 1.2656e+00, -2.5214e-01,  6.6549e-01, -5.1003e-01, -1.5784e+00,\n",
      "          -2.4036e+00, -1.4526e+00,  1.7377e+00,  6.6068e-01,  9.4541e-01,\n",
      "           8.1688e-01,  1.0357e-02, -8.3947e-02,  2.0328e+00,  1.6150e+00,\n",
      "           6.7208e-01, -3.8169e-01, -7.9617e-01, -2.5470e-01,  4.8483e-01,\n",
      "          -9.6016e-01,  6.9212e-01,  5.8383e-01, -2.1978e-01, -6.3910e-01,\n",
      "           1.5510e+00, -4.9145e-01, -4.6033e-02,  2.6212e-02,  8.4505e-01,\n",
      "          -1.0918e-01,  3.3702e-01,  7.1742e-01,  4.7594e-01,  2.2993e+00,\n",
      "          -1.4554e-01,  5.6742e-01,  2.4728e-01, -5.9901e-02, -1.8721e+00,\n",
      "          -2.3967e+00,  6.9245e-01, -1.0847e+00, -2.0605e+00, -2.4533e-01,\n",
      "          -1.6653e+00,  1.2280e-01, -1.7350e-01, -4.3389e-01, -1.3708e+00,\n",
      "           5.0896e-01, -8.2716e-01, -4.1665e-02,  4.8520e-01,  1.0372e+00,\n",
      "          -7.9346e-01,  8.4835e-01, -4.6133e-01,  2.0484e-01,  3.0967e-01,\n",
      "          -1.0055e+00,  2.4476e-01,  7.6807e-01,  3.4455e-01],\n",
      "         [-1.2364e+00, -2.1910e-01, -1.1653e+00, -1.2643e-01, -1.7800e+00,\n",
      "           2.9727e-01, -9.9130e-01,  3.7176e-01, -1.4737e-01,  1.0395e+00,\n",
      "           1.8902e+00,  7.2855e-01,  1.6620e-02,  3.6409e-01, -6.9374e-01,\n",
      "           5.4410e-01,  1.4249e+00,  1.1596e+00, -6.4275e-01, -1.4290e+00,\n",
      "           5.3971e-01, -8.9794e-01,  2.3503e+00, -6.0462e-01,  1.4189e-01,\n",
      "           1.9845e+00, -9.6826e-01, -1.0404e+00, -1.4003e+00,  1.6045e+00,\n",
      "          -6.0154e-01, -3.7561e-01, -1.1375e+00,  2.7854e-02, -8.7578e-01,\n",
      "           1.3686e+00,  1.0908e+00, -5.6881e-01,  1.5156e+00,  4.2147e-01,\n",
      "          -1.3861e+00,  8.9027e-02,  1.3265e+00, -1.8100e+00,  1.1212e+00,\n",
      "          -4.7015e-01,  9.2662e-03, -5.3973e-01, -8.0810e-01,  1.0246e+00,\n",
      "           2.4279e-01,  2.7303e-01,  6.5921e-01,  7.6070e-01,  1.2377e+00,\n",
      "           8.8825e-01, -1.2402e+00, -4.5227e-01,  5.9616e-01, -9.1400e-01,\n",
      "          -6.6499e-01, -1.0615e+00, -5.3209e-01, -3.2914e-01],\n",
      "         [-1.3432e+00,  9.0694e-01, -1.2839e-01, -1.3002e+00, -7.3718e-01,\n",
      "           1.3497e+00,  3.6279e-01, -5.7629e-01, -3.8165e-01, -1.1046e+00,\n",
      "           1.6610e+00,  2.8228e-01,  1.5677e+00, -1.9137e+00, -6.2705e-01,\n",
      "           1.2789e+00,  5.0256e-01,  3.7761e-01,  6.2057e-01,  1.8064e+00,\n",
      "           9.6433e-01, -7.4693e-01, -6.3406e-01, -9.8120e-01,  9.4427e-01,\n",
      "          -1.0590e+00,  1.4889e+00,  8.6060e-01, -7.4376e-02, -8.5428e-03,\n",
      "           6.5668e-02, -7.6767e-02, -5.7753e-02,  8.5061e-01,  2.3979e-01,\n",
      "           3.5283e-02, -2.2214e-01, -1.7975e+00,  6.3277e-01, -2.3408e+00,\n",
      "           8.1030e-01, -1.4753e+00, -1.0107e+00, -3.1187e-01, -1.4734e+00,\n",
      "           1.1239e+00,  7.9080e-01,  9.9617e-01, -5.7504e-01, -1.6430e-01,\n",
      "          -2.3654e-01, -1.5025e-01,  1.1172e+00, -8.7324e-01,  1.2403e+00,\n",
      "           7.5876e-01,  8.2364e-01, -1.0164e+00, -1.0515e+00,  1.4688e+00,\n",
      "          -1.4202e+00, -1.1827e+00,  3.7610e-01,  7.4797e-01]],\n",
      "\n",
      "        [[ 1.3173e+00, -1.0389e+00, -1.3571e+00, -1.0263e+00, -3.0293e-01,\n",
      "           2.2767e-01,  4.0002e-01,  6.2744e-01, -1.3357e+00,  1.0401e+00,\n",
      "           9.4537e-01,  1.2389e+00,  8.1495e-01, -8.3399e-01,  4.0454e-01,\n",
      "          -4.1810e-01, -3.0703e+00, -1.3228e+00,  7.0705e-01,  7.9279e-02,\n",
      "           1.2833e+00, -3.4610e-01, -1.0394e+00,  5.3100e-01, -1.4435e+00,\n",
      "           1.5254e+00, -9.0920e-01, -5.0449e-01,  1.0266e+00, -5.0173e-01,\n",
      "           6.2677e-01,  2.7056e+00, -1.2424e+00,  6.8819e-01,  3.6759e-01,\n",
      "          -4.6194e-01,  3.7296e-01, -9.5308e-01,  6.5706e-01,  9.3969e-01,\n",
      "          -1.0171e-01, -1.7683e-01,  1.9058e-01, -1.1490e+00, -2.5384e-02,\n",
      "           3.9866e-01, -1.3111e-01,  1.6623e-01,  1.5578e+00, -5.7273e-01,\n",
      "          -5.1824e-01,  1.4258e+00,  4.2440e-01, -4.8175e-01,  5.1414e-01,\n",
      "          -4.1525e-01, -1.6054e+00,  1.3191e+00, -2.6108e-01,  1.2905e+00,\n",
      "          -1.3235e+00, -3.9010e-01, -9.6031e-01,  4.0623e-01],\n",
      "         [ 4.1544e-03, -9.1208e-01, -3.3044e-01, -1.8890e-01,  2.2407e+00,\n",
      "          -1.0132e+00, -3.7167e-01, -9.0321e-02, -7.4292e-01, -2.3654e-01,\n",
      "           3.7587e-01,  1.1343e+00, -4.4390e-01, -9.8834e-01,  1.8275e-01,\n",
      "          -3.7092e-01,  1.4570e+00, -1.9437e-02, -4.4898e-01,  3.0378e-02,\n",
      "          -1.1725e-01, -1.9553e-01, -3.1205e-01,  1.7411e+00, -3.8047e-01,\n",
      "          -3.0348e-01, -5.0988e-01,  7.4533e-01, -6.9055e-01, -2.0083e+00,\n",
      "           5.8159e-01,  1.1319e+00, -1.4919e+00, -1.2038e+00, -5.9663e-01,\n",
      "          -7.5008e-01,  1.2334e+00, -1.7342e+00,  8.7664e-01,  1.5208e+00,\n",
      "           1.7592e+00,  1.1380e+00,  2.3447e-01, -3.0259e+00, -1.6297e+00,\n",
      "          -6.5530e-01,  6.3010e-01, -4.9540e-01, -9.7175e-01, -3.8390e-01,\n",
      "          -6.3371e-02,  9.0012e-01, -1.3685e-01,  1.1345e+00,  1.4118e+00,\n",
      "          -1.2715e-01, -3.3108e-01, -3.1265e-01,  5.2002e-01,  1.2627e+00,\n",
      "          -2.5191e-01,  1.7493e+00, -5.7931e-02,  8.9857e-01],\n",
      "         [ 7.0978e-01,  6.9009e-01,  2.0314e+00,  2.2728e-01, -6.4439e-01,\n",
      "          -1.5596e+00,  1.0264e+00,  6.3406e-01,  6.6732e-01,  1.5604e+00,\n",
      "           3.3463e-01,  9.0766e-01,  1.0264e+00,  2.5516e-01,  1.5223e+00,\n",
      "          -1.4729e+00,  8.3979e-01, -8.3187e-01,  1.1394e-01,  2.1305e-01,\n",
      "           4.3212e-02, -7.5888e-01,  8.0561e-01, -4.7693e-01,  1.5268e+00,\n",
      "          -7.0562e-01,  2.7736e-01,  2.6944e-01, -7.5340e-01,  6.0397e-01,\n",
      "          -4.6309e-01, -5.8300e-02, -1.8763e+00, -9.8419e-01,  1.3903e-01,\n",
      "           1.9863e-01,  1.3989e+00, -1.9379e+00, -1.4719e+00, -5.1673e-01,\n",
      "           5.1759e-01,  1.5942e-01, -8.9560e-02, -8.6501e-01,  4.5072e-01,\n",
      "          -7.0683e-01,  1.2206e+00,  1.7744e+00,  3.4241e-01,  2.4597e-01,\n",
      "          -7.6486e-02, -4.4044e-01, -6.7308e-02,  5.1155e-01,  1.9726e+00,\n",
      "          -3.4751e-01, -2.0558e+00, -1.4833e+00, -3.9477e-01, -2.6107e+00,\n",
      "          -2.3030e-01, -1.5187e-01, -8.7665e-01, -3.0933e-01],\n",
      "         [-1.9474e+00, -1.2917e-01,  1.5554e+00,  5.9965e-01, -2.3298e+00,\n",
      "          -3.8380e-01,  4.5293e-01, -1.3849e-01, -1.1754e+00, -2.1448e+00,\n",
      "          -1.7699e-01, -2.4122e-01, -1.9756e-01,  2.1699e+00, -2.3542e-01,\n",
      "           1.6391e+00,  6.2889e-01,  1.4249e+00, -6.0710e-02, -1.0608e+00,\n",
      "           2.6354e-01, -2.8752e+00,  4.0476e-01, -6.1295e-01,  4.4285e-01,\n",
      "           7.7223e-01, -4.7575e-02, -1.5553e+00, -2.7129e-01,  2.8686e-01,\n",
      "           5.7343e-01, -4.0231e-01,  1.7577e-01,  3.0888e-01,  8.4936e-02,\n",
      "           5.8100e-01, -8.9210e-01,  1.0011e+00, -7.6504e-01,  2.4585e-01,\n",
      "           1.0722e+00, -3.5798e-01,  1.8436e+00, -2.3640e-01, -1.6150e+00,\n",
      "          -9.7994e-01,  2.6862e-01,  4.1729e-01, -6.6622e-01,  8.7414e-01,\n",
      "           3.8361e-01, -2.7297e-01,  1.2055e+00,  7.4036e-01,  1.4725e+00,\n",
      "           1.3771e+00, -4.5349e-01,  8.9384e-01,  7.8588e-02, -2.5433e-01,\n",
      "          -2.9756e-01, -1.4195e-01, -1.2712e+00, -4.8963e-02],\n",
      "         [ 3.9589e-02, -1.9078e-02, -5.8783e-01, -8.5292e-01, -2.2476e-01,\n",
      "          -1.6376e-01, -2.6427e-01,  2.7651e-02,  4.0877e-01, -1.5333e+00,\n",
      "           1.8553e+00, -2.1091e-01,  3.8807e-01,  1.2293e-01,  6.3868e-01,\n",
      "          -4.8842e-01, -1.3933e+00,  9.1254e-01,  1.2731e+00,  1.3855e+00,\n",
      "          -1.9541e-01, -1.4023e-01, -1.7727e+00,  1.5423e+00,  4.3409e-01,\n",
      "          -6.7611e-01,  1.6750e+00,  2.9366e-01,  1.1117e+00, -2.0502e+00,\n",
      "          -1.6351e+00, -6.4123e-01,  1.4866e+00, -9.5600e-01,  6.1759e-01,\n",
      "          -9.6200e-01, -9.0261e-01, -7.4690e-01,  6.9829e-02,  4.1454e-01,\n",
      "           2.1017e+00,  1.2876e+00, -7.7062e-01, -4.0871e-01,  1.9403e-01,\n",
      "           7.2437e-01, -4.7135e-01,  1.0677e+00,  7.2345e-01, -1.4500e+00,\n",
      "           1.2195e+00, -1.6886e-01, -9.8433e-01, -1.5146e-01, -4.5998e-01,\n",
      "          -1.6903e+00, -4.2388e-01,  7.7543e-01, -8.5927e-01,  5.6716e-01,\n",
      "           2.3562e+00, -5.6180e-01,  2.4145e-01, -1.1384e+00]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}