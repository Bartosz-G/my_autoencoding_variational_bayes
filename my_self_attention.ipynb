{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def Attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, d_k: int):\n",
    "    dim = len(V.shape)\n",
    "\n",
    "    if dim == 3:\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, 1, 2))\n",
    "    else:\n",
    "        QK_T = torch.matmul(Q, torch.t(K))\n",
    "\n",
    "    QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "    softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "    return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.qkv = nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            qkv = nn.ModuleList([nn.Linear(d_model, d_k), nn.Linear(d_model, d_k), nn.Linear(d_model, d_v)])\n",
    "            self.qkv.append(qkv)\n",
    "\n",
    "\n",
    "        self."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.qkv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 16])\n",
      "Shape of K: torch.Size([2, 5, 16])\n",
      "Shape of V: 3\n",
      "tensor([[[-1.5192e-01, -1.2545e-01,  4.4548e-01,  1.8337e-01, -1.6018e-01,\n",
      "          -2.0040e-01,  1.2407e+00,  1.7578e-02,  1.7038e-01,  6.4647e-01,\n",
      "          -3.2317e-01,  5.0211e-01,  1.4055e-01,  8.1878e-01, -7.4401e-01,\n",
      "          -6.1711e-01,  1.3111e-01, -5.7276e-01, -5.2867e-01,  1.5531e-01,\n",
      "          -6.3535e-01, -4.2816e-02,  6.7704e-01, -1.1522e-01, -3.4724e-01,\n",
      "           4.5483e-01, -1.8262e-01,  3.8665e-02,  1.1033e+00,  1.0140e+00,\n",
      "           4.6200e-01,  5.8121e-01,  4.8660e-01,  1.3067e+00,  8.3132e-01,\n",
      "           9.8502e-01,  7.4770e-01,  6.6981e-01,  1.4395e+00, -1.1651e-02,\n",
      "          -1.1223e-01,  3.7959e-01,  1.7023e-01,  4.2798e-01,  1.0195e-02,\n",
      "          -1.3068e+00,  4.9770e-01,  4.3065e-01,  9.5144e-02,  2.8363e-01,\n",
      "           1.1345e+00,  7.8481e-01, -2.6121e-01,  5.8710e-02,  7.1560e-01,\n",
      "          -9.5310e-03,  1.4651e-01,  2.4114e-01,  3.4512e-01, -2.0547e-01,\n",
      "          -2.7022e-01,  3.8016e-01, -9.0800e-01,  6.4720e-01],\n",
      "         [-3.7395e-01, -2.9800e-01, -1.9092e-01,  5.3015e-01, -2.2167e-01,\n",
      "           4.7924e-01,  9.0013e-01, -2.2702e-01,  8.7384e-01,  4.6068e-01,\n",
      "           2.6582e-01,  2.5629e-01,  4.3683e-01,  1.3300e+00, -1.1937e-01,\n",
      "          -3.5441e-01,  1.1086e-01, -2.3467e-01, -9.1182e-01,  1.8380e-01,\n",
      "           1.6902e-02,  1.2916e-02,  1.7037e-01,  5.2782e-02, -7.5608e-01,\n",
      "          -1.7549e-02,  4.0548e-01, -5.9865e-02,  9.4720e-01,  2.2997e-01,\n",
      "           2.7821e-01,  4.0781e-01,  5.5836e-01,  6.6379e-01,  1.1443e+00,\n",
      "           1.4370e+00,  1.0312e+00,  1.2255e+00,  1.3441e+00,  1.1063e-01,\n",
      "          -3.6620e-01,  3.8339e-01,  4.3344e-01,  3.5252e-01, -7.2322e-01,\n",
      "          -6.8647e-01, -3.2185e-01,  6.4127e-01,  1.4956e-01,  3.1201e-01,\n",
      "           5.0896e-01,  6.1030e-01, -4.4522e-01,  6.0222e-02,  4.0788e-01,\n",
      "          -2.4158e-03, -4.5635e-01,  5.2468e-01,  5.5362e-01, -3.9187e-01,\n",
      "          -4.8456e-01,  7.2214e-01, -4.9930e-01,  5.7477e-01],\n",
      "         [ 2.8730e-01, -3.0090e-01, -6.3783e-01,  5.2377e-01, -3.9018e-01,\n",
      "           7.6872e-01,  2.9595e-01,  1.8158e-02,  4.8837e-01,  7.3122e-01,\n",
      "           4.2310e-01,  3.2507e-01,  5.1466e-01,  7.7255e-01,  1.0942e-01,\n",
      "          -2.0032e-01,  1.8393e-01, -1.3993e-01, -1.0615e+00, -1.7177e-01,\n",
      "           1.0748e-01, -2.4273e-01,  4.5244e-02, -3.1702e-01, -7.0073e-01,\n",
      "          -2.1199e-02,  1.8644e-01,  6.7091e-02,  6.4418e-01, -7.1139e-02,\n",
      "           2.0210e-01,  1.3486e-01,  9.0619e-01,  3.0732e-01,  1.0376e+00,\n",
      "           1.5722e+00,  9.4951e-01,  1.1110e+00,  1.0491e+00,  3.1769e-01,\n",
      "          -6.5658e-01,  3.2394e-01,  5.4719e-01, -1.0413e-01,  6.0156e-02,\n",
      "          -5.5255e-01,  9.6262e-02,  2.3265e-01, -1.6497e-01,  5.0669e-01,\n",
      "           2.5594e-01,  6.5162e-01, -2.3302e-01,  4.2728e-01,  1.8958e-01,\n",
      "           1.1195e-01, -6.9404e-01,  1.6486e-01,  3.8823e-02, -3.2004e-02,\n",
      "          -5.7385e-01,  5.2042e-01, -2.9204e-01,  1.5620e-01],\n",
      "         [ 3.6201e-01, -2.6819e-01, -3.9150e-01,  3.8130e-01, -5.3299e-01,\n",
      "           6.6370e-01,  3.7063e-01,  2.7595e-02,  2.2924e-01,  1.1440e+00,\n",
      "           1.4258e-01,  1.1904e-01,  2.8770e-01,  5.9197e-01,  3.0091e-03,\n",
      "          -3.1513e-02,  4.1280e-01, -2.3135e-01, -8.7244e-01, -2.0364e-02,\n",
      "          -1.8348e-01, -1.7073e-01,  3.4884e-01, -1.0512e-01, -5.5931e-01,\n",
      "           3.3119e-01,  2.1187e-01,  2.3346e-01,  5.5312e-01,  1.8815e-01,\n",
      "           2.8742e-01,  2.2445e-01,  9.1986e-01,  6.5679e-01,  9.6846e-01,\n",
      "           1.6233e+00,  9.2272e-01,  9.7506e-01,  1.1236e+00,  5.1950e-01,\n",
      "          -6.4607e-01,  4.8349e-01,  4.3912e-01, -1.8544e-01, -7.1928e-02,\n",
      "          -6.9682e-01,  3.7074e-01,  1.0791e-01, -9.0292e-02,  7.1485e-01,\n",
      "           1.5608e-01,  6.9711e-01, -4.3473e-01,  3.8160e-01,  1.6910e-01,\n",
      "           1.4844e-01, -4.8527e-01, -8.0797e-03,  1.5578e-01,  2.3433e-01,\n",
      "          -3.5356e-01,  2.7244e-01, -3.0251e-01,  3.1266e-01],\n",
      "         [ 1.2197e-01, -4.6452e-01, -5.9309e-01,  1.3636e-01, -1.6704e-01,\n",
      "           1.0747e+00,  7.4518e-01,  3.8373e-01, -8.1834e-01,  1.0345e+00,\n",
      "          -2.8148e-01,  1.3245e+00,  2.0420e-01,  4.6988e-01,  1.0079e-01,\n",
      "          -4.1501e-01, -1.0511e-01, -2.4993e-01, -9.5391e-01, -8.9090e-01,\n",
      "           5.1487e-02, -3.6504e-02, -8.2615e-01, -2.1318e-03,  3.9134e-01,\n",
      "           2.9138e-01,  3.8569e-01,  5.5990e-02,  3.7068e-02, -1.0608e+00,\n",
      "          -6.1864e-02, -1.3235e-01,  8.6208e-01,  4.0783e-01,  1.1445e+00,\n",
      "           8.1031e-01,  1.2583e+00,  1.1434e+00,  7.0063e-01,  1.2251e+00,\n",
      "           8.5850e-02,  2.3454e-01,  6.7016e-01, -7.5054e-01,  5.6092e-01,\n",
      "          -5.8367e-01, -5.4558e-01, -8.9818e-02, -1.0698e-01, -9.5327e-02,\n",
      "          -1.7578e-01,  7.2972e-01,  5.0922e-01,  3.6199e-01,  4.2607e-01,\n",
      "          -2.2134e-01, -8.6929e-01, -8.6979e-01,  3.3332e-03,  4.1328e-01,\n",
      "          -6.9825e-01,  6.0232e-01, -7.6245e-02, -7.1155e-02]],\n",
      "\n",
      "        [[ 5.8716e-01,  8.6917e-01,  5.5071e-01, -2.0039e-01,  2.9471e-01,\n",
      "           4.3311e-01,  1.2801e+00,  1.7739e-01, -2.9544e-02,  1.0224e+00,\n",
      "           9.2883e-01, -3.1589e-01, -1.3945e+00, -1.0471e+00,  2.3737e-01,\n",
      "           1.3743e+00, -1.8532e-01, -6.2111e-02,  1.0693e+00,  1.3493e+00,\n",
      "           5.5639e-01, -9.5428e-02, -2.2448e+00,  8.6078e-01, -1.3376e+00,\n",
      "           7.6208e-02,  3.6597e-01, -9.6484e-01, -4.9866e-01, -5.9742e-01,\n",
      "           6.9148e-01, -9.4945e-01,  5.1368e-01,  2.4557e+00,  9.2390e-01,\n",
      "           1.0207e+00,  9.5535e-01,  1.1560e+00,  6.3273e-01, -4.4104e-01,\n",
      "           1.0270e+00, -1.8652e-01,  3.6857e-01, -1.3226e+00,  2.9210e-01,\n",
      "           5.8396e-01, -1.0731e+00,  6.4883e-01, -1.4025e+00,  1.9872e+00,\n",
      "           5.4262e-01, -6.5770e-01, -4.4830e-01, -1.3731e+00,  1.0968e+00,\n",
      "           6.1633e-01,  4.7757e-01,  7.9226e-01,  8.6577e-01,  8.1663e-01,\n",
      "           7.5262e-01,  6.9181e-01,  3.0923e-01,  6.8072e-01],\n",
      "         [ 9.3564e-01, -8.7900e-01, -2.0367e-01,  1.2835e+00,  9.0027e-01,\n",
      "          -6.8660e-01, -1.3022e-01, -9.4903e-01,  2.6248e-01,  1.1933e+00,\n",
      "           8.4082e-01,  2.9838e-01, -2.1791e-01,  1.2226e-01, -1.0351e-01,\n",
      "           1.2492e+00, -4.7854e-02,  1.2293e-01,  5.7860e-01,  6.9560e-01,\n",
      "           6.9508e-01,  6.9886e-02,  4.1360e-01,  3.8594e-01, -5.5254e-01,\n",
      "          -1.3568e+00, -3.9586e-01,  4.7357e-01,  9.9930e-01, -6.7463e-02,\n",
      "          -1.8550e-01,  1.1839e+00,  3.6621e-01,  1.2559e+00, -3.2433e-01,\n",
      "          -5.8979e-02, -5.1700e-01,  4.7769e-01,  5.6819e-01,  3.9845e-01,\n",
      "          -7.0465e-02, -8.8260e-01, -8.2149e-01, -3.2896e-01,  1.3035e+00,\n",
      "          -6.3616e-01, -4.6451e-01,  5.4129e-01,  1.7087e-01,  7.7378e-02,\n",
      "          -7.7642e-01, -1.5884e+00, -7.2159e-02, -2.2329e-01,  2.0130e-01,\n",
      "           1.4841e-02, -2.8951e-01, -2.0102e-01,  1.0183e+00, -5.1842e-01,\n",
      "          -2.7499e-01, -7.2183e-01,  1.6873e-01, -1.6304e-01],\n",
      "         [ 4.9240e-01,  8.8677e-01,  4.4680e-01, -2.8753e-01,  2.2275e-01,\n",
      "           3.8392e-01,  1.1176e+00,  1.6535e-01, -6.5170e-02,  9.2587e-01,\n",
      "           9.0218e-01, -3.4328e-01, -1.3223e+00, -1.0471e+00,  1.5650e-01,\n",
      "           1.3312e+00, -2.6518e-01,  8.2873e-02,  1.0056e+00,  1.3245e+00,\n",
      "           4.9911e-01, -1.8543e-01, -2.1418e+00,  7.8320e-01, -1.3042e+00,\n",
      "           1.1530e-01,  3.4701e-01, -8.9529e-01, -5.3607e-01, -4.8919e-01,\n",
      "           6.4479e-01, -9.4864e-01,  5.5957e-01,  2.4079e+00,  8.8747e-01,\n",
      "           1.0224e+00,  1.0419e+00,  1.2101e+00,  5.2714e-01, -4.0137e-01,\n",
      "           1.0893e+00, -2.0052e-01,  4.3297e-01, -1.1905e+00,  2.6737e-01,\n",
      "           4.8368e-01, -1.1252e+00,  6.6256e-01, -1.3863e+00,  1.8287e+00,\n",
      "           5.7068e-01, -6.5096e-01, -5.1585e-01, -1.3715e+00,  1.0448e+00,\n",
      "           6.3296e-01,  5.9884e-01,  7.3723e-01,  7.4637e-01,  7.8633e-01,\n",
      "           6.7183e-01,  6.2580e-01,  2.6209e-01,  7.7216e-01],\n",
      "         [ 5.1180e-01,  8.5355e-02, -1.3626e-01,  1.0833e+00, -1.1406e-01,\n",
      "           1.6511e-02,  1.9157e-01, -3.8157e-01, -1.2962e-01,  7.8760e-01,\n",
      "           2.1001e-01, -6.5514e-02, -7.5736e-01, -6.5463e-02, -2.7908e-01,\n",
      "           9.5229e-01, -6.2828e-02,  7.3850e-01,  7.5544e-01,  9.5466e-01,\n",
      "           7.8388e-01, -1.2363e-01, -2.3440e-01,  3.0850e-01, -8.6334e-01,\n",
      "          -4.4767e-01, -4.4683e-02,  4.5232e-01,  3.1699e-01,  9.8581e-02,\n",
      "          -2.9253e-01,  5.4494e-01,  3.9438e-01,  1.0850e+00,  3.6112e-01,\n",
      "           5.7307e-01,  1.0530e-01,  9.1952e-03,  5.9325e-01,  9.1733e-02,\n",
      "           4.0034e-01, -3.3655e-01, -3.5848e-01, -4.3931e-01,  7.5320e-01,\n",
      "          -5.9002e-01, -7.0159e-01,  5.7495e-01, -5.7571e-02,  1.2299e-01,\n",
      "          -4.8496e-01, -1.3095e+00,  1.0961e-01, -2.2844e-01,  1.2965e-01,\n",
      "          -2.9684e-01,  5.9890e-01,  3.7566e-01,  8.5354e-01, -2.6170e-01,\n",
      "          -4.6185e-01, -8.0012e-01,  1.9132e-01,  5.8593e-01],\n",
      "         [ 3.8408e-01, -6.7265e-01, -6.0777e-01,  3.3563e-01,  7.9514e-01,\n",
      "          -8.7835e-01, -7.4291e-01, -9.4731e-01,  9.8230e-02,  8.0626e-01,\n",
      "           1.0132e+00,  4.8901e-02,  1.7776e-02, -1.2967e-01, -3.8201e-01,\n",
      "           1.0919e+00, -6.0087e-01,  7.0824e-01,  2.8162e-01,  6.5471e-01,\n",
      "           2.2748e-01, -3.8894e-01,  5.4224e-01,  1.8234e-01, -4.5766e-01,\n",
      "          -9.1151e-01, -3.0838e-01,  3.6060e-01,  5.5580e-01,  3.8339e-01,\n",
      "          -7.6118e-02,  6.7968e-01,  7.2437e-01,  1.4706e+00, -4.7263e-01,\n",
      "          -5.6627e-02,  1.2035e-01,  1.2747e+00,  2.8520e-02,  4.8942e-01,\n",
      "           4.5158e-01, -9.0842e-01, -2.8505e-01,  1.7265e-01,  1.0691e+00,\n",
      "          -9.1971e-01, -8.2089e-01,  6.5030e-01, -2.0872e-01, -2.8337e-01,\n",
      "          -2.8478e-01, -1.3808e+00, -6.1258e-01, -6.2058e-01,  1.8133e-01,\n",
      "           5.1111e-01,  3.2739e-01, -5.7744e-01,  3.6790e-01, -3.8059e-01,\n",
      "          -4.0463e-01, -6.6826e-01, -1.1778e-01,  2.2892e-01]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "model_dim = 64    # Dimension of the model\n",
    "d_k = 16          # Dimension of the keys (and queries)\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "print(Attention(Q, K, V, torch.Tensor([d_k])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}