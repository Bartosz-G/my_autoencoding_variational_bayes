{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch.matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention1.set_mask(mask)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class AttentionIsAllYouNeed(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, number_of_encoder_stacks: int, number_of_decoder_stacks: int):\n",
    "        super(AttentionIsAllYouNeed, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "\n",
    "        encoder_list = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_encoder_stacks):\n",
    "            encoder_list.append(EncoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_list)\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_decoder_stacks):\n",
    "            self.decoder.append(DecoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.final_layer = nn.Linear(d_v, d_v)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        for layer in self.decoder:\n",
    "            layer.set_mask(mask)\n",
    "\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        X_encoded = self.encoder(X)\n",
    "        for stack in self.decoder:\n",
    "            Y = stack(Y, X_encoded, X_encoded)\n",
    "\n",
    "        Y_hat = self.final_layer(Y)\n",
    "\n",
    "        return Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[-7.7158e+36, -3.9808e+35, -6.3614e+36, -1.1052e+34,  1.9115e+36,\n",
      "          -1.5771e+34,  1.2227e+35, -6.0128e+36, -9.0464e+35,  1.0916e+37,\n",
      "           1.9311e+35, -2.3737e+37, -1.0969e+36, -2.4792e+34, -2.3777e+34,\n",
      "           2.8126e+36,  8.5865e+36, -7.0440e+36,  5.9270e+34, -1.1741e+35,\n",
      "           3.2193e+34,  1.0127e+34, -5.5693e+34, -2.9127e+35,  4.4485e+35,\n",
      "           2.9197e+32, -2.6269e+36, -5.2739e+36,  5.5094e+36,  2.5264e+35,\n",
      "          -2.7202e+35,  8.2020e+36, -1.2285e+35, -3.8584e+36, -1.1080e+37,\n",
      "          -7.2806e+36, -7.1910e+34,  6.5290e+36, -4.3211e+36, -1.9017e+36,\n",
      "          -2.5199e+36, -6.7443e+35, -1.1761e+37, -4.0528e+36, -3.9418e+34,\n",
      "          -1.2263e+34, -5.7671e+34,  1.2610e+37, -8.7630e+35,  2.4204e+36,\n",
      "          -4.2404e+36, -1.6847e+36,  2.8929e+36, -4.0092e+36,  4.4405e+34,\n",
      "          -9.0855e+36, -4.8206e+36, -6.7392e+36, -7.4730e+34, -3.2626e+36,\n",
      "          -2.3593e+35,  4.5027e+33,  5.1362e+36, -2.9267e+36],\n",
      "         [-5.4264e+36, -7.4911e+35, -1.3719e+37,  5.9406e+32,  2.4388e+36,\n",
      "          -3.4215e+32,  1.0772e+35, -7.0725e+36, -4.3989e+35,  1.2691e+37,\n",
      "           6.1230e+35, -3.3348e+37,  1.1113e+36,  3.4187e+34, -6.9586e+34,\n",
      "           3.7079e+36,  8.3953e+36, -4.9575e+36,  1.1139e+35, -1.2224e+35,\n",
      "           3.1362e+34,  4.2014e+34,  3.2543e+36, -2.8319e+35,  5.6679e+35,\n",
      "          -1.5114e+34,  4.4529e+36, -4.3752e+36, -7.5746e+35,  2.0789e+35,\n",
      "          -1.3944e+35,  8.5786e+36, -1.4778e+35, -4.3733e+36, -1.1384e+37,\n",
      "          -6.1570e+36, -6.7412e+34,  5.5746e+36, -1.1977e+36, -1.0401e+36,\n",
      "          -8.7727e+35, -1.2500e+36, -1.2080e+37, -5.2577e+36, -7.5651e+34,\n",
      "          -3.2303e+33, -9.2464e+34,  1.1590e+37, -1.4849e+36, -3.3493e+36,\n",
      "          -7.0258e+36,  3.1340e+36,  1.3045e+36, -4.8124e+36,  6.4086e+34,\n",
      "          -1.1385e+37, -4.7988e+36, -4.3404e+36,  2.5442e+34, -4.8816e+36,\n",
      "           1.5891e+36, -2.3537e+34,  9.6554e+36, -8.3139e+35],\n",
      "         [-7.9426e+36, -8.6059e+35, -1.2481e+37, -2.1460e+34,  5.5963e+36,\n",
      "          -3.1975e+33,  1.1643e+35, -8.3969e+36, -1.2851e+36,  1.7653e+37,\n",
      "           1.7933e+36, -3.4866e+37, -3.7200e+36,  3.9552e+34, -4.5207e+34,\n",
      "           4.0839e+36,  9.6268e+36, -7.4105e+36,  6.9964e+34, -1.2067e+35,\n",
      "           2.0148e+34, -7.7808e+32,  1.7654e+36, -4.0664e+35,  5.5255e+35,\n",
      "          -6.8483e+33,  2.6190e+36, -4.5847e+36, -1.0357e+37,  2.4556e+35,\n",
      "          -2.1411e+35,  7.5401e+36, -1.5788e+35, -4.3723e+36, -1.0898e+37,\n",
      "          -6.4334e+36, -4.6515e+34,  5.0073e+36, -2.8135e+36, -1.0678e+36,\n",
      "           1.1045e+36, -4.8964e+35, -1.1554e+37, -4.4417e+36, -2.9231e+34,\n",
      "           1.9715e+34, -1.0459e+35,  1.1431e+37,  4.1256e+35, -1.6718e+35,\n",
      "          -4.4870e+36,  8.1160e+35, -3.3985e+36, -3.3759e+36, -2.3336e+34,\n",
      "          -1.2631e+37, -6.8305e+36, -5.1432e+36,  4.9983e+34, -5.4701e+36,\n",
      "           1.3589e+36, -1.3857e+35,  7.3896e+36, -2.1080e+36],\n",
      "         [-6.7980e+36, -5.1083e+35, -9.3701e+36, -1.0635e+34,  6.1140e+36,\n",
      "          -1.3273e+34,  1.3852e+35, -5.3970e+36,  6.4524e+35,  1.4853e+37,\n",
      "           4.5688e+35, -2.6345e+37, -2.0354e+34, -2.4182e+34, -3.1336e+34,\n",
      "           2.7008e+36,  6.4869e+36, -6.1983e+36,  1.2044e+35, -1.1981e+35,\n",
      "           2.8127e+34,  3.8219e+34,  2.8134e+35, -3.1431e+35,  3.7618e+35,\n",
      "          -1.2222e+34, -4.3346e+35, -4.5508e+36, -6.5825e+36,  1.6658e+35,\n",
      "          -1.3381e+35,  7.0952e+36, -2.8053e+35, -7.0576e+36, -1.2616e+37,\n",
      "          -6.3359e+36, -1.3374e+35,  2.6474e+36, -2.5757e+36, -2.0220e+36,\n",
      "           1.9422e+36, -1.4723e+36, -1.3381e+37, -5.8232e+36, -8.9581e+34,\n",
      "          -8.6094e+34, -5.3183e+34,  1.0930e+37,  1.3244e+36,  2.8033e+36,\n",
      "          -6.7079e+36, -5.4938e+36, -2.7553e+35, -6.4382e+36, -4.3107e+34,\n",
      "          -9.6608e+36, -6.0662e+36, -7.2518e+36,  7.9693e+34, -6.8797e+36,\n",
      "          -1.2515e+36,  2.4595e+34,  4.4891e+36,  1.4715e+36],\n",
      "         [-7.0457e+36, -5.7458e+35, -7.4554e+36, -1.8400e+34,  5.4344e+36,\n",
      "          -1.7765e+34,  1.0113e+35, -8.1443e+36, -3.5838e+35,  1.6863e+37,\n",
      "           3.5233e+36, -2.7014e+37, -2.5435e+36, -9.0386e+33, -2.8157e+34,\n",
      "           4.8594e+36,  1.0161e+37, -6.4953e+36,  9.7811e+34, -1.1278e+35,\n",
      "           1.9217e+34,  1.3401e+34, -5.5176e+35, -3.8305e+35,  5.9701e+35,\n",
      "          -7.8246e+33,  6.8678e+34, -4.4254e+36, -1.1198e+37,  2.3727e+35,\n",
      "          -1.5579e+35,  6.9718e+36, -1.6361e+35, -4.5424e+36, -1.1496e+37,\n",
      "          -6.1395e+36, -5.3426e+34,  4.5705e+36, -1.9298e+36, -1.9442e+36,\n",
      "           2.2915e+36, -2.9594e+35, -1.2189e+37, -4.9176e+36, -1.7251e+34,\n",
      "           6.0808e+33, -7.2251e+34,  1.1628e+37, -2.7348e+35,  1.4302e+36,\n",
      "          -5.4470e+36, -5.1986e+36, -2.1898e+36, -4.1481e+36,  3.2751e+34,\n",
      "          -9.4444e+36, -6.1884e+36, -7.6321e+36,  9.5281e+34, -5.9469e+36,\n",
      "          -1.1351e+36, -9.2479e+34,  3.4380e+36, -1.4678e+36]],\n",
      "\n",
      "        [[-5.3644e+36, -3.6516e+35, -6.1031e+36, -1.1233e+34,  5.3079e+36,\n",
      "          -1.4861e+34,  1.0414e+35,  2.2876e+36, -4.8213e+36,  5.8571e+36,\n",
      "           6.7606e+36,  2.4183e+37, -2.2288e+36, -1.8707e+35,  1.3903e+35,\n",
      "           3.2744e+36, -1.2992e+36, -5.1526e+36, -8.5469e+34, -3.7581e+34,\n",
      "          -1.0251e+34, -1.1447e+35, -3.3382e+36,  1.4224e+35,  5.8690e+34,\n",
      "          -2.7846e+34, -8.3884e+36,  1.8152e+36,  2.0859e+37, -1.9101e+35,\n",
      "          -4.3158e+35,  8.7398e+36,  1.2263e+35,  2.5569e+36, -2.9031e+35,\n",
      "           2.5613e+36,  1.4526e+35,  8.7547e+36, -7.2931e+36,  2.3362e+36,\n",
      "           1.5339e+37,  4.4253e+35, -3.0593e+35, -4.8042e+36,  2.2072e+34,\n",
      "           1.1863e+35,  2.2572e+34,  4.2977e+36,  6.7106e+36,  1.1806e+37,\n",
      "          -2.6627e+36, -1.3006e+37, -2.5253e+37,  5.4262e+36, -6.4042e+35,\n",
      "          -8.9706e+36, -5.7977e+36,  1.5327e+36, -3.2335e+35,  7.2783e+34,\n",
      "          -1.0254e+37, -4.1105e+34,  1.5581e+36,  3.1784e+36],\n",
      "         [-5.7538e+36, -4.4395e+35, -5.6225e+36, -2.3800e+32,  3.6762e+36,\n",
      "          -8.6807e+33,  6.8233e+34,  3.4736e+36, -3.7928e+36,  2.9210e+36,\n",
      "           5.6111e+36,  1.4375e+37, -2.1138e+36, -1.7418e+35,  2.3025e+35,\n",
      "           4.0109e+36, -7.3022e+34, -5.4451e+36, -1.7065e+35, -3.6814e+34,\n",
      "          -1.4810e+34, -1.1442e+35, -7.2781e+36,  1.5485e+35,  2.2800e+34,\n",
      "          -2.8233e+34, -1.1862e+37,  1.9433e+36,  4.2277e+37, -1.6730e+35,\n",
      "          -5.8711e+35,  7.9301e+36,  2.8867e+35,  6.8871e+36, -1.3788e+36,\n",
      "           2.6455e+36,  2.0005e+34,  7.2942e+36, -1.3028e+37, -3.9001e+35,\n",
      "           1.5834e+37,  2.2085e+36, -1.4738e+36, -1.6848e+36,  1.3284e+35,\n",
      "          -1.8282e+34,  3.1503e+34,  4.2545e+36,  7.1866e+36,  1.5661e+37,\n",
      "          -8.4853e+36, -1.3602e+37, -2.2297e+37,  6.1695e+35, -6.9407e+35,\n",
      "          -4.6733e+36, -8.0157e+36,  1.1373e+36, -4.1645e+35, -1.7672e+36,\n",
      "          -1.2990e+37, -2.9748e+35, -4.6774e+35,  2.5334e+36],\n",
      "         [-5.0830e+36, -3.1615e+35, -2.7338e+36,  1.0885e+33,  3.8412e+36,\n",
      "          -1.7772e+34,  8.0663e+34,  3.5804e+36, -4.4018e+36,  2.4317e+36,\n",
      "           6.1485e+36,  6.8360e+36, -3.4588e+35, -1.6799e+35,  1.4188e+35,\n",
      "           7.8800e+36,  3.6577e+36, -4.8091e+36, -1.3054e+35, -2.6411e+34,\n",
      "          -6.8582e+33, -9.4277e+34, -6.2722e+35,  1.7564e+35,  3.2687e+35,\n",
      "          -4.5307e+34, -7.3911e+36,  2.3266e+36,  3.0650e+37, -1.6579e+35,\n",
      "          -5.2946e+35,  4.7025e+36,  1.7225e+35,  4.2434e+36, -4.9480e+36,\n",
      "           3.1824e+36, -7.4746e+34,  6.6433e+36, -1.3223e+37, -3.9389e+36,\n",
      "           2.0350e+37,  1.7499e+36, -5.2514e+36, -3.6316e+36,  1.0417e+35,\n",
      "          -1.0613e+35,  8.7450e+33, -1.5360e+36,  3.4640e+36,  9.2909e+36,\n",
      "          -6.7829e+36, -9.9995e+36, -1.9191e+37,  1.1682e+36, -3.5114e+35,\n",
      "          -3.0640e+36, -3.0882e+36,  3.3134e+35, -8.7598e+34,  5.5639e+35,\n",
      "          -1.1911e+37,  1.0207e+35,  5.8594e+36,  2.0630e+36],\n",
      "         [-5.4296e+36, -3.6938e+35, -4.6631e+36, -6.0612e+33,  4.9154e+36,\n",
      "          -1.8370e+34,  8.7121e+34,  1.5331e+36, -4.5646e+36,  4.1280e+36,\n",
      "           6.4261e+36, -4.4137e+36, -3.2385e+36, -1.5746e+35,  1.8277e+35,\n",
      "           3.8536e+36, -1.1403e+36, -5.1328e+36, -1.3344e+35, -3.0472e+34,\n",
      "          -9.8755e+33, -5.0800e+34, -9.3961e+35,  1.6140e+35,  2.7437e+34,\n",
      "          -3.1890e+34, -8.6336e+36,  1.6906e+36,  2.6210e+37, -2.0808e+35,\n",
      "          -4.8186e+35,  4.7864e+36,  1.6183e+35,  3.7831e+36, -2.3909e+36,\n",
      "           2.4014e+36, -3.0865e+33,  7.0820e+36, -1.0956e+37, -1.0878e+35,\n",
      "           1.6944e+37,  7.9464e+35, -2.5382e+36, -4.0398e+36,  4.4901e+34,\n",
      "          -3.6844e+34,  1.5499e+34,  1.7021e+36,  6.1629e+36,  1.4460e+37,\n",
      "          -6.4825e+36, -1.3972e+37, -2.3334e+37,  2.1876e+36, -5.9057e+35,\n",
      "          -5.4298e+36, -6.5865e+36,  8.5283e+35, -3.0009e+35,  3.0867e+35,\n",
      "          -1.0962e+37, -7.7680e+34,  3.8278e+36,  3.0268e+36],\n",
      "         [-5.8556e+36, -3.7557e+35, -7.2648e+36, -3.4145e+33,  4.9607e+36,\n",
      "          -1.6696e+34,  5.4127e+34,  2.5893e+36, -5.4355e+36,  3.8559e+36,\n",
      "           6.0668e+36,  1.0455e+36, -4.2182e+36, -1.6015e+35,  1.9195e+35,\n",
      "           3.7404e+36, -1.9394e+36, -5.4528e+36, -1.3487e+35, -3.6642e+34,\n",
      "          -1.2999e+34,  2.4778e+34,  1.9419e+36,  2.4263e+35,  2.8984e+34,\n",
      "          -5.2941e+34, -6.9017e+36,  1.4774e+36,  3.0701e+37, -2.3828e+35,\n",
      "          -5.5315e+35,  7.6000e+36,  1.8891e+35,  4.2231e+36, -9.4705e+35,\n",
      "           2.0616e+36,  3.8642e+34,  6.0696e+36, -1.1465e+37,  2.2297e+36,\n",
      "           1.9906e+37,  5.6302e+35, -1.0062e+36, -3.6301e+36,  3.1122e+34,\n",
      "           1.9675e+34, -1.7495e+34,  1.5693e+36,  5.0901e+36,  1.0216e+37,\n",
      "          -1.0267e+37, -1.6081e+37, -2.2237e+37,  2.7860e+36, -5.6967e+35,\n",
      "          -9.5150e+36, -7.3427e+36,  9.3358e+35, -3.9842e+35,  4.3465e+35,\n",
      "          -8.3249e+36,  1.8782e+35,  8.3412e+36,  2.5308e+36]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[-0.1039,  1.4438,  0.8375, -1.0493, -1.2741,  0.7297,  3.1908,\n",
      "           0.0864, -0.7791, -0.2108, -0.5236, -0.2370, -0.7821,  1.7480,\n",
      "           0.4905, -0.4368, -0.9548, -0.5537, -0.6005,  1.2184,  1.1913,\n",
      "          -0.6256, -0.7231, -0.3026, -2.4158,  0.4136,  2.2691,  1.7100,\n",
      "           0.5128,  0.3327, -1.1567, -0.2180,  0.6617,  0.1009, -0.3515,\n",
      "           0.4473, -0.9262,  0.3200, -0.8453, -0.0689,  0.5461, -0.5779,\n",
      "          -1.4740,  1.2302,  1.5161, -0.7276,  0.8295, -0.4684,  0.0144,\n",
      "           0.8289, -1.1465,  0.2345, -0.9876, -0.4537, -0.3000,  0.1688,\n",
      "          -1.3168,  0.3393, -0.6929,  1.1514, -1.6150,  0.7342, -0.7498,\n",
      "           0.3517],\n",
      "         [-0.7210, -0.6491,  0.1282,  0.1741,  1.3528,  0.1402, -0.5439,\n",
      "          -0.9692, -0.3829, -0.8711,  0.5084, -0.7831, -0.0687,  0.0108,\n",
      "          -0.9466, -0.3992,  0.0377, -1.8376,  0.2573, -0.2666, -0.6284,\n",
      "           0.8730, -0.4118,  0.7247, -0.1336,  0.1962,  1.6095,  0.0335,\n",
      "          -1.2034,  0.8293,  1.4636,  1.0941,  0.5563, -0.5794, -0.2455,\n",
      "          -0.4310,  0.8119,  1.6000,  1.4952, -1.5132,  0.7967, -1.2295,\n",
      "           1.3761,  1.9333,  1.2957, -0.6540,  1.2594,  0.4918,  0.9984,\n",
      "          -0.0847, -1.3142, -0.2574,  0.5229, -0.2868, -3.4833, -2.0870,\n",
      "           0.6457,  0.1545, -0.1970,  1.1291, -0.8270, -0.9254, -0.1174,\n",
      "           0.5487],\n",
      "         [-0.2131,  0.4102, -0.1463,  0.2263, -0.5101,  1.4985,  1.6396,\n",
      "           0.8081, -0.2602, -0.5064,  1.0912,  1.6107, -0.0230,  0.1629,\n",
      "           0.1693,  0.6218, -1.6007,  0.6437,  1.2953, -1.1432, -1.9120,\n",
      "           0.3048, -1.6289, -1.4858,  2.7320, -0.7000, -0.5843, -1.1971,\n",
      "           1.0293, -0.1644,  0.0487,  0.7602, -1.4382,  0.9936, -1.1229,\n",
      "          -0.6360,  1.1099, -0.2536, -0.2554,  0.0115, -1.0880, -1.1890,\n",
      "          -1.1688,  0.5015, -0.0214,  1.4538,  0.3495, -0.4320, -0.5874,\n",
      "           0.5776, -1.6744,  1.9795, -0.0436,  0.2713, -0.4018,  0.1918,\n",
      "           0.5172,  0.8833, -0.5066,  1.1930, -0.1425, -2.0224,  0.2274,\n",
      "          -0.2539],\n",
      "         [-0.8542,  0.9381,  0.6426,  0.7964, -0.9336,  2.3718,  1.2000,\n",
      "          -1.1325, -1.2884,  0.0187, -1.0925, -0.0484, -0.9408, -0.2771,\n",
      "          -0.0255,  2.6355, -1.6608,  0.9807, -0.4004, -0.5387,  0.2627,\n",
      "          -0.1897,  0.2540,  1.2257,  1.7034, -0.0968, -0.5131,  0.0095,\n",
      "          -0.8621,  1.9157, -0.7832,  0.4828, -0.3789,  0.6838, -0.1341,\n",
      "          -0.1216,  0.2642,  0.1530,  0.3733, -1.8730, -0.5297, -2.0941,\n",
      "          -1.2336,  1.7756,  0.2395,  1.2769, -0.3968,  0.1394,  0.2366,\n",
      "           0.5695, -1.9103, -0.5177, -0.6547,  0.4078,  0.3008,  1.0140,\n",
      "          -1.0298,  0.7402, -0.4098,  0.5138, -0.7755,  0.3671,  0.6951,\n",
      "          -1.4905],\n",
      "         [-1.3580,  1.2477, -1.3794, -1.5891, -0.0257,  0.6148,  0.7998,\n",
      "          -0.2755, -0.1454, -0.2200, -0.4129,  0.5747, -0.9049,  0.5041,\n",
      "          -2.5410,  0.5449,  0.5540,  0.7767,  0.8612, -1.2978, -0.5001,\n",
      "           0.3746, -0.1181, -1.1842,  1.0970, -1.7579,  1.3369,  0.2906,\n",
      "          -0.3735, -0.5531,  1.1789,  0.4201, -0.2818,  0.4423,  1.6596,\n",
      "          -1.4966, -1.7725,  2.7512,  0.7093,  1.2923, -0.6519, -0.4548,\n",
      "           0.9759,  0.8665, -0.2587, -0.1029,  0.2862,  0.4907, -0.5780,\n",
      "          -1.3314, -1.1932,  2.0301,  0.1794, -0.1171,  0.9550,  0.6022,\n",
      "          -1.0673,  0.2309, -0.4347, -0.4808,  0.3579,  0.8790, -0.9924,\n",
      "          -0.0336]],\n",
      "\n",
      "        [[ 1.6483, -0.9563, -1.0870, -0.3361, -0.7819, -0.9279, -0.0066,\n",
      "          -0.0096, -0.3377, -0.2455,  0.9971,  1.0053, -2.1255, -0.3619,\n",
      "           0.8151,  0.5622, -1.2737,  0.5903, -0.8267, -0.8388,  1.9866,\n",
      "          -1.9069,  0.1888, -1.1705, -0.6242,  0.3313, -0.4289, -0.5536,\n",
      "           0.4400, -1.3822,  1.8827, -0.5890,  2.0323, -0.6256,  0.5842,\n",
      "           1.6086, -0.5521, -0.0118, -0.0558, -0.4900,  0.5417,  1.3595,\n",
      "          -0.8880,  0.6731,  1.9111,  0.8101,  1.0982,  0.1105,  1.4123,\n",
      "          -0.9788,  0.4351, -0.5026, -1.5798, -0.3961, -0.6709,  0.6296,\n",
      "          -0.5368, -1.1698,  0.2120,  0.2763, -1.4566,  0.2654,  0.9213,\n",
      "           1.3560],\n",
      "         [ 0.3799,  0.1115, -0.2588,  1.2082, -1.8391, -0.1608, -0.1915,\n",
      "          -0.9973, -0.5208,  0.2958,  0.2963, -0.0922,  0.8422, -0.8981,\n",
      "          -0.5980, -1.3350, -0.0071,  0.0718,  0.0123,  0.2408,  0.0879,\n",
      "           1.7108,  0.2018,  1.1404, -1.1842,  1.2600,  0.4784,  0.0155,\n",
      "           0.0120,  0.2974, -0.4635,  0.0259, -0.9116,  2.0529, -0.8285,\n",
      "           1.7567, -0.5996,  0.7591, -2.1922,  0.7000,  1.4479,  2.2493,\n",
      "          -0.7490,  0.5590, -0.5111,  1.4032,  0.4078, -0.6666, -0.3529,\n",
      "           0.8878, -1.9012,  0.0823, -1.2587, -0.2306, -0.4945,  1.8316,\n",
      "          -2.2600,  0.3133, -1.5214, -0.0607, -1.2953,  0.0236,  0.2405,\n",
      "           0.9764],\n",
      "         [-0.6439, -0.8543,  0.6928, -0.1750, -0.0486,  0.6856,  0.3029,\n",
      "          -0.6718, -2.0535,  0.0751, -0.7366, -0.2239, -0.8638, -0.5764,\n",
      "          -0.3944,  1.8677, -0.9579,  0.3416,  0.2749, -1.6714,  0.6400,\n",
      "           0.9586,  1.6585, -0.6708,  0.0533,  0.0784, -0.1122,  0.3768,\n",
      "           1.4792,  0.3098, -1.2113,  0.8382, -0.4950,  1.0823, -0.2131,\n",
      "          -0.0120, -0.6472, -1.8402,  0.5950, -0.4065,  2.0841, -0.1851,\n",
      "          -0.9494,  1.3669, -1.9483,  1.2961,  2.0993, -0.3711,  0.8316,\n",
      "           1.8550, -1.4810, -2.1029, -0.2914,  0.9958, -0.2460,  0.3405,\n",
      "          -1.1820,  0.0999,  0.6176,  0.5255, -0.9683, -0.1339,  0.0422,\n",
      "           0.8741],\n",
      "         [-1.2700,  0.3891,  0.2752,  0.3105,  0.6979,  0.0734, -1.2247,\n",
      "           1.5147, -3.5589,  0.2494,  0.3804,  0.9224, -0.0810,  0.4111,\n",
      "           1.0949,  0.9867, -0.7529,  0.4788,  0.0881, -1.1136, -0.2013,\n",
      "           0.4950,  1.5000,  2.1980, -0.7075,  0.4666, -0.7748,  0.9235,\n",
      "          -0.6661,  0.7679, -0.0336, -0.3143, -0.8077, -0.7167,  0.7468,\n",
      "          -0.8622,  2.1656, -1.3473, -0.4223,  1.8930,  0.2198, -0.9665,\n",
      "          -0.6476,  0.9567, -0.7371, -1.3877,  1.0037,  0.0566,  1.2530,\n",
      "          -0.1601,  0.4768,  0.5796, -0.2700,  0.6431,  0.4451, -1.3356,\n",
      "          -1.2329, -0.3866,  0.6085,  0.2004, -0.8885, -1.4035, -0.2327,\n",
      "          -0.9687],\n",
      "         [ 1.6049, -0.4087,  0.2388, -0.1180, -1.1660,  0.0154,  0.0883,\n",
      "           0.6384, -1.3901,  0.9424,  0.2410,  0.5584,  0.6308, -0.7704,\n",
      "           0.8643,  0.7434,  0.6360,  1.1541, -1.3326, -0.5057, -0.5479,\n",
      "           0.9965,  0.7390,  0.8266,  1.3797,  1.5118, -0.9301,  0.3229,\n",
      "          -0.5821, -1.6426, -1.2463,  1.1334,  0.0348, -0.8025,  1.7523,\n",
      "          -1.4748, -1.9713,  0.4409, -1.3963,  0.2451,  0.9956,  2.2570,\n",
      "          -0.4503, -0.7023, -0.4100,  0.1257, -0.3045, -0.2193,  0.5822,\n",
      "          -0.8755, -0.8460, -0.2788, -0.7019, -0.2402,  1.3271,  1.2266,\n",
      "           1.4320, -1.4289,  0.2232, -0.4561, -0.4145, -2.5909,  0.6604,\n",
      "          -0.3647]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[-8.9488e-01,  7.7572e-01, -3.1857e-01, -2.8679e+00, -1.7455e+00,\n",
      "          -1.0111e-01,  2.3035e+00, -8.7628e-01,  1.1383e+00, -4.0371e-01,\n",
      "          -1.0498e+00, -1.4647e+00,  3.3917e-01,  8.8089e-01,  5.1263e-01,\n",
      "          -7.9509e-01, -1.4071e+00,  3.5050e-01, -4.3758e-01,  1.0316e+00,\n",
      "           1.2164e+00, -1.8801e-01, -4.6487e-01, -1.6847e-02, -2.7856e+00,\n",
      "          -3.2570e-01,  1.9901e+00,  1.3409e+00,  1.0290e+00,  9.2778e-01,\n",
      "          -1.4002e-01, -1.7413e-02,  7.0592e-01,  3.9062e-01,  6.2905e-02,\n",
      "          -2.4959e-03, -1.0263e+00,  6.0896e-01, -1.4153e+00,  6.8070e-01,\n",
      "           1.7097e+00,  4.2345e-01, -1.0255e+00, -1.4547e-01,  4.9664e-01,\n",
      "          -9.5736e-01,  2.5796e-01,  1.6593e-01, -2.4868e-01,  1.0797e+00,\n",
      "           3.7239e-01,  5.2953e-01, -4.2472e-01,  7.5374e-01,  4.4056e-01,\n",
      "           7.9163e-02, -2.0336e-01, -6.1342e-01,  5.7913e-01,  4.5169e-02,\n",
      "          -1.5017e+00,  1.2628e+00, -8.6726e-01,  2.5094e-01],\n",
      "         [-7.8617e-01, -6.0391e-01, -7.7439e-01, -1.5797e+00,  5.7869e-01,\n",
      "          -8.2847e-01, -7.7395e-01, -1.8679e+00,  8.0884e-01, -1.5467e+00,\n",
      "           3.7271e-01, -1.1785e+00,  9.5163e-01, -5.4147e-01, -1.2206e+00,\n",
      "          -6.0819e-01, -3.8811e-01, -4.3044e-01,  7.5962e-02,  7.1093e-02,\n",
      "          -3.4008e-01,  1.2740e+00, -4.1611e-01,  4.7761e-01, -1.1403e+00,\n",
      "          -4.0324e-01,  1.9603e+00, -3.6748e-01, -4.5020e-01,  1.2386e+00,\n",
      "           2.0179e+00,  5.6250e-01,  8.2115e-01,  2.7781e-01,  2.1902e-01,\n",
      "          -4.6823e-01,  6.0558e-01,  1.4996e+00,  9.1013e-01, -3.4374e-01,\n",
      "           1.9779e+00,  9.2984e-01,  1.0240e+00,  5.1958e-01,  1.8222e-01,\n",
      "          -5.9622e-01,  6.3767e-01,  1.4331e+00,  5.4745e-01,  7.4610e-02,\n",
      "           2.7329e-01, -1.2205e-01,  6.6206e-01,  6.4014e-01, -2.0625e+00,\n",
      "          -3.0064e+00,  1.1447e+00, -1.3919e+00,  9.1163e-01,  5.7230e-02,\n",
      "          -9.1375e-01, -3.4366e-01, -1.5405e-01, -8.9934e-02],\n",
      "         [-3.1667e-01,  4.1638e-01, -6.8981e-01, -1.0434e+00, -6.9054e-01,\n",
      "           4.0833e-01,  2.3965e-01, -6.6354e-02,  1.0805e+00, -1.7502e+00,\n",
      "           8.5157e-01,  1.1370e+00,  9.9221e-01, -2.1653e-02, -6.4076e-02,\n",
      "          -1.2720e-01, -2.3214e+00,  1.4953e+00,  1.3361e+00, -1.2585e+00,\n",
      "          -2.2707e+00,  7.7910e-01, -7.5324e-01, -1.5536e+00,  2.0208e+00,\n",
      "          -9.6102e-01,  2.4170e-01, -1.5076e+00,  7.9228e-01,  3.0906e-01,\n",
      "           6.4443e-01,  1.8692e-01, -1.0165e+00,  1.5527e+00, -9.4581e-01,\n",
      "          -3.7835e-01,  1.7439e+00,  1.0703e-01, -7.0499e-01,  7.7506e-01,\n",
      "           3.7076e-01,  6.7121e-01, -8.2487e-01, -4.9190e-01, -9.0625e-01,\n",
      "           1.2683e+00, -2.0740e-01,  6.6588e-01, -7.3419e-01,  5.7176e-01,\n",
      "          -6.0564e-01,  1.5380e+00, -3.4686e-01,  1.1134e+00,  4.6909e-01,\n",
      "          -1.1311e+00,  1.8174e+00, -4.1413e-01,  7.1529e-01, -1.3663e-01,\n",
      "          -8.2165e-02, -1.5195e+00, -2.8270e-01, -1.8606e-01],\n",
      "         [-1.4456e+00,  1.0686e+00,  3.4922e-01, -9.0469e-01, -1.5574e+00,\n",
      "           1.3948e+00,  4.6342e-01, -1.9910e+00,  7.0486e-02, -8.6019e-01,\n",
      "          -1.7113e+00, -9.3631e-01, -2.4975e-02, -4.2933e-01,  2.6150e-01,\n",
      "           1.9801e+00, -2.5593e+00,  1.8495e+00, -2.0802e-01, -6.2853e-01,\n",
      "          -1.2146e-01,  6.1305e-01,  6.7378e-01,  1.0529e+00,  2.6137e-01,\n",
      "          -7.2444e-02, -2.6840e-01, -3.5868e-01, -1.0737e+00,  2.6484e+00,\n",
      "           4.2810e-02,  5.2967e-01, -5.3831e-01,  1.5972e+00,  5.3153e-01,\n",
      "           6.4425e-01,  1.1709e-01,  3.3850e-01,  1.0680e-01, -1.0674e+00,\n",
      "           7.7328e-01,  6.7553e-01, -1.2958e+00,  6.6311e-01, -8.1733e-01,\n",
      "           4.5578e-01, -1.6869e+00,  9.3583e-01,  2.7673e-02,  8.7292e-01,\n",
      "          -6.5578e-01, -2.4783e-01, -8.9147e-01,  2.5846e-01,  1.0782e+00,\n",
      "           5.6932e-01,  3.7469e-01, -4.4444e-01,  9.0181e-01, -7.9689e-01,\n",
      "          -9.1703e-01,  1.0021e+00,  5.0574e-01, -1.1791e+00],\n",
      "         [-1.1500e+00,  1.0579e+00, -1.4134e+00, -2.6050e+00, -3.7327e-02,\n",
      "          -1.4976e+00, -3.4099e-01, -9.2179e-01,  6.2654e-01, -9.0506e-01,\n",
      "          -6.1757e-01, -1.6184e-01,  2.7619e-01, -1.9209e-01, -1.5074e+00,\n",
      "          -2.1899e-01, -2.1334e-01,  1.6375e+00,  5.8072e-01, -8.9191e-01,\n",
      "          -8.9793e-01,  8.8886e-01,  4.9991e-01, -7.7216e-01, -3.8654e-01,\n",
      "          -1.9347e+00,  1.3810e+00,  4.5587e-01, -5.1693e-01, -2.1691e-01,\n",
      "           1.2213e+00, -4.0909e-02,  7.0441e-01,  6.3643e-01,  1.8965e+00,\n",
      "          -4.9433e-01, -5.2961e-01,  2.6390e+00, -1.0932e-01,  1.5425e+00,\n",
      "           8.8620e-01,  9.5252e-01,  1.0820e+00, -3.0133e-01, -1.2556e+00,\n",
      "          -2.5024e-01, -1.2327e-01,  9.1721e-01, -5.0782e-01, -8.5954e-01,\n",
      "           7.1314e-01,  1.6559e+00,  1.7615e-01,  6.4186e-01,  8.8944e-01,\n",
      "          -4.3299e-01,  1.9149e-01, -1.1685e+00,  6.8835e-01, -1.3783e+00,\n",
      "           7.8707e-02,  9.6404e-01, -1.0872e+00,  5.6879e-02]],\n",
      "\n",
      "        [[ 1.0190e+00,  2.5403e-02, -1.8626e+00,  4.0373e-01, -1.3642e+00,\n",
      "          -1.0633e+00,  1.1077e+00,  2.1422e-01,  1.9645e+00,  8.0253e-02,\n",
      "           9.5749e-01,  6.9115e-01, -1.0939e+00,  3.7229e-01, -1.7610e-02,\n",
      "           2.2211e-01, -3.4741e-01,  1.2845e-01, -2.7662e-01, -1.1064e+00,\n",
      "           1.0942e+00, -2.4250e+00,  7.6795e-01, -1.5103e+00, -9.7756e-01,\n",
      "           5.0450e-02, -4.9179e-01, -1.3827e+00,  8.8802e-01, -9.0108e-01,\n",
      "           2.1142e+00, -2.3872e-01,  1.8764e+00, -2.9985e-02,  1.6296e+00,\n",
      "           1.2546e+00,  6.5499e-01,  1.0341e+00,  2.6169e-02,  6.1951e-03,\n",
      "          -2.0277e-02, -3.1221e-01, -9.8179e-01, -3.9835e-01,  1.6423e+00,\n",
      "          -4.5018e-01,  8.0969e-01,  8.6257e-02,  6.2025e-01, -1.1599e+00,\n",
      "           3.9917e-01, -2.5145e-01, -1.4510e+00, -2.8336e-01, -3.7994e-02,\n",
      "           9.2721e-01,  1.0244e+00, -1.9549e+00,  3.1128e-01, -6.2654e-01,\n",
      "          -1.7226e+00,  6.2402e-01, -1.5796e-01, -1.3016e-01],\n",
      "         [-7.9936e-01,  1.0106e+00, -8.0998e-01,  1.0074e+00, -1.7632e+00,\n",
      "           4.2553e-01,  1.1352e+00,  3.5143e-01,  3.0789e+00,  1.5981e+00,\n",
      "          -5.1130e-01, -3.5417e-01,  8.5300e-01, -2.4001e-01, -1.2204e+00,\n",
      "          -9.9980e-01,  4.0372e-01,  1.7138e-01,  5.7681e-01, -2.9983e-01,\n",
      "          -3.7581e-01,  3.2196e-01,  9.2626e-01, -8.5765e-02, -8.6561e-01,\n",
      "           5.8488e-01, -2.4931e-01, -8.2399e-01, -1.3838e-01, -1.8203e-01,\n",
      "           1.3545e-01,  5.9694e-01,  4.2124e-01,  2.3540e+00,  2.7222e-01,\n",
      "           1.6501e+00, -5.6836e-03,  1.3354e+00, -2.4445e+00,  1.0035e+00,\n",
      "           7.9755e-01,  4.3462e-01, -2.3646e-01, -7.0643e-01, -1.1128e+00,\n",
      "           3.4867e-01, -8.8267e-02, -2.8115e-01, -1.5620e+00,  3.5654e-01,\n",
      "          -1.4172e+00,  7.7650e-01, -1.5626e+00, -4.0530e-01,  8.8359e-02,\n",
      "           1.6392e+00,  6.1890e-02, -7.3054e-01, -1.3843e+00, -1.0087e+00,\n",
      "          -1.0280e+00, -1.7997e-02, -1.0797e+00,  7.3119e-02],\n",
      "         [-1.0163e+00,  5.4997e-01,  7.5503e-01,  1.0315e+00, -2.3401e-01,\n",
      "           1.1794e+00,  1.2443e+00,  6.7136e-01,  1.5518e+00,  1.0620e+00,\n",
      "          -1.1379e+00, -5.9099e-01, -3.2138e-01, -2.3970e-01, -1.0408e+00,\n",
      "           1.5592e+00, -6.6223e-01,  1.7385e-01, -5.5437e-03, -1.7616e+00,\n",
      "          -1.3058e-02, -1.7306e-01,  1.9626e+00, -1.4578e+00,  3.0679e-01,\n",
      "          -2.5287e-01, -5.7101e-01, -3.4067e-01,  1.0708e+00,  3.6678e-02,\n",
      "          -1.0364e+00,  1.0789e+00, -7.6590e-01,  1.4023e+00,  4.4491e-01,\n",
      "          -2.3206e-01, -8.0274e-01,  6.3007e-01,  1.4110e+00,  7.3179e-01,\n",
      "           2.7037e+00,  4.0129e-01, -9.3162e-01, -2.0038e-01, -2.7725e+00,\n",
      "           2.6379e-02,  1.0806e+00, -3.1608e-01, -6.5287e-01,  9.9141e-01,\n",
      "          -1.4175e+00, -1.0351e+00, -9.1555e-01, -7.7344e-03, -2.1168e-01,\n",
      "           5.2897e-01,  5.3361e-01, -1.3529e+00,  8.0892e-02, -8.0804e-02,\n",
      "          -1.4042e+00, -9.6898e-02, -1.2940e+00,  1.4481e-01],\n",
      "         [-1.1225e+00,  1.3234e+00, -2.2314e-01,  3.6992e-01, -1.2437e-01,\n",
      "           9.4377e-01,  8.5389e-01,  2.9655e+00,  1.3675e-01,  8.7186e-01,\n",
      "          -4.3635e-01,  1.5898e-01,  3.2762e-02,  1.0047e+00, -3.6089e-02,\n",
      "           7.6016e-01, -3.2872e-01, -3.7257e-02,  1.9354e-01, -1.3811e+00,\n",
      "          -4.8884e-01, -3.2068e-01,  1.2307e+00,  1.3148e+00, -6.9493e-01,\n",
      "          -1.9813e-01, -6.8181e-01, -1.5388e-01, -5.0187e-01,  5.0765e-01,\n",
      "           2.9151e-01, -2.1116e-01,  1.4776e-01, -1.1861e-01,  2.1885e+00,\n",
      "          -1.1376e+00,  2.0909e+00,  1.9809e-01, -3.2956e-01,  2.6201e+00,\n",
      "           5.6500e-01, -1.5646e+00, -7.9498e-01, -3.6907e-01, -1.3918e+00,\n",
      "          -1.6153e+00,  3.4428e-01, -3.1695e-02,  1.9991e-01, -5.1087e-01,\n",
      "           3.6034e-01,  8.8818e-01, -1.9412e-01,  8.2446e-03,  8.9841e-01,\n",
      "          -1.0302e+00,  3.9407e-01, -1.7927e+00,  1.7745e-01, -6.9874e-01,\n",
      "          -1.1768e+00, -1.5073e+00, -1.1947e+00, -1.6416e+00],\n",
      "         [ 6.7861e-01, -8.3997e-02, -4.1738e-01,  1.5002e-01, -1.5512e+00,\n",
      "          -3.7425e-01,  9.5810e-01,  1.2880e+00,  1.2409e+00,  6.0075e-01,\n",
      "           3.5463e-02,  1.1755e-01,  1.0767e+00,  2.2574e-01, -2.5089e-01,\n",
      "           2.4601e-01,  1.0367e+00,  8.7753e-01, -6.3036e-01, -9.1541e-01,\n",
      "          -6.5790e-01,  1.7712e-01,  1.3417e+00,  5.0457e-01,  1.4391e+00,\n",
      "           1.1202e+00, -1.3989e+00, -6.7728e-01, -8.1503e-01, -1.4661e+00,\n",
      "          -9.6325e-01,  1.0867e+00,  6.8380e-01,  2.4830e-01,  1.8096e+00,\n",
      "          -6.6896e-01, -1.2394e+00,  1.6642e+00, -9.3389e-01,  9.2825e-01,\n",
      "           9.9813e-01,  1.3504e+00, -4.5986e-01, -1.2595e+00, -8.1479e-01,\n",
      "          -6.3182e-01, -6.2715e-01,  5.5789e-01, -4.8155e-01, -5.1343e-01,\n",
      "          -1.8001e-01, -2.3512e-01, -7.1367e-01, -6.2234e-01,  7.1003e-01,\n",
      "           1.6225e+00,  2.2429e+00, -2.0759e+00, -9.0007e-02, -7.2691e-01,\n",
      "          -2.9707e-01, -2.2990e+00, -6.0802e-01, -1.3370e+00]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[ 0.3942, -1.6487,  0.4268,  0.8895,  0.9078, -0.1566,  0.3509,\n",
      "           0.3038, -0.4853, -0.5535, -0.0410, -0.6531, -0.7741,  0.2140,\n",
      "           0.1809,  0.4186,  0.3990,  0.2221,  0.6776,  0.2289,  0.2376,\n",
      "           0.4875,  0.7917,  0.1212, -0.4958,  0.6184, -0.0243,  0.8819,\n",
      "           0.0215,  0.8609,  0.4352, -1.9928, -0.1582,  1.0375, -0.6470,\n",
      "          -0.5815,  0.3517,  0.6750,  0.2117,  0.8613, -0.6931,  0.1117,\n",
      "           0.6397,  0.0764, -0.5994,  1.1049,  0.2024,  0.4491, -0.0975,\n",
      "           0.4945,  1.1976, -0.5432, -0.2462, -0.3191,  0.5830,  0.7496,\n",
      "           0.0413,  0.1388,  0.4894, -0.7450,  0.2143,  0.6303,  0.8272,\n",
      "          -0.8080],\n",
      "         [ 0.1982, -0.0075,  0.7543, -0.4238, -0.4128, -0.0271, -0.4509,\n",
      "           0.4221, -1.2759, -0.3138,  0.2374, -0.1568, -0.7380,  0.6623,\n",
      "          -0.0675, -0.6450,  0.4892, -0.6939, -0.3818, -0.6615,  0.5972,\n",
      "          -0.9151,  1.3200, -0.0820,  1.2155, -0.7662,  0.3730, -0.4824,\n",
      "          -0.1689, -1.0751,  0.5776, -0.5337, -0.5085,  1.4648, -0.7612,\n",
      "           0.1642,  0.6030,  1.1867, -0.5870, -0.9415,  0.4163,  0.4188,\n",
      "          -0.2607, -0.0101,  0.1296, -0.5587,  0.4015, -0.5446,  0.7182,\n",
      "          -0.4743, -0.0518,  0.4190,  0.6185,  0.1203, -0.3603, -0.5831,\n",
      "          -0.0747, -1.1224,  0.7902,  0.5053, -0.3958,  0.0511,  0.7928,\n",
      "          -1.4691],\n",
      "         [ 0.4371, -0.1075,  0.0595,  0.3259, -0.6893,  0.6287, -0.2991,\n",
      "           0.5159, -0.5449, -0.1845, -0.2887,  0.0046,  0.5639,  0.0231,\n",
      "           1.2720,  1.0271, -0.1401,  0.3949,  0.7105, -0.0526, -0.1031,\n",
      "          -0.5682,  0.0563,  0.3690,  0.8744, -0.1212, -0.1625, -1.3532,\n",
      "           0.5687, -0.1900,  0.3288, -0.4991, -0.4763,  0.2214, -0.4002,\n",
      "          -0.1623, -0.1637, -0.1006, -0.5895, -0.1742,  0.3161, -0.8449,\n",
      "           0.7142,  0.5659,  0.0793, -0.1090, -0.2050, -0.0816,  0.1454,\n",
      "          -0.2460,  0.4210, -0.5988,  0.5338, -1.0977,  0.5250,  0.0891,\n",
      "          -0.3666, -0.1372,  0.0166, -1.8042, -0.3655, -0.1422,  0.3652,\n",
      "           0.1697],\n",
      "         [ 0.4597,  0.6210, -1.4333,  1.2824, -0.6104, -0.3161,  0.3147,\n",
      "           1.2271,  0.8008,  0.1470, -0.7387, -0.2626, -0.1800, -0.3136,\n",
      "           0.3572,  0.0405, -0.3414, -0.1820,  0.1685, -0.0740, -0.0989,\n",
      "          -0.2383,  0.2222, -0.8649,  0.8576,  0.7961,  0.5112, -0.5234,\n",
      "           0.4417, -0.2936, -0.8986,  0.1424, -0.5363, -0.6890,  0.3823,\n",
      "           0.6228, -0.4879, -0.7161, -0.5065, -0.7202,  0.1822,  0.0093,\n",
      "          -0.0984,  0.7575,  0.1254,  0.3025, -0.3068, -0.5909, -1.5147,\n",
      "          -0.6618, -0.2201,  0.6304, -0.7020, -0.8699,  0.7478,  0.7086,\n",
      "           0.5245,  0.0030,  0.1560, -0.1158, -1.1760, -0.1988,  0.2022,\n",
      "           0.3709],\n",
      "         [ 0.5385,  0.6596,  0.4373,  0.0702,  0.4881,  0.3567, -0.0342,\n",
      "          -0.0305, -0.0859, -0.2349,  1.2063,  0.9418, -0.7397,  0.2897,\n",
      "          -0.1195,  0.5385,  0.4108,  0.1195,  0.1709, -0.1931, -0.1590,\n",
      "           0.7264,  0.0212, -0.2367, -0.1380,  0.7025, -0.8573, -0.8326,\n",
      "           0.3393,  0.3842,  0.2559,  0.8073, -0.0669, -1.0116, -0.7963,\n",
      "           0.2183,  0.3062, -1.4795, -0.1647, -0.1568,  0.1950,  0.8738,\n",
      "          -0.1853, -0.2100,  0.1823, -0.4661,  0.1304, -0.5971, -0.6679,\n",
      "           0.1061,  0.3740,  0.1189,  0.1250,  0.0758,  0.9344, -1.2386,\n",
      "          -0.7985, -0.1383, -0.2554, -1.1804,  0.5948, -1.5101, -0.2061,\n",
      "           0.6510]],\n",
      "\n",
      "        [[ 0.8422, -0.6607,  0.2010, -0.9054, -0.0625,  1.4290, -0.4885,\n",
      "           0.0713,  0.0214, -0.0553,  0.1532,  0.3800, -0.6065,  0.1429,\n",
      "           0.8372,  0.6743,  0.2223, -0.6521,  0.4109,  1.0609, -0.1313,\n",
      "           1.2225, -0.6572,  0.3862, -1.2261, -0.0630, -0.7028,  0.0635,\n",
      "           0.0139,  1.4590,  0.8208,  0.6749,  0.3387,  0.1704,  0.2705,\n",
      "           0.0284, -0.4562, -0.5632,  1.3264,  0.4177,  0.0547,  0.3598,\n",
      "           0.9349, -0.1398, -0.3287, -0.2580, -0.6013,  0.9811, -0.3901,\n",
      "          -0.4699,  0.6473, -0.5287,  0.3238, -0.3499, -0.1345,  0.0434,\n",
      "           0.2573,  0.2140,  0.3142, -0.0550, -0.0059, -0.1920, -0.0954,\n",
      "           1.8003],\n",
      "         [-0.9201,  0.8861, -1.0141, -0.4345, -0.8522, -0.2367, -0.2244,\n",
      "          -0.3625,  0.1843, -0.4765, -0.2619,  0.4044,  0.8020, -0.1707,\n",
      "          -0.2725, -0.4854, -0.3995,  1.0662,  0.6533, -0.2689, -0.5061,\n",
      "           0.3664,  0.3913,  0.7430,  0.4529,  0.0434,  0.6317, -0.3206,\n",
      "          -1.0082, -0.3348,  1.0540,  1.0264,  0.6812, -0.6391, -0.0225,\n",
      "           0.5483,  0.0169, -1.7983, -0.3537, -0.7464,  0.8661, -1.0589,\n",
      "           0.1603,  1.1528, -0.0945, -0.0555, -0.2142,  1.2148, -0.0123,\n",
      "           0.0319, -0.4980, -0.2136, -0.1328,  1.0018,  0.4611, -1.5389,\n",
      "           0.3138, -0.2182, -1.0850,  0.3697,  1.1611, -1.0351, -0.5497,\n",
      "          -0.2622],\n",
      "         [-1.3416,  0.6702,  0.4087, -0.2242, -0.4866, -0.5143,  0.6118,\n",
      "           0.0758, -0.3328,  0.4659,  0.3577,  0.5211, -0.0694,  0.1318,\n",
      "          -0.1107,  0.4180, -0.4509,  1.0176,  0.3193, -0.3688, -0.2964,\n",
      "           0.1653,  0.2265,  0.2235, -0.5550, -0.2594,  0.2347, -0.8274,\n",
      "           0.2049,  0.2586, -0.0338, -0.0313,  0.6635, -0.2378, -0.2003,\n",
      "           0.0993,  0.1891, -0.6329,  0.4968, -0.2742,  0.6866, -0.6692,\n",
      "           1.1524,  0.8353,  0.1864, -0.1017, -1.0366, -0.2873, -0.6823,\n",
      "          -1.3463,  0.3655, -0.4064, -0.1298, -0.0493, -0.2075, -0.5879,\n",
      "           0.3942, -1.0494, -0.2276,  0.5083, -0.4142, -0.5773, -1.6637,\n",
      "           0.7947],\n",
      "         [ 0.4854,  0.3626, -0.4271, -0.3009,  0.4521, -0.1598, -0.0814,\n",
      "          -0.5039, -0.3409,  0.5367, -0.0906,  0.2666,  0.0325,  0.7169,\n",
      "          -0.3961, -0.0253,  0.5922, -0.3173,  0.0930, -0.7682,  0.0378,\n",
      "           1.3112,  0.5569,  0.4965,  0.0605, -0.2073,  0.4176,  0.5277,\n",
      "          -0.3603, -0.5307,  0.4934, -0.1131, -0.0297,  0.0931, -1.0763,\n",
      "           1.0328,  0.8488,  0.0224, -0.3873, -0.6538,  0.3878,  0.5371,\n",
      "          -0.8229,  0.8343,  0.0804, -0.7417,  0.0146,  0.0681,  0.6188,\n",
      "           0.0529,  0.4238,  0.0696,  0.0089,  0.6095,  0.1109, -0.2609,\n",
      "          -0.4930, -0.0962, -0.4091,  0.9401, -0.1207, -0.7983, -0.1118,\n",
      "           0.0632],\n",
      "         [-0.0182,  1.4065,  0.0582, -0.5953, -0.0365, -0.1232,  0.0534,\n",
      "          -0.3899, -0.2935,  0.4806,  0.2842,  0.3878, -0.3536,  0.7466,\n",
      "           0.7862, -0.5966,  0.6935, -0.4757, -1.0912, -0.3240, -0.1737,\n",
      "          -0.5826,  0.5119,  0.1454,  0.0667,  0.4840,  0.5460,  0.0271,\n",
      "           0.4657,  0.2087,  1.2291,  1.1895, -1.2624, -0.0739,  0.3924,\n",
      "          -0.3207,  0.6162, -1.1478,  1.3228, -0.1970, -0.1530, -0.1750,\n",
      "          -0.4605,  0.5652, -0.2167, -0.0100, -1.5282,  0.2972, -1.5133,\n",
      "          -0.1610,  0.4801,  0.3441, -0.4200, -0.3083,  0.0211, -0.2900,\n",
      "           0.1757, -0.3047,  0.6263, -0.4318,  0.4373,  0.5157, -0.1626,\n",
      "           0.1063]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)\n",
    "Y = torch.randn(batch_size, seq_length, d_k)\n",
    "\n",
    "attention_is_all_you_need = AttentionIsAllYouNeed(model_dim, d_k, model_dim, h, 1, 1)\n",
    "output = attention_is_all_you_need(X, Y)\n",
    "print(output.shape)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}