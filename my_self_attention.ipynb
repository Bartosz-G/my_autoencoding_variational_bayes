{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, mask = None):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = mask\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, Q, K, V, d_k):\n",
    "        QK_T = torch.matmul(Q, torch.transpose(K, -1, -2))\n",
    "        QK_T_d_k = torch.div(QK_T, torch.sqrt(d_k))\n",
    "\n",
    "        if self.mask is not None:\n",
    "            QK_T_d_k += self.mask\n",
    "\n",
    "        softmax = F.softmax(QK_T_d_k, dim = -1)\n",
    "        return torch.matmul(softmax, V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k_value = torch.Tensor([d_k])\n",
    "        self.linear = nn.ModuleList()\n",
    "        self.W_O = nn.Parameter(torch.Tensor(h*d_v, d_model))\n",
    "        self.attention = Attention(mask)\n",
    "\n",
    "        for _ in range(self.h):\n",
    "            linear = nn.ModuleList([nn.Linear(d_k, d_model), nn.Linear(d_k, d_model), nn.Linear(d_v, d_model)])\n",
    "            self.linear.append(linear)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.attention.set_mask(mask)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor):\n",
    "        assert len(Q.shape) == len(K.shape) == len(V.shape), f\"invalid dimensions, got Q:{Q.shape}, K: {K.shape}, V:{V.shape}\"\n",
    "\n",
    "        heads = [self.attention(layer[0](Q), layer[1](K), layer[2](V), self.d_k_value) for layer in self.linear]\n",
    "        concat_heads = torch.cat(heads, dim = -1)\n",
    "        return torch.matmul(concat_heads, self.W_O)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, intermediate_features = None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if intermediate_features is None:\n",
    "            self.intermediate_features = in_features*4\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, self.intermediate_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.intermediate_features, out_features)\n",
    "        )\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int):\n",
    "        super(EncoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sublayer_1_output = self.multi_head_attention(X, X, X)\n",
    "        sublayer_1_normalised = self.layer_norm1(X + sublayer_1_output)\n",
    "        sublayer_2_output = self.position_wise_feed_forward(sublayer_1_normalised)\n",
    "        output = self.layer_norm2(sublayer_1_normalised + sublayer_2_output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, mask = None):\n",
    "        super(DecoderStack, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(d_model, d_k, d_v, h, mask)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.multi_head_attention2 = MultiHeadAttention(d_model, d_k, d_v, h)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.position_wise_feed_forward = PositionWiseFeedForward(d_v, d_v)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.multi_head_attention1.set_mask(mask)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def forward(self, X, Q, K):\n",
    "\n",
    "        V = self.multi_head_attention1(X, X, X)\n",
    "        V_norm = self.layer_norm1(X + V)\n",
    "        sublayer_2_output = self.multi_head_attention2(Q, K, V_norm)\n",
    "        sublayer_2_normalised = self.layer_norm2(V_norm + sublayer_2_output)\n",
    "        sublayer_3_output = self.position_wise_feed_forward(sublayer_2_normalised)\n",
    "        output = self.layer_norm3(sublayer_2_normalised + sublayer_3_output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class AttentionIsAllYouNeed(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int, d_v: int, h: int, number_of_encoder_stacks: int, number_of_decoder_stacks: int):\n",
    "        super(AttentionIsAllYouNeed, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "\n",
    "        self.positional_encoding = None # Implement\n",
    "\n",
    "        encoder_list = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_encoder_stacks):\n",
    "            encoder_list.append(EncoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_list)\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        for _ in range(number_of_decoder_stacks):\n",
    "            self.decoder.append(DecoderStack(d_model, d_k, d_v, h))\n",
    "\n",
    "        self.final_layer = nn.Linear(d_v, d_v)\n",
    "\n",
    "    def init_weights(self, init_fn):\n",
    "        self.apply(init_fn)\n",
    "\n",
    "    def embedding(self):\n",
    "        pass # Implement\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        for layer in self.decoder:\n",
    "            layer.set_mask(mask)\n",
    "\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        X_encoded = self.encoder(X)\n",
    "        for stack in self.decoder:\n",
    "            Y = stack(Y, X_encoded, X_encoded)\n",
    "\n",
    "        Y_hat = self.final_layer(Y)\n",
    "\n",
    "        return Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "glove_vectors = GloVe(name='6B', dim=100)\n",
    "\n",
    "start_token = \"</s>\"\n",
    "start_index = 0\n",
    "\n",
    "glove_vocab = vocab(glove_vectors.stoi)\n",
    "glove_vocab.insert_token(start_token, start_index)\n",
    "glove_vocab.set_default_index(start_index)\n",
    "pretrained_embeddings = glove_vectors.vectors\n",
    "pretrained_embeddings = torch.cat((torch.zeros(1,pretrained_embeddings.shape[1]),pretrained_embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([400001, 100])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "487"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vocab[\"november\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0-7): 8 x ModuleList(\n    (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(512, 512, 512, 8)\n",
    "multi_head_attention.linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([2, 5, 64])\n",
      "Shape of K: torch.Size([2, 5, 64])\n",
      "Shape of V: torch.Size([2, 5, 64])\n",
      "Shape of output:torch.Size([2, 5, 64])\n",
      "tensor([[[        nan,  4.0096e+37,  6.2351e+37, -4.2478e+36, -2.0465e+37,\n",
      "          -4.1833e+36,         nan,         nan, -7.8783e+36,  1.0117e+37,\n",
      "          -9.4423e+35,  2.0174e+36, -5.8751e+36, -3.1546e+36, -3.0629e+36,\n",
      "          -1.6364e+37, -1.8854e+36, -5.5364e+35,  3.2821e+36, -3.7999e+36,\n",
      "          -8.9872e+35, -2.5219e+36,  3.8568e+36, -5.8303e+36,  1.0008e+37,\n",
      "           4.1014e+35, -5.0816e+36, -1.4643e+35, -4.0584e+37, -1.0357e+38,\n",
      "                  nan, -3.9588e+36,  1.1894e+36,  9.2627e+36,         nan,\n",
      "           3.2849e+36, -2.2424e+36,  6.2773e+36,  6.5724e+36, -2.8173e+36,\n",
      "          -1.7648e+37, -4.5027e+37,  9.5633e+35, -3.9691e+37,  2.6361e+36,\n",
      "           6.8147e+36,  2.2129e+37,  4.7346e+36,  3.1635e+36,  1.1845e+36,\n",
      "          -9.1324e+36,         nan,  3.2725e+37, -2.1265e+37, -2.5488e+37,\n",
      "                  nan, -1.6300e+37, -9.5616e+36,         nan, -1.2026e+37,\n",
      "          -3.9547e+36, -6.9687e+36,         nan, -1.0574e+37],\n",
      "         [        nan,  4.5371e+37,  4.7310e+37,  5.4921e+35, -1.1456e+37,\n",
      "          -5.6767e+36,         nan,         nan, -1.7323e+37,  1.1040e+37,\n",
      "          -7.2874e+35,  2.3083e+35, -3.2761e+36, -1.7506e+36, -2.2463e+36,\n",
      "          -1.1745e+37, -2.2174e+36, -1.2140e+36,  3.5099e+36, -4.3533e+36,\n",
      "          -1.3467e+36, -2.7611e+36,  6.2095e+36, -5.3817e+36,  8.0789e+36,\n",
      "           1.7980e+36, -1.5847e+37, -7.4729e+35, -2.5404e+37, -7.9355e+37,\n",
      "                  nan, -8.5253e+35,  1.1945e+37,  6.3805e+36,         nan,\n",
      "          -4.4083e+35, -2.8747e+36,  6.0402e+36, -2.0758e+37, -2.3559e+36,\n",
      "          -1.8084e+37, -5.2724e+37,  1.5476e+36, -2.5422e+37,  1.9936e+36,\n",
      "           4.9472e+36,  2.3681e+37,  3.7612e+36,  2.3868e+36,  1.3099e+36,\n",
      "          -1.7271e+37,         nan,  3.9787e+37, -2.3462e+37, -3.1127e+37,\n",
      "                  nan, -7.4143e+36, -5.7982e+36,         nan, -9.0302e+36,\n",
      "          -2.3564e+36, -2.4159e+37,         nan, -1.0047e+37],\n",
      "         [        nan,  4.9118e+37,  2.8799e+37, -2.3690e+36, -1.3319e+37,\n",
      "          -6.3740e+36,         nan,         nan, -1.2797e+37,  1.8175e+37,\n",
      "           6.8685e+35,  1.9020e+36, -4.2108e+36, -4.3006e+35, -2.8865e+36,\n",
      "          -1.1824e+37, -1.8071e+36, -5.8824e+35,  7.4211e+36, -2.5073e+36,\n",
      "          -2.7051e+35, -3.2702e+36,  3.5792e+36, -5.2558e+36,  9.0414e+36,\n",
      "           3.1522e+36, -1.2399e+37, -1.6784e+35, -3.4883e+37, -4.8289e+37,\n",
      "                  nan, -3.7508e+35, -4.6989e+36,  5.9225e+36,         nan,\n",
      "           4.4778e+36, -3.5246e+36,  5.1440e+36, -3.7284e+37, -1.4928e+36,\n",
      "          -1.9490e+37, -2.9684e+37,  2.5439e+36, -3.7528e+37,  2.1151e+36,\n",
      "           4.8459e+36,  3.1743e+37,  5.6058e+36,  2.5647e+36,  1.6227e+36,\n",
      "          -2.3841e+37,         nan,  7.2269e+36, -2.1338e+37, -4.1046e+37,\n",
      "                  nan, -7.5939e+36, -7.1677e+36,         nan, -9.5367e+36,\n",
      "          -4.7869e+35, -2.0132e+37,         nan, -1.3228e+37],\n",
      "         [        nan,  4.2701e+37,  3.1278e+37, -3.4594e+36, -1.3173e+37,\n",
      "          -4.4869e+36,         nan,         nan, -2.0866e+37,  1.4397e+37,\n",
      "           9.2448e+35,  1.7016e+36, -3.9572e+36, -2.2918e+36, -3.2081e+36,\n",
      "          -1.2101e+37, -3.4974e+36, -1.4807e+35,  6.6339e+36, -3.5434e+36,\n",
      "          -5.6288e+35, -2.7698e+36,  3.6293e+36, -5.5168e+36,  9.3535e+36,\n",
      "           9.5465e+35, -2.0705e+37, -1.5194e+36, -2.7286e+37, -2.0721e+37,\n",
      "                  nan, -1.8357e+36,  3.2049e+37,  7.1670e+36,         nan,\n",
      "           4.6845e+36, -4.2058e+36,  6.2472e+36,  2.4124e+36, -2.6578e+36,\n",
      "          -2.2141e+37, -2.8636e+37, -4.1369e+35, -3.0506e+37,  1.9976e+36,\n",
      "           5.8713e+36,  3.0222e+37,  5.1576e+36,  3.6138e+36,  1.6406e+36,\n",
      "          -2.7575e+37,         nan, -4.5744e+36, -2.1539e+37, -2.7313e+37,\n",
      "                  nan, -8.7414e+36, -7.4574e+36,         nan, -8.2005e+36,\n",
      "           3.8115e+35, -2.6973e+37,         nan, -1.3051e+37],\n",
      "         [        nan,  2.8682e+37,  6.1103e+37, -2.6967e+36, -1.5549e+37,\n",
      "          -5.8104e+36,         nan,         nan, -1.0133e+37,  6.5978e+36,\n",
      "          -8.2512e+35,  1.2320e+36, -4.6589e+36, -3.4753e+36, -2.1134e+36,\n",
      "          -1.4487e+37, -3.2187e+36,  7.7062e+35,  3.4364e+36, -4.9016e+36,\n",
      "          -8.6521e+35, -4.1841e+36,  2.6152e+36, -7.9143e+36,  8.8688e+36,\n",
      "           2.0476e+36, -4.5180e+36,  1.5368e+35, -2.7410e+37, -9.4083e+37,\n",
      "                  nan, -2.1257e+36,  9.6171e+36,  8.8870e+36,         nan,\n",
      "           3.1058e+36, -5.2376e+36,  4.9070e+36,  3.7013e+37, -3.1266e+36,\n",
      "          -2.1147e+37, -3.5390e+37, -2.3136e+35, -3.0475e+37,  1.3876e+36,\n",
      "           5.2787e+36,  2.8096e+37,  6.7754e+36,  3.1925e+36,  1.3650e+36,\n",
      "          -2.2867e+37,         nan, -2.0622e+36, -2.0660e+37, -3.1112e+37,\n",
      "                  nan, -1.1481e+37, -8.1132e+36,         nan, -1.1277e+37,\n",
      "          -4.6646e+35, -9.7187e+36,         nan, -8.9855e+36]],\n",
      "\n",
      "        [[        nan, -6.7382e+37, -4.2625e+37, -2.0470e+36, -1.2120e+37,\n",
      "           6.9545e+36,         nan,         nan,  4.5927e+36, -4.6779e+36,\n",
      "           3.6045e+36,  3.8402e+36, -3.9906e+35,  4.0838e+36, -4.7419e+35,\n",
      "          -5.7820e+36, -3.2959e+36,  2.5794e+36,  1.2670e+37,  2.2927e+36,\n",
      "          -6.5024e+36,  3.9236e+36, -4.9213e+36,  4.4147e+36, -2.3720e+36,\n",
      "           8.0673e+34,  3.2192e+36,  8.3080e+36, -2.1957e+36,  1.1026e+38,\n",
      "                  nan, -4.4608e+36,  2.4272e+37,  1.6779e+36,         nan,\n",
      "           1.2393e+37, -3.2946e+36, -1.3854e+36,  1.0151e+38, -2.3405e+36,\n",
      "          -8.0749e+34,  6.3272e+37, -5.7680e+35, -2.0210e+37,  1.2589e+36,\n",
      "           3.9819e+36,  1.5783e+37, -1.7765e+37,  2.7154e+36,  5.9493e+34,\n",
      "          -7.5366e+37,         nan,  5.1530e+37, -1.2407e+37,  9.6505e+37,\n",
      "                  nan, -1.2620e+37, -4.3970e+36,         nan, -6.5372e+36,\n",
      "          -1.5062e+36,  5.9815e+37,         nan, -1.8907e+36],\n",
      "         [        nan, -4.7847e+37, -7.2829e+37, -4.0247e+36, -9.4325e+36,\n",
      "           3.2184e+36,         nan,         nan,  2.0933e+36, -4.4742e+36,\n",
      "           3.3286e+36,  3.7517e+36, -7.4822e+35,  3.7271e+36,  5.6983e+34,\n",
      "          -5.2446e+36, -5.6281e+35,  3.2002e+36,  1.6434e+37,  4.0984e+36,\n",
      "          -6.8866e+36,  9.5101e+35, -8.3455e+36,  3.3271e+36, -3.5104e+36,\n",
      "          -9.1216e+35, -1.4527e+36,  1.0200e+37, -9.2892e+36,  1.0743e+38,\n",
      "                  nan, -4.9266e+36,  2.4732e+37,  2.8699e+35,         nan,\n",
      "           1.8105e+37, -3.9720e+36, -4.2993e+36,  1.2451e+38, -9.3465e+35,\n",
      "           5.9127e+35,  9.2176e+37,  1.0130e+36, -1.8885e+37,  8.3359e+35,\n",
      "           2.3037e+36,  5.7416e+37, -2.1029e+37,  2.3048e+36, -8.0261e+35,\n",
      "          -6.6816e+37,         nan,  3.6136e+37, -6.5235e+36,  8.6220e+37,\n",
      "                  nan, -1.1122e+37, -4.8830e+36,         nan, -8.2444e+36,\n",
      "          -1.5866e+36,  5.5652e+37,         nan, -4.3248e+35],\n",
      "         [        nan, -7.7807e+37, -8.4701e+37, -1.5271e+35, -1.0586e+37,\n",
      "           4.2972e+36,         nan,         nan,  8.2462e+36, -7.3330e+36,\n",
      "           3.9321e+36,  3.5485e+36, -1.1267e+36,  4.2229e+36, -5.0422e+34,\n",
      "          -5.1021e+36, -1.5769e+36,  2.3742e+36,  2.1336e+37,  3.6011e+36,\n",
      "          -7.0191e+36,  5.1292e+36, -6.4630e+36,  4.9931e+36, -1.5905e+36,\n",
      "          -2.1721e+35,  5.5742e+36,  8.1162e+36, -1.1983e+37,  1.0641e+38,\n",
      "                  nan, -5.1742e+36,  2.3310e+37,  2.7715e+36,         nan,\n",
      "           1.9239e+37, -2.8520e+36,  4.8822e+35,  1.4534e+38, -2.0154e+36,\n",
      "          -4.1000e+35,  8.3717e+37,  2.9008e+36, -1.7059e+37,  3.5598e+36,\n",
      "           5.8412e+36,  5.2233e+35, -1.8547e+37,  3.5854e+36,  5.3653e+35,\n",
      "          -8.1522e+37,         nan,  6.3159e+37, -1.2448e+37,  8.7578e+37,\n",
      "                  nan, -1.1996e+37, -2.9783e+36,         nan, -5.9547e+36,\n",
      "          -3.2199e+35,  6.0936e+37,         nan,  1.4890e+35],\n",
      "         [        nan, -4.0866e+37, -8.1404e+35, -3.8771e+36, -1.2921e+37,\n",
      "           9.6882e+36,         nan,         nan,  7.4035e+35, -1.1025e+36,\n",
      "           9.3987e+35,  3.4053e+36, -1.0225e+36,  2.9446e+36,  4.6211e+35,\n",
      "          -6.9564e+36,  8.7896e+35,  1.3443e+36,  2.3434e+36,  2.0795e+36,\n",
      "          -7.1560e+36,  6.1440e+35, -6.0527e+36,  1.5701e+36, -5.3739e+36,\n",
      "           7.3904e+35,  8.8220e+35,  7.2547e+36,  3.1726e+36,  7.9941e+37,\n",
      "                  nan, -1.9231e+36,  2.5558e+37,  1.0059e+36,         nan,\n",
      "           6.1404e+36, -3.1408e+36, -2.3816e+36,  6.9959e+37, -1.8776e+36,\n",
      "          -3.5930e+35,  4.1430e+37,  4.6489e+36, -1.5787e+37, -1.7887e+36,\n",
      "           4.5677e+36,  5.2622e+37, -1.7067e+37,  1.2779e+36,  1.3825e+35,\n",
      "          -6.5667e+37,         nan,  4.8443e+37, -4.4556e+36,  7.9856e+37,\n",
      "                  nan, -1.3251e+37, -3.3514e+36,         nan, -8.7271e+36,\n",
      "          -1.4011e+36,  6.8786e+37,         nan,  3.4693e+36],\n",
      "         [        nan, -5.0943e+37, -2.2526e+37, -3.2216e+36, -1.1693e+37,\n",
      "           6.9668e+36,         nan,         nan,  1.3614e+37, -4.3622e+36,\n",
      "           3.3956e+36,  3.7700e+36, -5.9977e+35,  4.0461e+36,  3.0312e+35,\n",
      "          -6.0069e+36,  1.1386e+36,  2.6835e+36,  1.1526e+37,  3.6401e+36,\n",
      "          -6.6442e+36,  1.1696e+36, -6.7622e+36,  3.2996e+36, -3.9768e+36,\n",
      "          -1.0856e+36,  1.2781e+37,  8.2962e+36, -1.5192e+37,  1.4187e+38,\n",
      "                  nan, -5.2713e+36,  1.8595e+37,  1.2509e+36,         nan,\n",
      "           1.1996e+37, -2.0380e+36, -9.9744e+35,  1.1079e+38, -1.6160e+36,\n",
      "           1.6075e+36,  6.6058e+37,  1.6729e+36, -2.0665e+37,  2.1890e+36,\n",
      "           3.2580e+36,  4.0523e+37, -1.9129e+37,  3.1845e+36, -2.2442e+35,\n",
      "          -6.7577e+37,         nan,  4.7854e+37, -7.3919e+36,  9.2492e+37,\n",
      "                  nan, -1.2633e+37, -3.5836e+36,         nan, -7.0847e+36,\n",
      "          -2.8390e+36,  6.1240e+37,         nan,  9.7956e+35]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = 64  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "Q = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "K = torch.randn(batch_size, seq_length, d_k)  # Keys\n",
    "V = torch.randn(batch_size, seq_length, model_dim)  # Values\n",
    "\n",
    "# Print the shapes for confirmation\n",
    "print(\"Shape of Q:\", Q.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of K:\", K.shape)  # Expected: (batch_size, seq_length, d_k)\n",
    "print(\"Shape of V:\", V.shape)  # Expected: (batch_size, seq_length, model_dim)\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(model_dim, d_k, model_dim, h)\n",
    "output = multi_head_attention(Q, K, V)\n",
    "print(f'Shape of output:{output.shape}')\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)  # Queries\n",
    "encoder_stack = EncoderStack(model_dim, d_k, model_dim, h)\n",
    "encoder_output = encoder_stack(X)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)\n",
    "\n",
    "decoder_stack = DecoderStack(model_dim, d_k, model_dim, h, None)\n",
    "decoder_output = decoder_stack(X, encoder_output, encoder_output)\n",
    "print(decoder_output.shape)\n",
    "print(decoder_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64])\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2    # Example batch size\n",
    "seq_length = 5    # Length of the sequence (number of tokens)\n",
    "d_k = 64         # Dimension of the keys (and queries)\n",
    "model_dim = d_k  # Dimension of the model\n",
    "h = 8\n",
    "\n",
    "# Generate random tensors for Q, K, V\n",
    "X = torch.randn(batch_size, seq_length, d_k)\n",
    "Y = torch.randn(batch_size, seq_length, d_k)\n",
    "\n",
    "attention_is_all_you_need = AttentionIsAllYouNeed(model_dim, d_k, model_dim, h, 1, 1)\n",
    "output = attention_is_all_you_need(X, Y)\n",
    "print(output.shape)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}